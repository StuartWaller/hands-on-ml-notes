{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter 16: Natural Language Processing with RNNs and Attention.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMb09t1KsXkogPJ6+IBYuU8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b1b4e666aa114b33a4f8356e0d7a428b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_69275020882a49ea8c2c1578437fc9b3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_783b574b5e5a4fa29640f214117866fc","IPY_MODEL_f1526faa0ffc4f5dbb01f7a421256eeb"]}},"69275020882a49ea8c2c1578437fc9b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"783b574b5e5a4fa29640f214117866fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_478ffae82a524e71ba679f05034bd538","_dom_classes":[],"description":"Dl Completed...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0257eeacf5424e58bf80ae8c13164ae8"}},"f1526faa0ffc4f5dbb01f7a421256eeb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_759c4d213909416c9e7b02ed66530b8f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:08&lt;00:00,  8.89s/ url]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7f4243d79d434875b5f2d914ed020179"}},"478ffae82a524e71ba679f05034bd538":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0257eeacf5424e58bf80ae8c13164ae8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"759c4d213909416c9e7b02ed66530b8f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7f4243d79d434875b5f2d914ed020179":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5fbbae3312c445e9cabbc270f1ccddc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4db7299eae1b48a5ae2fdda07af94504","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_883fd589e9f94720816ab607118bbaf7","IPY_MODEL_fcc7163ac12f4626ad2d8a3ddf53128b"]}},"4db7299eae1b48a5ae2fdda07af94504":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"883fd589e9f94720816ab607118bbaf7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_18f2e8f371f2455385641215a8fc474e","_dom_classes":[],"description":"Dl Size...: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e2753aec4a60487a82d2544b8bb46e67"}},"fcc7163ac12f4626ad2d8a3ddf53128b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f81bced9ebd14207ba9adc60cd105bf2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 80/80 [00:08&lt;00:00,  9.03 MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d42e41025cce488ca793ec211bd07b85"}},"18f2e8f371f2455385641215a8fc474e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e2753aec4a60487a82d2544b8bb46e67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f81bced9ebd14207ba9adc60cd105bf2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d42e41025cce488ca793ec211bd07b85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"35540427648241dfb166fc1567759f10":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_605e1dcc46e74027aedf54b1c2a47888","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_550d5ce717af44508f155b8c2234bb9d","IPY_MODEL_b5166bbb8701433d843d16757607afbb"]}},"605e1dcc46e74027aedf54b1c2a47888":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"550d5ce717af44508f155b8c2234bb9d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_895baa44fc444ec6a75232bee78be666","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e6f0c79ceec142b78f616b679a604503"}},"b5166bbb8701433d843d16757607afbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cd6e3155a7584c3a8a1bbd8960d53a47","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 25000/0 [00:13&lt;00:00, 3316.80 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_caa03f7a0dbe4d819e2d463474caf5d0"}},"895baa44fc444ec6a75232bee78be666":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e6f0c79ceec142b78f616b679a604503":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cd6e3155a7584c3a8a1bbd8960d53a47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"caa03f7a0dbe4d819e2d463474caf5d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c42506ac4d5418c8ece7bac2c101e1a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4e5ae6a11d034d72ab6e6f73403485ba","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_84d7f9b417c841e1a18dab3cf63153dc","IPY_MODEL_467816d261ec42f3ae7510e7ccc01a4b"]}},"4e5ae6a11d034d72ab6e6f73403485ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"84d7f9b417c841e1a18dab3cf63153dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_91b9ae43121541929dbf8e10e9fb9e94","_dom_classes":[],"description":" 54%","_model_name":"FloatProgressModel","bar_style":"danger","max":25000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":13514,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4da5796f732942f087e6fd4ee8619ecf"}},"467816d261ec42f3ae7510e7ccc01a4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_82875623e38f423fa8c382a857186949","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 13514/25000 [00:00&lt;00:00, 135138.52 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_90259bb844b1453287fa83682639a638"}},"91b9ae43121541929dbf8e10e9fb9e94":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4da5796f732942f087e6fd4ee8619ecf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"82875623e38f423fa8c382a857186949":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"90259bb844b1453287fa83682639a638":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d7d11352555740a7abf7ce3347821d1e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_e0ec7f99ee8348549874d7d6c8466f80","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_69e7d32c759842f99f332b5d6eff2044","IPY_MODEL_576624953cf14908abc8794052f2a286"]}},"e0ec7f99ee8348549874d7d6c8466f80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"69e7d32c759842f99f332b5d6eff2044":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_904d1c839c8641f6b34af28fe1c93f11","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7e3ea536f24e449c98ee9a979e8a055e"}},"576624953cf14908abc8794052f2a286":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4f90b486f56e48f988021c706a4e4c87","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 25000/0 [00:12&lt;00:00, 3451.18 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_adc4b7f8c7054d43b29bc02d109a4b84"}},"904d1c839c8641f6b34af28fe1c93f11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7e3ea536f24e449c98ee9a979e8a055e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4f90b486f56e48f988021c706a4e4c87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"adc4b7f8c7054d43b29bc02d109a4b84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5bf3c7a4ed0c45c1adbfae564b9e5d70":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2730f4187260410e827270d9d4fdeaf7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_116996682e2b410c9422c2aaf63ef602","IPY_MODEL_15cd9fcbab344bc8b9de6a626b67aa2a"]}},"2730f4187260410e827270d9d4fdeaf7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"116996682e2b410c9422c2aaf63ef602":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_45e9c893d52543859d9b04d5dfe93807","_dom_classes":[],"description":" 63%","_model_name":"FloatProgressModel","bar_style":"danger","max":25000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":15727,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_680db9ab5f0244ce9fd007769a420197"}},"15cd9fcbab344bc8b9de6a626b67aa2a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_164840d1d7ac42749bdd41412d979218","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 15727/25000 [00:00&lt;00:00, 157265.65 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1b4d6316169e4c16a597d381ccc04ff0"}},"45e9c893d52543859d9b04d5dfe93807":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"680db9ab5f0244ce9fd007769a420197":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"164840d1d7ac42749bdd41412d979218":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1b4d6316169e4c16a597d381ccc04ff0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6d3a9ffb33c54c4a8c1cbbf907888034":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_54b0d369c5af49a4be7a0d78533f8c46","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dabb60e44a234a7ca3f99ddd4058569c","IPY_MODEL_856042332b5c4e03897b574909d586cf"]}},"54b0d369c5af49a4be7a0d78533f8c46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dabb60e44a234a7ca3f99ddd4058569c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_234adfb8dfec47abb92c63e965aa268a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1355b3e77b684b36841b75321a6526ab"}},"856042332b5c4e03897b574909d586cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_19ccaae2eb844a788c6c9f3af05e1456","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 50000/0 [00:18&lt;00:00, 3383.75 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0776d037a86b4117b2620963baa82359"}},"234adfb8dfec47abb92c63e965aa268a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1355b3e77b684b36841b75321a6526ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19ccaae2eb844a788c6c9f3af05e1456":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0776d037a86b4117b2620963baa82359":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"967889ed2fcd4e31a53fd3d1365fdc53":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_deb199c0dce64e5d97fd022243acecbd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_b4e7e54989ae4b939df9776c7f1828d6","IPY_MODEL_00e2d689fc0e41cd868dcb6963ba0ff9"]}},"deb199c0dce64e5d97fd022243acecbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b4e7e54989ae4b939df9776c7f1828d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7ea40bd06b8e4d4d829f4f51814d8487","_dom_classes":[],"description":" 69%","_model_name":"FloatProgressModel","bar_style":"danger","max":50000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":34257,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cdc31cd430c046b2acdc19105db31810"}},"00e2d689fc0e41cd868dcb6963ba0ff9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_905a5b7620224d6b877f23a440da3c87","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 34257/50000 [00:00&lt;00:00, 83647.57 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a2e8ff80a7ae4c66b3134bad5d9d4b4b"}},"7ea40bd06b8e4d4d829f4f51814d8487":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cdc31cd430c046b2acdc19105db31810":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"905a5b7620224d6b877f23a440da3c87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a2e8ff80a7ae4c66b3134bad5d9d4b4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"LD6L1GYxPxBy","colab_type":"text"},"source":["# Setup"]},{"cell_type":"code","metadata":{"id":"hVoIcZJeOFw8","colab_type":"code","colab":{}},"source":["# Python ≥3.5 is required\n","import sys\n","assert sys.version_info >= (3, 5)\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","    !pip install -q -U tensorflow-addons\n","    IS_COLAB = True\n","except Exception:\n","    IS_COLAB = False\n","\n","# TensorFlow ≥2.0 is required\n","import tensorflow as tf\n","from tensorflow import keras\n","assert tf.__version__ >= \"2.0\"\n","\n","if not tf.config.list_physical_devices('GPU'):\n","    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n","    if IS_COLAB:\n","        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","tf.random.set_seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)\n","\n","# Where to save the figures\n","PROJECT_ROOT_DIR = \".\"\n","CHAPTER_ID = \"nlp\"\n","IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n","os.makedirs(IMAGES_PATH, exist_ok=True)\n","\n","def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n","    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n","    print(\"Saving figure\", fig_id)\n","    if tight_layout:\n","        plt.tight_layout()\n","    plt.savefig(path, format=fig_extension, dpi=resolution)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"macii0rFP8Zp","colab_type":"text"},"source":["# Introduction"]},{"cell_type":"markdown","metadata":{"id":"NxJBA2zDQLHf","colab_type":"text"},"source":["- recurrent neural networks are popular for natural language processing\n","- a ***character RNN***  is trained to predict the next character in a sentence\n","- a ***stateless RNN*** learns on random portions of text at each iteration without any context \n","- a ***stateful RNN*** preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns\n","- an RNN performs sentiment analysis by treating sentences as sequences of words rather than characters\n","- RNNs can be used to build Encoder-Decoder architectures capable of performing neural machine translation (NMT)\n","---\n","- ***attention mechanisms*** are neural network components that learn to select the part of the inputs that the rest of the model should focus on at each step: \n"," - attention can be used to boost performance\n"," - there is also an attention-only architecture called the ***Transformer***\n"]},{"cell_type":"markdown","metadata":{"id":"835sSRtbSN5o","colab_type":"text"},"source":["# Character RNNs"]},{"cell_type":"markdown","metadata":{"id":"uNOauJn4SUAJ","colab_type":"text"},"source":["- the ***Char-RNN*** can be used to generate novel text, one character at a time:\n"," - it can learn words, grammar, and punctuation just by learning to predict the next character in a sentence"]},{"cell_type":"markdown","metadata":{"id":"yg8SrXwoUHpo","colab_type":"text"},"source":["## Character Encoding"]},{"cell_type":"markdown","metadata":{"id":"BpkTKDIeUMK7","colab_type":"text"},"source":["- let's first download all of Shakepeare's work using Keras's `get_file()` function:"]},{"cell_type":"code","metadata":{"id":"WxFAoQoMUjOr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"b979d7f7-ffea-4c4f-f284-8e9981584b36","executionInfo":{"status":"ok","timestamp":1590863736287,"user_tz":420,"elapsed":17539,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n","filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n","with open(filepath) as f:\n","    shakespeare_text = f.read()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n","1122304/1115394 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LCus3xDUUsme","colab_type":"code","outputId":"e1d7c05c-718a-4cff-8e7c-44f820580997","executionInfo":{"status":"ok","timestamp":1590863736288,"user_tz":420,"elapsed":17532,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(shakespeare_text[0]) "],"execution_count":3,"outputs":[{"output_type":"stream","text":["F\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d7tqVl9LUo8C","colab_type":"code","outputId":"d32be22d-7bd8-4066-9150-d0c979e417da","executionInfo":{"status":"ok","timestamp":1590863736288,"user_tz":420,"elapsed":17525,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":173}},"source":["print(shakespeare_text[:148]) "],"execution_count":4,"outputs":[{"output_type":"stream","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"X37z3quHU096","colab_type":"text"},"source":["- next, we must encode every character as an integer using Keras's `Tokenizer` class: \n"," - first, we need to fit a tokenizer to the text, in which it will identify every unique character and map each one to a different character ID from 1 to the number of distinct characters (doesn't start at 0 by default)"]},{"cell_type":"code","metadata":{"id":"zVo8EXAiVp7v","colab_type":"code","colab":{}},"source":["tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","tokenizer.fit_on_texts(shakespeare_text)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eV2J7AbnVxQL","colab_type":"text"},"source":["- we set `char_level=True` to get character-level encoding rather than the default word-level coding"]},{"cell_type":"code","metadata":{"id":"DW2mW6S7WGim","colab_type":"code","outputId":"eb6257d1-8108-47a6-f631-8ad648bb9aea","executionInfo":{"status":"ok","timestamp":1590863737037,"user_tz":420,"elapsed":18261,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tokenizer.texts_to_sequences([\"love\"])"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[12, 4, 26, 2]]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"q92QSeiEWPMS","colab_type":"code","outputId":"df70b77c-a5d8-42f3-90f2-6bd7a6fa81d3","executionInfo":{"status":"ok","timestamp":1590863737037,"user_tz":420,"elapsed":18253,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tokenizer.sequences_to_texts([[12, 4, 26, 2]])"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['l o v e']"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"V7kYogWcYNSc","colab_type":"code","outputId":"57f64e11-4785-4df7-a35e-5b0cb8f588a7","executionInfo":{"status":"ok","timestamp":1590863737037,"user_tz":420,"elapsed":18245,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["max_id = len(tokenizer.word_index) # number of distinct characters\n","dataset_size = tokenizer.document_count # total number of characters\n","max_id, dataset_size"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(39, 1115394)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"2nF-r5NFWgCj","colab_type":"text"},"source":["- let's encode the full text so each character is represented by its ID: \n"," - we subtract 1 to get IDs from 0 to 38 rather than 1 to 39"]},{"cell_type":"code","metadata":{"id":"LwSDf3UmYPtz","colab_type":"code","colab":{}},"source":["[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iSgiXmqTYRZY","colab_type":"text"},"source":["## Splitting Sequential Data"]},{"cell_type":"markdown","metadata":{"id":"HR117puHYejt","colab_type":"text"},"source":["- it is important to avoid any overlap between the training set, validation set, and the test set\n","- with time series, it is common to split across time: \n"," - for example, taking the years 2000 to 2012 for the training set, 2013 to 2015 for the validation set, etc.\n"," - this assumes that the patterns the RNN can learn in the past (in the training set) will exist in the future (a ***stationary*** time series)\n","- to ensure a time series is sufficiently stationary, plot the model's errors on the validation set across time: \n"," - if the model performs much better on the first part of the validation set than on the last part, the time series may not be stationary enough\n","---\n","- for our Shakespearean text: first 90% --> training set (keeping the rest for the validation/test sets)"]},{"cell_type":"code","metadata":{"id":"0iMfaYj1a5ie","colab_type":"code","colab":{}},"source":["train_size = dataset_size * 90 // 100\n","dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_sg_LREmbAIm","colab_type":"text"},"source":["## Chopping the Sequential Dataset into Multiple Windows"]},{"cell_type":"markdown","metadata":{"id":"Jt2oSWUNbK3B","colab_type":"text"},"source":["- the training set now consists of a single million+ character sequence, which means we can't train the neural network just yet:\n"," - the RNN would have a million+ layers and we would only have a single, very long instance to train it\n","- instead, we must use the dataset's `window()` method to convert this long sequence of characters into many smaller windows of text:\n"," - every instance will be a short substring of the whole text\n"," - the RNN will be unrolled over the length of these substrings (***truncated backpropagation through time***) "]},{"cell_type":"code","metadata":{"id":"kt7n8pz6cG0r","colab_type":"code","colab":{}},"source":["n_steps = 100\n","window_length = n_steps + 1 # target = input shifted 1 character ahead\n","dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qBL08woycdyI","colab_type":"text"},"source":["- we use `shift=1` so that the first window contains characters 0 to 100, the second contains characters 1 to 101, etc. (nonoverlapping windows)\n","- to ensure all windows are 101 characters long (which allows us to create batches without any padding), we set `drop_remainder=True`\n","---\n","- `window()` creates a dataset containing windows, each of which representing a unique dataset (a ***nested dataset***): \n"," - this is a list of lists\n","- we cannot use a nested dataset directly for training as our model expects tensors as inputs, not datasets:\n"," - we must call `flat_map()`, which converts a dataset into a ***flat dataset***\n"," - nested dataset: `{{1, 2}, {3, 4, 5, 6}}` --> \n"," - flat dataset: `{1, 2, 3, 4, 5, 6}`\n","- passing `lambda ds: ds.batch(2)` to `flat_map()` transforms the nested dataset `{{1, 2}, {3, 4, 5, 6}}` into a flat dataset of size 2 tensors `{[1, 2], [3, 4], [5, 6]}`"]},{"cell_type":"code","metadata":{"id":"4XgVN6VYe_OP","colab_type":"code","colab":{}},"source":["dataset = dataset.flat_map(lambda window: window.batch(window_length))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6Kqko5FsgDPk","colab_type":"text"},"source":["- we call `batch(window_length)` on each window since all windows are the same length, we get a single tensor for each\n","- we now have a dataset containing consecutive windows of 101 characters each"]},{"cell_type":"markdown","metadata":{"id":"NKhwi6lhgc8h","colab_type":"text"},"source":["## Shuffling the Windows"]},{"cell_type":"markdown","metadata":{"id":"xpmq-H5-gsBK","colab_type":"text"},"source":["- Gradient Descent works best when the instances in the training set are independent and identically distributed, so we need to shuffle these windows\n","- next, we can batch the windows and separate the inputs (the first 100 characters) from the target (the last character)"]},{"cell_type":"code","metadata":{"id":"MSzFJlVVhuTp","colab_type":"code","colab":{}},"source":["np.random.seed(42)\n","tf.random.set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9pCM-eQhvop","colab_type":"code","colab":{}},"source":["batch_size = 32\n","dataset = dataset.shuffle(10000).batch(batch_size)\n","dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Cu3o8aTh0rK","colab_type":"text"},"source":["- categorical input features should be encoded as one-hot vectors or embeddings:"]},{"cell_type":"code","metadata":{"id":"9WBf0bgYiGEm","colab_type":"code","colab":{}},"source":["dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LdbSDagqiJTV","colab_type":"text"},"source":["- finally, just add prefetching:"]},{"cell_type":"code","metadata":{"id":"QATXzfqpiLLW","colab_type":"code","colab":{}},"source":["dataset = dataset.prefetch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jB2b-94JiUW4","colab_type":"code","outputId":"b5bd4f93-b2c2-4eaf-a8a5-27318034886d","executionInfo":{"status":"ok","timestamp":1590863744849,"user_tz":420,"elapsed":26014,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["for X_batch, Y_batch in dataset.take(1):\n","    print(X_batch.shape, Y_batch.shape)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["(32, 100, 39) (32, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PAuh1Hhdif8T","colab_type":"text"},"source":["## Building and Training the Char-RNN Model"]},{"cell_type":"markdown","metadata":{"id":"7ECr5TREijmc","colab_type":"text"},"source":["- to predict the next character based on the previous 100, we'll use an RNN with 2 `GRU` layers with 128 units each and 20% dropout on both the inputs (`dropout`) and the hidden states (`recurrent_dropout`)\n","- the output layer is a time-distributed `Dense` layer, which has 39 units (`max_id`) because there's 39 distinct characters in the text:\n"," - we want to output a probability for each possible character at each time step\n"," - the output probabilities should sum up to 1 at each time step, so the softmax activation function is applied to the `Dense` layer's outputs"]},{"cell_type":"code","metadata":{"id":"gKCbBaTmjgH0","colab_type":"code","outputId":"fc147dca-310b-4a4f-cb2d-a5ac171a6e00","executionInfo":{"status":"ok","timestamp":1590863745293,"user_tz":420,"elapsed":26452,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n","                     dropout=0.2, recurrent_dropout=0.2),\n","    keras.layers.GRU(128, return_sequences=True,\n","                     dropout=0.2, recurrent_dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","])"],"execution_count":18,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1HMWwzR53mqH","colab_type":"code","colab":{}},"source":["# takes hours\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","history = model.fit(dataset, steps_per_epoch=train_size // batch_size,\n","                    epochs=10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8hmXiZVY3r4B","colab_type":"text"},"source":["## Using the Model"]},{"cell_type":"markdown","metadata":{"id":"vJLoYlZY9bjh","colab_type":"text"},"source":["- we now have a model that can predict the next character in Shakespearean text\n","- first, we need to employ the same preprocessing steps as earlier:"]},{"cell_type":"code","metadata":{"id":"NMAQjgo_9tAe","colab_type":"code","colab":{}},"source":["def preprocess(texts):\n","    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n","    return tf.one_hot(X, max_id)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQRG_s9e99RR","colab_type":"text"},"source":["- now, let's use the model to predict the next letter in some text:"]},{"cell_type":"code","metadata":{"id":"onoIPClw9zdb","colab_type":"code","colab":{}},"source":["X_new = preprocess([\"How are yo\"])\n","Y_pred = model.predict_classes(X_new)\n","tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ERl1DYGX-AOm","colab_type":"text"},"source":["`'u'`"]},{"cell_type":"markdown","metadata":{"id":"38J-lWtU-Q3Z","colab_type":"text"},"source":["## Using the Model to Generate Text"]},{"cell_type":"markdown","metadata":{"id":"BoPObi3gB5Wm","colab_type":"text"},"source":["- to generate new text using our Char-RNN model, we pick the next character randomly with a probability equal to the estimated probability using `tf.random.categorical()`:\n"," - `categorical()` samples random class indices given the class log probabilities (logits)\n","- for better control over the diversity of the geneated text, we can divide the logits by the ***temperature***:\n"," - a temperature of ~0 favors high probability characters, while a very high temperature gives all characters an equal probability\n","- the following `next_char()` function uses this approach to pick the next character to add to the input text:"]},{"cell_type":"code","metadata":{"id":"MQjU_eaJDN5L","colab_type":"code","colab":{}},"source":["tf.random.set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3iY-suxwDSHi","colab_type":"code","colab":{}},"source":["def next_char(text, temperature=1):\n","    X_new = preprocess([text])\n","    y_proba = model.predict(X_new)[0, -1:, :]\n","    rescaled_logits = tf.math.log(y_proba) / temperature\n","    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n","    return tokenizer.sequences_to_texts(char_id.numpy())[0]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMIt8S4_FAHy","colab_type":"text"},"source":["- the following `complete_text()` function repeatedly calls `next_char()` to get the next character and append it to the text:"]},{"cell_type":"code","metadata":{"id":"4vy71tzKFFUt","colab_type":"code","colab":{}},"source":["def complete_text(text, n_chars=50, temperature=1):\n","    for _ in range(n_chars):\n","        text += next_char(text, temperature)\n","    return text"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ok_c2EY_FXJQ","colab_type":"text"},"source":["- now, let's generate some text (experimenting with different temperatures):"]},{"cell_type":"code","metadata":{"id":"gbaJl9tfFg0y","colab_type":"code","colab":{}},"source":["tf.random.set_seed(42)\n","\n","print(complete_text(\"t\", temperature=0.2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J92kpcnGFkDr","colab_type":"text"},"source":["`the belly the great and who shall be the belly the`"]},{"cell_type":"code","metadata":{"id":"sXZ7zQxJFoKw","colab_type":"code","colab":{}},"source":["print(complete_text(\"w\", temperature=1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eZe_i5yyFrlr","colab_type":"text"},"source":["`thing? or why you gremio.`\n","\n","`who make which the first`"]},{"cell_type":"code","metadata":{"id":"APLzjoPJGFQG","colab_type":"code","colab":{}},"source":["print(complete_text(\"w\", temperature=2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WDGV-WW2GQ2s","colab_type":"text"},"source":["`th no cce:`\n","\n","`yeolg-hormer firi. a play asks.`\n","\n","`fol rusb`"]},{"cell_type":"markdown","metadata":{"id":"bHUpw5yoGcMV","colab_type":"text"},"source":["- our Shakespeare model works best with a temperature close to 1\n","- to generate more convincing text, use more `GRU` layers with more neurons, increase the training time, and add some regularization (e.g., setting `recurrent_dropout=0.3` in the `GRU` layers)"]},{"cell_type":"markdown","metadata":{"id":"3iwe-UhfHIQM","colab_type":"text"},"source":["## Stateful Character RNN"]},{"cell_type":"markdown","metadata":{"id":"XFz7pDlVHzYy","colab_type":"text"},"source":["- so far, we have only used ***stateless RNNs***: the model starts with a hidden state full of zeros at each training iteration, then updates this state at each time step, then discards it after the last time step\n","- ***stateful RNNs*** preserve this final state after processing one training batch and use it as the initial state for the next training batch:\n"," - this enables them to learn long-term patterns despite only backpropagating through short sequences\n","---\n","- stateful RNNs only function if each batch's input sequence starts exactly where the corresponding sequence in the previous batch left off: \n"," - therefore, we must use sequential and nonoverlapping input sequences (rather than the shuffled and overlapping sequences we used to train our stateless, Char-RNN)\n","- when creating the `Dataset`, we must therefore use `shift=n_steps` instead of `shift=1` when calling `window()`\n","---\n","- batching is much more difficult with stateful RNNs:\n"," - we chop Shakespeare's text into 32 texts of equal length\n"," - then we create one dataset of consecutive input sequences for each of them\n"," - then we use `tf.train.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows))` to create proper consecutive batches"]},{"cell_type":"code","metadata":{"id":"DJUR4AggKelR","colab_type":"code","colab":{}},"source":["tf.random.set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pj0Fwe1pKvCu","colab_type":"code","colab":{}},"source":["batch_size = 32\n","encoded_parts = np.array_split(encoded[:train_size], batch_size)\n","datasets = []\n","for encoded_part in encoded_parts:\n","    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n","    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n","    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n","    datasets.append(dataset)\n","dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n","dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n","dataset = dataset.map(\n","    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n","dataset = dataset.prefetch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aOQNZrNKLiUI","colab_type":"text"},"source":["- now, let's create the stateful RNN:\n"," - first, set `stateful=True` when creating every recurrent layer\n"," - second, set the `batch_input_shape` argument in the first layer (leave the second dimension unspecified as the inputs could have any length)"]},{"cell_type":"code","metadata":{"id":"mBOGgz5fL_Rm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"84b19fb9-f3d3-4dd0-9cf6-5c5bbc52fe6d","executionInfo":{"status":"ok","timestamp":1590869097844,"user_tz":420,"elapsed":1029,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["model = keras.models.Sequential([\n","    keras.layers.GRU(128, return_sequences=True, stateful=True,\n","                     dropout=0.2, recurrent_dropout=0.2,\n","                     batch_input_shape=[batch_size, None, max_id]),\n","    keras.layers.GRU(128, return_sequences=True, stateful=True,\n","                     dropout=0.2, recurrent_dropout=0.2),\n","    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n","                                                    activation=\"softmax\"))\n","])"],"execution_count":27,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer gru_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer gru_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TNybYf-XMJey","colab_type":"text"},"source":["- at the end of each epoch, we must reset the states before going back to the beginning of the text:\n"," - for this, we can use a small callback"]},{"cell_type":"code","metadata":{"id":"J9QlR250MZvn","colab_type":"code","colab":{}},"source":["class ResetStatesCallback(keras.callbacks.Callback):\n","    def on_epoch_begin(self, epoch, logs):\n","        self.model.reset_states()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WfmNVKIpMkZT","colab_type":"text"},"source":["- each epoch is much shorter than earlier and there is only one instance per batch:"]},{"cell_type":"code","metadata":{"id":"jN7mTEQuMzoY","colab_type":"code","colab":{}},"source":["# takes an hour to run\n","model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n","steps_per_epoch = train_size // batch_size // n_steps\n","history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n","                    callbacks=[ResetStatesCallback()])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hluuJgFrM5kk","colab_type":"text"},"source":["- after training this model, it will only make predictions for batches of the same size as were used during training:\n"," - create an identical model and copy this model's weights to avoid this restriction"]},{"cell_type":"markdown","metadata":{"id":"7tCoa9FLNdlc","colab_type":"text"},"source":["# Sentiment Analysis"]},{"cell_type":"markdown","metadata":{"id":"kXCh_WbgNiC7","colab_type":"text"},"source":["- while MNIST is the \"hello world\" of computer vision, the IMDb reviews dataset is the \"hello world\" of natural language processing: \n"," - it consists of 50,000 English movie reviews (25,000 training | 25,000 testing) along with a binary target for each review indicating whether it is negative (0) or positive (1)"]},{"cell_type":"code","metadata":{"id":"JLmOK3faOEjN","colab_type":"code","colab":{}},"source":["tf.random.set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W6mNJlV4OFpQ","colab_type":"code","colab":{}},"source":["(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"emZDyNKwOf-f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3cd18048-eb2d-46f0-e6e6-74217824204b","executionInfo":{"status":"ok","timestamp":1590871740244,"user_tz":420,"elapsed":5212,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["X_train[0][:10]"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"uoe7L1mvOh1f","colab_type":"text"},"source":["- the dataset has already been preprocessed: \n"," - `X_train` consits of a list of reviews, each of which is represented as a NumPy array of integers, where each integer represents a word\n","- the integers 0, 1, and 2 represent the padding token, the ***start-of-sequence*** (SSS) token, and unknown words\n"]},{"cell_type":"code","metadata":{"id":"rqoWJ3jEQueJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"c72ae770-f2ca-4429-cdb0-444b74b210f5","executionInfo":{"status":"ok","timestamp":1590871740244,"user_tz":420,"elapsed":3917,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["# visualizing a review\n","word_index = keras.datasets.imdb.get_word_index()\n","id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n","for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n","    id_to_word[id_] = token\n","\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<sos> this film was just brilliant casting location scenery story'"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"h-SjfgJdWwX5","colab_type":"text"},"source":["- let's load the original IMDb reviews as text (byte strings) using TensorFlow Datasets:"]},{"cell_type":"code","metadata":{"id":"fpE-hqAtW-NY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":353,"referenced_widgets":["b1b4e666aa114b33a4f8356e0d7a428b","69275020882a49ea8c2c1578437fc9b3","783b574b5e5a4fa29640f214117866fc","f1526faa0ffc4f5dbb01f7a421256eeb","478ffae82a524e71ba679f05034bd538","0257eeacf5424e58bf80ae8c13164ae8","759c4d213909416c9e7b02ed66530b8f","7f4243d79d434875b5f2d914ed020179","d5fbbae3312c445e9cabbc270f1ccddc","4db7299eae1b48a5ae2fdda07af94504","883fd589e9f94720816ab607118bbaf7","fcc7163ac12f4626ad2d8a3ddf53128b","18f2e8f371f2455385641215a8fc474e","e2753aec4a60487a82d2544b8bb46e67","f81bced9ebd14207ba9adc60cd105bf2","d42e41025cce488ca793ec211bd07b85","35540427648241dfb166fc1567759f10","605e1dcc46e74027aedf54b1c2a47888","550d5ce717af44508f155b8c2234bb9d","b5166bbb8701433d843d16757607afbb","895baa44fc444ec6a75232bee78be666","e6f0c79ceec142b78f616b679a604503","cd6e3155a7584c3a8a1bbd8960d53a47","caa03f7a0dbe4d819e2d463474caf5d0","8c42506ac4d5418c8ece7bac2c101e1a","4e5ae6a11d034d72ab6e6f73403485ba","84d7f9b417c841e1a18dab3cf63153dc","467816d261ec42f3ae7510e7ccc01a4b","91b9ae43121541929dbf8e10e9fb9e94","4da5796f732942f087e6fd4ee8619ecf","82875623e38f423fa8c382a857186949","90259bb844b1453287fa83682639a638","d7d11352555740a7abf7ce3347821d1e","e0ec7f99ee8348549874d7d6c8466f80","69e7d32c759842f99f332b5d6eff2044","576624953cf14908abc8794052f2a286","904d1c839c8641f6b34af28fe1c93f11","7e3ea536f24e449c98ee9a979e8a055e","4f90b486f56e48f988021c706a4e4c87","adc4b7f8c7054d43b29bc02d109a4b84","5bf3c7a4ed0c45c1adbfae564b9e5d70","2730f4187260410e827270d9d4fdeaf7","116996682e2b410c9422c2aaf63ef602","15cd9fcbab344bc8b9de6a626b67aa2a","45e9c893d52543859d9b04d5dfe93807","680db9ab5f0244ce9fd007769a420197","164840d1d7ac42749bdd41412d979218","1b4d6316169e4c16a597d381ccc04ff0","6d3a9ffb33c54c4a8c1cbbf907888034","54b0d369c5af49a4be7a0d78533f8c46","dabb60e44a234a7ca3f99ddd4058569c","856042332b5c4e03897b574909d586cf","234adfb8dfec47abb92c63e965aa268a","1355b3e77b684b36841b75321a6526ab","19ccaae2eb844a788c6c9f3af05e1456","0776d037a86b4117b2620963baa82359","967889ed2fcd4e31a53fd3d1365fdc53","deb199c0dce64e5d97fd022243acecbd","b4e7e54989ae4b939df9776c7f1828d6","00e2d689fc0e41cd868dcb6963ba0ff9","7ea40bd06b8e4d4d829f4f51814d8487","cdc31cd430c046b2acdc19105db31810","905a5b7620224d6b877f23a440da3c87","a2e8ff80a7ae4c66b3134bad5d9d4b4b"]},"outputId":"fb0896d7-f1a1-4322-ace8-7710a5078983","executionInfo":{"status":"ok","timestamp":1590872035714,"user_tz":420,"elapsed":56017,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["import tensorflow_datasets as tfds\n","\n","datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"],"execution_count":51,"outputs":[{"output_type":"stream","text":["\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1b4e666aa114b33a4f8356e0d7a428b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Completed...', max=1.0, style=Progre…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5fbbae3312c445e9cabbc270f1ccddc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Dl Size...', max=1.0, style=ProgressSty…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"35540427648241dfb166fc1567759f10","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5KD5GA/imdb_reviews-train.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c42506ac4d5418c8ece7bac2c101e1a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7d11352555740a7abf7ce3347821d1e","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5KD5GA/imdb_reviews-test.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bf3c7a4ed0c45c1adbfae564b9e5d70","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=25000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6d3a9ffb33c54c4a8c1cbbf907888034","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incomplete5KD5GA/imdb_reviews-unsupervised.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"967889ed2fcd4e31a53fd3d1365fdc53","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=50000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[1mDataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n","\r"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Grd6pGsoXZsA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7629b25d-3869-4297-b71e-e240f7ecd226","executionInfo":{"status":"ok","timestamp":1590872089234,"user_tz":420,"elapsed":808,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["datasets.keys()"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys(['test', 'train', 'unsupervised'])"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"11WPEvLDXcij","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8fe799c9-ad12-493c-c9d0-dd39b8a621fd","executionInfo":{"status":"ok","timestamp":1590872107025,"user_tz":420,"elapsed":262,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["train_size = info.splits[\"train\"].num_examples\n","test_size = info.splits[\"test\"].num_examples\n","train_size, test_size"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 25000)"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"wWmbks6dXhZ6","colab_type":"text"},"source":["- now, let's write the preprocessing function:"]},{"cell_type":"code","metadata":{"id":"GVZ_ChsZXmiF","colab_type":"code","colab":{}},"source":["def preprocess(X_batch, y_batch):\n","    X_batch = tf.strings.substr(X_batch, 0, 300)\n","    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n","    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n","    X_batch = tf.strings.split(X_batch)\n","    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuSi2hW8XwU7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"d28ed212-91ed-47ed-cd6f-589976c15961","executionInfo":{"status":"ok","timestamp":1590872187196,"user_tz":420,"elapsed":775,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["preprocess(X_batch, y_batch)"],"execution_count":56,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n"," array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n","         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n","         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n","         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n","         b'their', b'worst', b'role', b'in', b'history', b'Even',\n","         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n","         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n","         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n","         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n","        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n","         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n","         b'to', b'a', b'combination', b'of', b'things', b'including',\n","         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n","         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n","         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n","         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n","         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n","         b'Cons']], dtype=object)>,\n"," <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0])>)"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"markdown","metadata":{"id":"c1xkWQbhX05z","colab_type":"text"},"source":["- `preprocess()` starts by truncating the reviews, only keeping the first 300 characters of each: \n"," - this will speed up training and not impact performance as you can generally tell the sentiment of a review from the first few sentences\n","- then it uses ***regular expressions*** to replace `<br />` tags with spaces, and to replace any characters other than the letters and quotes with spaces:\n"," - for example, the text `\"Well, I can't<br />\"` becomes `\"Well I can't\"`\n","- finally, `preprocess()` splits the reviews by the spaces, which returns a ragged tensor, and then converts this ragged tensor to a dense tensor, padding all reviews with the padding token `\"<pad>\"` to equalize their lengths\n","---\n","- next, we need to construct the vocabulary: \n"," - this requires going through the entire training set once, applying `preprocess()`, and using a `Counter` to count the number of occurances of each word:"]},{"cell_type":"code","metadata":{"id":"JXlRbYdsb276","colab_type":"code","colab":{}},"source":["from collections import Counter\n","\n","vocabulary = Counter()\n","for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n","    for review in X_batch:\n","        vocabulary.update(list(review.numpy()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cp5A-Cjyb9kL","colab_type":"text"},"source":["- let's look at the three most common words:"]},{"cell_type":"code","metadata":{"id":"ZTWWKuuIb__9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"7c2d77ea-0dd8-4ce0-ecba-6184f99db181","executionInfo":{"status":"ok","timestamp":1590873294062,"user_tz":420,"elapsed":565,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["vocabulary.most_common()[:3]"],"execution_count":58,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"]},"metadata":{"tags":[]},"execution_count":58}]},{"cell_type":"code","metadata":{"id":"QygVuNa0cRY0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"feb41898-e6d6-4bc6-8538-ad8eaf8b46b9","executionInfo":{"status":"ok","timestamp":1590873365298,"user_tz":420,"elapsed":842,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["len(vocabulary)"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/plain":["53893"]},"metadata":{"tags":[]},"execution_count":59}]},{"cell_type":"markdown","metadata":{"id":"jmk1fnV0cHfD","colab_type":"text"},"source":["- we probably don't need our model to know all the words in the dictionary to yield good performance, so let's truncate the vocabulary, only keeping the 10,000 most common words:"]},{"cell_type":"code","metadata":{"id":"Btw5IWf_cSsA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"b613f30e-0127-4363-d62b-c7663573c368","executionInfo":{"status":"ok","timestamp":1590873398727,"user_tz":420,"elapsed":859,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["vocab_size = 10000\n","truncated_vocabulary = [\n","    word for word, count in vocabulary.most_common()[:vocab_size]]\n","len(truncated_vocabulary)"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10000"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"markdown","metadata":{"id":"3OK7iBLNcgsh","colab_type":"text"},"source":["- now, we need to add a preprocessing step to replace each word with its ID (index in the vocabulary):"]},{"cell_type":"code","metadata":{"id":"OLjJHrZzcroj","colab_type":"code","colab":{}},"source":["words = tf.constant(truncated_vocabulary)\n","word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n","vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n","num_oov_buckets = 1000\n","table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQsKL5SRcyOS","colab_type":"text"},"source":["- we can use this table to find the IDs of a few words:"]},{"cell_type":"code","metadata":{"id":"nSgBiKC1c1pi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ebbefdc9-2a9c-483f-ad75-05e1b27dca53","executionInfo":{"status":"ok","timestamp":1590873517433,"user_tz":420,"elapsed":867,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"],"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]])>"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"fv77fz_zc8ov","colab_type":"text"},"source":["- \"this\", \"movie\", and \"was\" were found in the table, so their IDs are lower than 10,000:\n"," - \"faaaaaantastic\", however, was not found in the table, so it was mapped to one of the oov buckets with an ID greater than or equal to 10,000\n","---\n","- now, we're ready to create the final training set:\n"," - first, we batch the reviews\n"," - second, we convert them to short sequences of words using `preprocess()`\n"," - third, we encode these words using `encode_words()`, which uses the table we just built\n"," - finally, we prefetch the next batch"]},{"cell_type":"code","metadata":{"id":"-yrWIm4BeBk6","colab_type":"code","colab":{}},"source":["def encode_words(X_batch, y_batch):\n","    return table.lookup(X_batch), y_batch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eXHIF4yqeCyw","colab_type":"code","colab":{}},"source":["train_set = datasets[\"train\"].repeat().batch(32).map(preprocess)\n","train_set = train_set.map(encode_words).prefetch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RW5rSj_leIeQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":173},"outputId":"7cd17b67-7d57-4dbc-cee5-e3e785201e34","executionInfo":{"status":"ok","timestamp":1590873926150,"user_tz":420,"elapsed":794,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["for X_batch, y_batch in train_set.take(1):\n","    print(X_batch)\n","    print(y_batch)"],"execution_count":71,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[  22   11   28 ...    0    0    0]\n"," [   6   21   70 ...    0    0    0]\n"," [4099 6881    1 ...    0    0    0]\n"," ...\n"," [  22   12  118 ...  331 1047    0]\n"," [1757 4101  451 ...    0    0    0]\n"," [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n","tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LrWfcc_-eFJw","colab_type":"text"},"source":["- at last, we can create the model and train it:"]},{"cell_type":"code","metadata":{"id":"zfcn18yaecpd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"533d59f9-1922-4d3a-9ba1-1eff5998c6b6","executionInfo":{"status":"ok","timestamp":1590874069480,"user_tz":420,"elapsed":121363,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["embed_size = 128\n","model = keras.models.Sequential([\n","    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n","                           mask_zero=True, # not shown in the book\n","                           input_shape=[None]),\n","    keras.layers.GRU(128, return_sequences=True),\n","    keras.layers.GRU(128),\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"],"execution_count":72,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","781/781 [==============================] - 22s 29ms/step - loss: 0.5305 - accuracy: 0.7281\n","Epoch 2/5\n","781/781 [==============================] - 22s 28ms/step - loss: 0.3459 - accuracy: 0.8554\n","Epoch 3/5\n","781/781 [==============================] - 22s 28ms/step - loss: 0.1913 - accuracy: 0.9319\n","Epoch 4/5\n","781/781 [==============================] - 22s 28ms/step - loss: 0.1341 - accuracy: 0.9536\n","Epoch 5/5\n","781/781 [==============================] - 21s 27ms/step - loss: 0.1011 - accuracy: 0.9623\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rX9-SbPtf0WA","colab_type":"text"},"source":["- the `Embedding` layer converts word IDs into embeddings:\n"," - the embedding matrix must have one row per word ID (`vocab_size + num_oov_buckets`) and one column per embedding dimension (this example uses 128 dimensions, but this is a tunable hyperparameter)\n","- the inputs of the model will be 2D tensors of shape **[batch size, time steps]**, but the output of the `Embedding` layer will be a 3D tensor of shape **[batch size, time steps, embedding size]**\n","- there are two `GRU` layers, with the second one returning only the output of the last time step\n","- the output layer is a single neuron using the sigmoid activation function to output the estimated probability that the review expresses a positive sentiment"]},{"cell_type":"markdown","metadata":{"id":"9Y_-6hCXg7IX","colab_type":"text"},"source":["## Masking"]},{"cell_type":"markdown","metadata":{"id":"uIxUf8PjhHC6","colab_type":"text"},"source":["- we want the model to ignore the padding tokens (whose ID is 0) and focus on the data that matters, so we add `masking_zero=True` when creating the `Embedding` layer\n","- using masking layers and automatic mask propagation works best for simple `Sequential`: \n"," - for example, the following model is identical to the previous model, except it is built using the Functional API and handles masking manually"]},{"cell_type":"code","metadata":{"id":"hn4bHgS1iHVw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"4a4bddb3-4235-4eb2-f4de-49c34c27d3f7","executionInfo":{"status":"ok","timestamp":1590875011773,"user_tz":420,"elapsed":115583,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["K = keras.backend\n","embed_size = 128\n","inputs = keras.layers.Input(shape=[None])\n","mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\n","z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\n","z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\n","z = keras.layers.GRU(128)(z, mask=mask)\n","outputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\n","model = keras.models.Model(inputs=[inputs], outputs=[outputs])\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"],"execution_count":73,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","781/781 [==============================] - 22s 28ms/step - loss: 0.5426 - accuracy: 0.7156\n","Epoch 2/5\n","781/781 [==============================] - 21s 27ms/step - loss: 0.3477 - accuracy: 0.8556\n","Epoch 3/5\n","781/781 [==============================] - 21s 27ms/step - loss: 0.1748 - accuracy: 0.9371\n","Epoch 4/5\n","781/781 [==============================] - 21s 27ms/step - loss: 0.1288 - accuracy: 0.9527\n","Epoch 5/5\n","781/781 [==============================] - 21s 27ms/step - loss: 0.1065 - accuracy: 0.9604\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"44U3SHAMi5ch","colab_type":"text"},"source":["## Reusing Pretrained Embeddings"]},{"cell_type":"markdown","metadata":{"id":"4M6tUQmGi9k9","colab_type":"text"},"source":["- TensorFlow Hub makes it easy to reuse pretrained model components, called ***modules***, in your own models: \n"," - for example, let's use the `nnlm-en-dim50` sentence embedding module, version 1, in our own sentiment analysis model"]},{"cell_type":"code","metadata":{"id":"vw4CjZ5HjhhH","colab_type":"code","colab":{}},"source":["tf.random.set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bRO5N5EImivg","colab_type":"text"},"source":["- TF Hub caches the downloaded files into the local system's temporary directory, however, you may prefer to download them into a more permanent directory to avoid having to download them again after every system clean up: \n"," - for this, set the `TFHUB_CACHE_DIR` environment variable to the directory of your choice"]},{"cell_type":"code","metadata":{"id":"b6W0su84jlZY","colab_type":"code","colab":{}},"source":["TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n","os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R70b5BSnjmbr","colab_type":"code","colab":{}},"source":["import tensorflow_hub as hub\n","\n","model = keras.Sequential([\n","    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n","                   dtype=tf.string, input_shape=[], output_shape=[50]),\n","    keras.layers.Dense(128, activation=\"relu\"),\n","    keras.layers.Dense(1, activation=\"sigmoid\")\n","])\n","model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n","              metrics=[\"accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SsCpNN5kEnN","colab_type":"text"},"source":["- the `hub.KerasLayer` layer downloads the module from the URL\n","- this module is a ***sentence encoder***: it takes strings as input and encodes each one as a single vector (in this case, a 50-dimensional vector): \n"," - internally, it parses the string (splitting words on spaces) and embeds each word using an embedding matrix that was pretrained on a huge corpus: the Google News 7B corpus (seven billion words long)\n","---\n","- next, we can simply load the IMDb reviews dataset and train the model directly: \n"," - no need for preprocessing (except for batching and prefetching) "]},{"cell_type":"code","metadata":{"id":"SEVBv-rBmJK5","colab_type":"code","colab":{}},"source":["import tensorflow_datasets as tfds\n","\n","datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n","train_size = info.splits[\"train\"].num_examples\n","batch_size = 32\n","train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sv98hot_mQI2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"1afd13e9-a7e4-41ea-bd4f-6b12368bc131","executionInfo":{"status":"ok","timestamp":1590876023240,"user_tz":420,"elapsed":30354,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"],"execution_count":78,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","781/781 [==============================] - 5s 6ms/step - loss: 0.5460 - accuracy: 0.7267\n","Epoch 2/5\n","781/781 [==============================] - 5s 6ms/step - loss: 0.5129 - accuracy: 0.7494\n","Epoch 3/5\n","781/781 [==============================] - 5s 6ms/step - loss: 0.5082 - accuracy: 0.7530\n","Epoch 4/5\n","781/781 [==============================] - 4s 6ms/step - loss: 0.5046 - accuracy: 0.7538\n","Epoch 5/5\n","781/781 [==============================] - 5s 7ms/step - loss: 0.5017 - accuracy: 0.7561\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sGi5r8ACnOdK","colab_type":"text"},"source":["# Encoder-Decoder Network for Neural Machine Translation"]},{"cell_type":"markdown","metadata":{"id":"KNwjTNPFnSp9","colab_type":"text"},"source":["- let's look at a neural machine translation model that translates English sentences to French: \n"," - the English sentences are fed to the encoder and the decoder outputs the French translations\n","- the English sentences are reversed before they are fed to the encoder, which ensures that the beginning of the English sentence is fed to the encoder last, which is useful because that's the first thing that the decoder needs to translate\n","---\n","- initially, each word is represented by its ID\n","- next, an `embedding` layer returns the word embedding: \n"," - these word embeddings are what are actually fed to the encoder and decoder\n","---\n","- at each step, the decoder outputs a score for each word in the output vocabulary (French), then the softmax layer turns these scores into probabilities: \n"," - for example, at the first step, the word \"Je\" may have a probability of 20%, \"Tu\" may have a probability of 1%, etc.\n"," - the word with the highest probability is output\n","- this is similar to a regular classification task, so you can train the model using the `\"sparse_categorical_crossentropy\"` loss\n","---\n","- so far, we've assumed that all input sequences have a constant length, but obviously that's not true for sentences\n","- regular tensors have fixed shapes, so they can only contain sentences of the same length\n","- therefore, we must group sentences into buckets of similar lengths using padding for the shorter sentences to ensure all sentences in a bucket have the same length\n","---\n","- we must ignore any output past the end-of-sentence (EOS) token:\n"," - for example, if the model outputs `\"Je bois du lait <eos> oui\"`, the loss for the last word should be ignored\n","---\n","- when the output vocabulary is large, outputting a probability for each and every possible word would be tremendously slow\n","- let's say the target vocabulary contains 50,000 French words: \n"," - the decoder would output 50,000-dimensional vectors\n"," - computing the softmax function over such a large vector would be very computationally intensive\n","- one solution is to only look at the logits for the correct word and for a random sample of incorrect words, then compute an approximation of the loss based only on these logits\n","- you can use `tf.nn.sampled_softmax_loss()` to employ this ***sampled softmax*** technique during training: \n"," - cannot be used during inference as it requires knowing the target"]},{"cell_type":"code","metadata":{"id":"KcVFNYdgwCv2","colab_type":"code","colab":{}},"source":["tf.random.set_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aHvhwW8BwKmd","colab_type":"code","colab":{}},"source":["vocab_size = 100\n","embed_size = 10"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S-q0t2wMOFzY","colab_type":"code","colab":{}},"source":["# doesn't run\n","import tensorflow_addons as tfa\n","\n","encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n","decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n","sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n","\n","embeddings = keras.layers.Embedding(vocab_size, embed_size)\n","encoder_embeddings = embeddings(encoder_inputs)\n","decoder_embeddings = embeddings(decoder_inputs)\n","\n","encoder = keras.layers.LSTM(512, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n","encoder_state = [state_h, state_c]\n","\n","sampler = tfa.seq2seq.sampler.TrainingSampler()\n","\n","decoder_cell = keras.layers.LSTMCell(512)\n","output_layer = keras.layers.Dense(vocab_size)\n","decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n","                                                 output_layer=output_layer)\n","final_outputs, final_state, final_sequence_lengths = decoder(\n","    decoder_embeddings, initial_state=encoder_state,\n","    sequence_length=sequence_lengths)\n","Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n","\n","model = keras.models.Model(\n","    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n","    outputs=[Y_proba])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FTBrUFl4w7go","colab_type":"code","colab":{}},"source":["model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vs2c2Outw8rI","colab_type":"code","colab":{}},"source":["X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n","Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n","X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n","seq_lengths = np.full([1000], 15)\n","\n","history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJfnI158xESo","colab_type":"text"},"source":["## Bidirectional RNNs"]},{"cell_type":"markdown","metadata":{"id":"PE65vGb9xHi2","colab_type":"text"},"source":["- at each time step, a regular recurrent layer only looks at past and present inputs before generating an output:\n"," - it is \"casual\", meaning it cannot look into the future\n","---\n","- for many NLP tasks, such as Neural Machine Translation, it is preferable to look ahead at the next words before encoding a given word\n","- to implement this, run two recurrent layers on the same inputs, one reading the words from left to right and the other reading them from right to left: \n"," - then combine their outputs at each time step by concatenating them\n"," - this is called a ***bidirectional recurrent layer***\n","---\n","- for implementation, wrap a recurrent layer in a `keras.layers.Bidirectional` layer:"]},{"cell_type":"code","metadata":{"id":"vIGvLzEVzjYT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":224},"outputId":"52a8f49c-9fbf-4b10-ee4b-e60fdd034b7a","executionInfo":{"status":"ok","timestamp":1590879481242,"user_tz":420,"elapsed":1437,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}}},"source":["model = keras.models.Sequential([\n","    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n","    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n","])\n","\n","model.summary()"],"execution_count":83,"outputs":[{"output_type":"stream","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","gru_8 (GRU)                  (None, None, 10)          660       \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, None, 20)          1320      \n","=================================================================\n","Total params: 1,980\n","Trainable params: 1,980\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kUWc8ZeKz0HB","colab_type":"text"},"source":["- the `Bidirectional` layer will create a clone of the `GRU` layer (in the reverse direction) and it will run both and concatenate their outputs: \n"," - so although the `GRU` layer has 10 units, the `Bidirectional` layer will output 20 values per time step"]},{"cell_type":"markdown","metadata":{"id":"0M0W8YQ70Gb4","colab_type":"text"},"source":["## Beam Search"]},{"cell_type":"markdown","metadata":{"id":"-F6mKOlC0Hc1","colab_type":"text"},"source":["- ***beam search*** keeps tack of a short list of the *k* most promising sentences, which helps boost performance: \n"," - at each decoder step, it tries to extend them by one word, keeping only the *k* most likely sentences\n"," - *k* is called the ***beam width***"]},{"cell_type":"markdown","metadata":{"id":"tfA_252F17UZ","colab_type":"text"},"source":["## Attention Mechanisms"]},{"cell_type":"markdown","metadata":{"id":"9yIYAXe82Byd","colab_type":"text"},"source":["- ***attention mechanisms*** allow the decoder to focus on the appropriate words (as encoded by the encoder) at each time step: \n"," - for example, at the time step where the decoder needs to output the word \"lait\", it will focus its attention on the word \"milk\"\n"," - this shortens the path from an input word to its translation, which alleviates the impact of short-term memory limitations\n","- attention mechanisms revolutionized neural machine translation (and NLP in general), allowing a significant improve in the state of the art, especially for long sentences (30+ words)\n"]},{"cell_type":"markdown","metadata":{"id":"IguxJzxD4O2C","colab_type":"text"},"source":["## Visual Attention"]},{"cell_type":"markdown","metadata":{"id":"Re9J26iQ4PvR","colab_type":"text"},"source":["- beyond NMT, attention mechanisms can be used for generating image captions using visual attention: \n"," - first, a CNN processes the image and outputs feature maps\n"," - second, a decoder RNN equipped with an attention mechanisms generates the caption one word at a time\n"," - at each decoder time step (each word), the decoder uses the attention model to focus on just the right part of the image"]},{"cell_type":"markdown","metadata":{"id":"JiSrXD3-5YFQ","colab_type":"text"},"source":["## Explainability"]},{"cell_type":"markdown","metadata":{"id":"otOkq3x75iek","colab_type":"text"},"source":["- attention mechanisms can also make it easier to understand what led the model to produce its output: \n"," - this is called ***explainability***, which can be used as a tool to debug a model\n","---\n","- attention mechanisms are so powerful that they can be the sole component of a state-of-the-art model"]},{"cell_type":"markdown","metadata":{"id":"3kghIlv361b_","colab_type":"text"},"source":["## The Transformer Architecture"]},{"cell_type":"markdown","metadata":{"id":"SAtzQxhu65Ow","colab_type":"text"},"source":["- the ***Transformer*** architecture significantly improved the state of the art in NMT without using any recurrent or convolutional layers (only attention mechanisms):\n"," - as a bonus, this architecture was also much faster to train and easier to parallelize"]},{"cell_type":"markdown","metadata":{"id":"GGUEAnAE80Ng","colab_type":"text"},"source":["### Positional Embeddings"]},{"cell_type":"markdown","metadata":{"id":"3IpMvVFB83LX","colab_type":"text"},"source":["- a ***positional embedding*** is a dense vector that encodes the position of a word within a sentence, which can be learned by the model"]},{"cell_type":"code","metadata":{"id":"vMPTNx0U93mg","colab_type":"code","colab":{}},"source":["class PositionalEncoding(keras.layers.Layer):\n","    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n","        super().__init__(dtype=dtype, **kwargs)\n","        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n","        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n","        pos_emb = np.empty((1, max_steps, max_dims))\n","        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n","        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n","        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n","    def call(self, inputs):\n","        shape = tf.shape(inputs)\n","        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v3tyH5c0-BEp","colab_type":"text"},"source":["- now, we can create the first layers of the Transformer:"]},{"cell_type":"code","metadata":{"id":"WPtLz5Wm-Doy","colab_type":"code","colab":{}},"source":["embed_size = 512; max_steps = 500; vocab_size = 10000\n","encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n","decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n","embeddings = keras.layers.Embedding(vocab_size, embed_size)\n","encoder_embeddings = embeddings(encoder_inputs)\n","decoder_embeddings = embeddings(decoder_inputs)\n","positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n","encoder_in = positional_encoding(encoder_embeddings)\n","decoder_in = positional_encoding(decoder_embeddings)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T5vyueiL-FfI","colab_type":"text"},"source":["### Multi-Head Attention"]},{"cell_type":"markdown","metadata":{"id":"Axh6CPyF-I2v","colab_type":"text"},"source":["- the ***Multi-Head Attention*** layer is based on the ***Scaled Dot-Product Attention*** layer\n","---\n","- if we ignore the skip connections, the layer normalization layers, the Feed Forward blocks, and the fact that this is Scaled Dot-Product Attention (not exactly Multi-Head Attention), then the rest of the transformer model can be implemented like so:"]},{"cell_type":"code","metadata":{"id":"e9cSNhRQ_PJW","colab_type":"code","colab":{}},"source":["Z = encoder_in\n","for N in range(6):\n","    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n","\n","encoder_outputs = Z\n","Z = decoder_in\n","for N in range(6):\n","    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n","    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n","\n","outputs = keras.layers.TimeDistributed(\n","    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AfOiFwsD_auX","colab_type":"text"},"source":["- the Multi-Head Attention layer is just a bunch of Scaled Dot-Product Attention layers, each preceded by a linear transformation of the values, keys, and queries\n","- all the outputs are concatenated, and they go through a final linear transformation\n","---\n","- TensorFlow's great tutorial for building a Transformer model for language understanding: https://www.tensorflow.org/tutorials/text/transformer"]},{"cell_type":"markdown","metadata":{"id":"OOYqaPM0DFAV","colab_type":"text"},"source":["# Exercises"]},{"cell_type":"markdown","metadata":{"id":"jnoznCMaDGNT","colab_type":"text"},"source":["*1) What are the pros and cons of using a stateful RNN versues a stateless RNN?*\n","\n","*2) Why do people use Encoder-Decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?*\n","\n","*3) How can you deal with variable-length input sequences? What about variable-length output sequences?*\n","\n","*4) What is beam search and why would you use it? What tool can you use to implement it?*\n","\n","*5) What is an attention mechanism? How does it help?*\n","\n","*6) What is the most important layer in the Transformer architecture? What is its purpose?*\n","\n","*7) When would you need to use sampled softmax?*"]},{"cell_type":"markdown","metadata":{"id":"yk1gtXTMDy5c","colab_type":"text"},"source":["1) Stateful RNNs can capture longer-term patterns, however, they are much harder to implement.\n","\n","2) A sequence-to-sequence RNN would start translating a sequence immediately after reading the first word, while an Encoder-Decoder RNN will read the whole sentence before translating it.\n","\n","3) Variable-length input sequences can be handled by padding the shorter sequences to equalize the lengths of all the sequences in a batch. For variable-length output sequences, train the model to output an end-of-sequence token at the end of each sequence.\n","\n","4) Beam search is a technique used to improve the performance of a trained Encoder-Decoder model, which can be implemented using TensorFlow Addons.\n","\n","5) An attention mechanism is a technique initially used in Encoder-Decoder models to give the decoder more direct access to the input sequence, allowing it to process longer input sequences. In addition, it makes the model easier to debug and serves as the core of the Transformer architecture (in the Multi-Head Attention layers).\n","\n","6) The most important layer in the Transformer architecutre is the Multi-Head Attention layer, which allows the model to identify which words are most aligned with each other, and then improve each word's representation using these contextual clues.\n","\n","7) Sampled softmax is used when training a classification model when there are many classes (thousands) as it speeds up training considerably."]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Custom Models and Training with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -U tqdm\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so far, we've only used TensorFlow's high-level API, `tf.keras`, but it's gotten us pretty far\n",
    "- we've built various neural network architectures, including regression and classification nets, Wide & Deep nets, and self-normalizing nets, using all sorts of techniques, such as Batch Normalization, dropout, and learning rate schedules\n",
    "- in fact, 95% of the use cases you will encounter will not require anything more than `tf.keras` and `tf.data`\n",
    "---\n",
    "- but now it's time to dive deeper into TensorFlow and take a look at its lower-level Python API, which will be useful when you need extra control to write custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Tour of TensorFlow\n",
    "- the most popular Deep Learning library, particularly well suited and fine-tuned for large-scale Machine Learning\n",
    "- developed by Google's Brain team and it powers many of Google's large-scale services\n",
    "- very similar core to NumPy, but with GPU support\n",
    "- take some time to browse through TensorFlow's API as you will find that it is quite rich and well documented\n",
    "- for example, I had no idea `tf.estimator` was one of TensorFlow's *High-Level Deep Learning APIs* (in addition to `tf.keras`)\n",
    "---\n",
    "- at the lowest level, each TensorFlow operation (*op*) is implemented using highly efficient C++ code\n",
    "- many operations have multiple implementations called *kernels*, where each kernel is dedicated to a specific device type (CPUs, GPUs, or even TPUs)\n",
    "- GPUs dramatically speed up computations by splitting them into many smaller chunks and running them in parallel across many GPU threads\n",
    "- you can even run your models directly in your browser with *TensorFlow.js*\n",
    "---\n",
    "- there's more to TensorFlow than the library\n",
    "- there's TensorBoard for visualization\n",
    "- there's TensorFlow Extended (TFX), which is a set of libraries built by Google to productionize TensorFlow projects\n",
    "- Google's *TensorFlow Hub* provides a way to easily download and reuse pretrained neural networks\n",
    "- you can also get many neural network architectures, some of them pretrtained, in TensorFlow's model garden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and Operations\n",
    "- TensorFlow's API revolves around *tensors*, which flow from operation to operation, hence the name Tensor*Flow*\n",
    "- a tensor is very similar to a NumPy `ndarray`: it is usually a multidimensional array, but it can also hold a scalar (a simple value, such as `42`)\n",
    "---\n",
    "- you can create a tensor with `tf.constant()`\n",
    "- for example, here is a tensor representing a matrix with 2 rows and 3 columns of floats: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 165
    }
   ],
   "source": [
    "tf.constant([[1., 2., 3.], [4., 5., 6.]]) # matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=int32, numpy=42>"
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "tf.constant(42) # scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- similar to an `ndarray`, a `tf.Tensor` has a shape and data type (`dtype`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(TensorShape([2, 3]), tf.float32)"
     },
     "metadata": {},
     "execution_count": 167
    }
   ],
   "source": [
    "t = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
    "t.shape, t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- indexing works much like in NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[2., 3.],\n       [5., 6.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 168
    }
   ],
   "source": [
    "t[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[2.],\n       [5.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 169
    }
   ],
   "source": [
    "t[..., 1, tf.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most importantly, all sorts of tensor operations are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[11., 12., 13.],\n       [14., 15., 16.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 170
    }
   ],
   "source": [
    "t + 10 # added 10 to each float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 1.,  4.,  9.],\n       [16., 25., 36.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 171
    }
   ],
   "source": [
    "tf.square(t) # squared each float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[14., 32.],\n       [32., 77.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 172
    }
   ],
   "source": [
    "t @ tf.transpose(t) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you will find all the basic math operations you need and most operations that you can find in NumPy \n",
    "- some functions, however, have a different name than in NumPy\n",
    "- for example, in TensorFlow, you must write `tf.transpose(t)`; you cannot just write `t.T` like in NumPy\n",
    "- the reason is that the `tf.transpose()` function does not do exactly the same thing as NumPy's `T` attribute\n",
    "- in TensorFlow, a new tensor is created with its own copy of the transposed data, while in NumPy, `t.T` is just a transposed view on the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras' Low-Level API\n",
    "- the Keras API has its own low-level API, located in `keras.backend`\n",
    "- it includes functions like `square()`, `exp()`, and `sqrt()`\n",
    "- in `tf.keras`, these functions generally just call the corresponding TensorFlow operations\n",
    "- here is a small example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[11., 26.],\n       [14., 35.],\n       [19., 46.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 173
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "K = keras.backend\n",
    "K.square(K.transpose(t)) + 10 # squares then adds 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and NumPy\n",
    "- tensors play nice with NumPy, as you can create a tensor from a NumPy array and vice versa\n",
    "- you can even apply TensorFlow operations to NumPy arrays and NumPy operations to tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>"
     },
     "metadata": {},
     "execution_count": 174
    }
   ],
   "source": [
    "a = np.array([2., 4., 5.]) \n",
    "tf.constant(a) # ndarry --> tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "t.numpy() # tensor --> ndarry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "np.array(t) # tensor --> ndarry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([ 4., 16., 25.])>"
     },
     "metadata": {},
     "execution_count": 177
    }
   ],
   "source": [
    "tf.square(a) # tensorflow operation on an ndarry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1.,  4.,  9.],\n       [16., 25., 36.]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 178
    }
   ],
   "source": [
    "np.square(t) # numpy operation on a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- notice that NumPy uses 64-bit precision by default, while TensorFlow uses 32-bit\n",
    "- this is because 32-bit precision is generally more than enough for neural networks, plus it runs faster and uses less RAM\n",
    "- so when you create a tensor from a NumPy array, make sure to set the `dtype=tf.float32`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Conversions\n",
    "- type conversions can significantly hurt performance, and they can easily go unnoticed when they are done automatically\n",
    "- to avoid this, TensorFlow does not perform any type conversions automatically\n",
    "- for example, you cannot add a float tensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: add/\n"
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40) # cannot perform float tensor + integer tensor\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a double tensor [Op:AddV2] name: add/\n"
    }
   ],
   "source": [
    "try:\n",
    "    tf.constant(2.0) + tf.constant(40., dtype=tf.float64) # cannot add tensors of different float dtypes\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this may be annoying at first, but remember that it's for your own good\n",
    "- even so, you can use `tf.cast()` when you really need to convert types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=42.0>"
     },
     "metadata": {},
     "execution_count": 181
    }
   ],
   "source": [
    "t2 = tf.constant(40., dtype=tf.float64)\n",
    "tf.constant(2.0) + tf.cast(t2, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "- the `tf.Tensor` values we've seen so far are immutable, meaning you cannot modify them\n",
    "- **therefore, we cannot use regular tensors to implement weights in a neural network (as they must be tweaked by backpropagation)**\n",
    "- plus, other parameters may also need to change over time (momentum optimizer keeps track of past gradients)\n",
    "- what we need is a `tf.Variable`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\narray([[1., 2., 3.],\n       [4., 5., 6.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 182
    }
   ],
   "source": [
    "v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a `tf.Variable` acts similar to a `tf.Tensor`: you can perform the same operations with it, it plays nicely with NumPy as well, and it is just as picky with types\n",
    "- but, a `tf.Variable` can also be modified in place using the `assign()` method \n",
    "- you can also modify individual cells (or slices) by using the cell's (or slice's) `assign()` method or by using the `scatter_update()` or `scatter_nd_update()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2.,  4.,  6.],\n       [ 8., 10., 12.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 183
    }
   ],
   "source": [
    "v.assign(2 * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2., 42.,  6.],\n       [ 8., 10., 12.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 184
    }
   ],
   "source": [
    "v[0, 1].assign(42) # v[0, 1] = 1st row, 2nd column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[ 2., 42.,  0.],\n       [ 8., 10.,  1.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 185
    }
   ],
   "source": [
    "v[:, 2].assign([0., 1.]) # all rows, 3rd colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "'ResourceVariable' object does not support item assignment\n"
    }
   ],
   "source": [
    "try:\n",
    "    v[1] = [7., 8., 9.]\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[100.,  42.,   0.],\n       [  8.,  10., 200.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 187
    }
   ],
   "source": [
    "v.scatter_nd_update(indices=[[0, 0], [1, 2]],\n",
    "                    updates=[100., 200.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Variable 'UnreadVariable' shape=(2, 3) dtype=float32, numpy=\narray([[4., 5., 6.],\n       [1., 2., 3.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 188
    }
   ],
   "source": [
    "sparse_delta = tf.IndexedSlices(values=[[1., 2., 3.], [4., 5., 6.]],\n",
    "                                indices=[1, 0])\n",
    "v.scatter_update(sparse_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in practice, you will rarely have to create variables manually, since Keras provides an `add_weight()` method that will take care of it for you, as we will see\n",
    "- moreover, model parameters will generally be updated directly by the optimizers, so you will rarely need to update variables manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Data Structures: \n",
    "- TensorFlow supports several other data structures, including the following:\n",
    "---\n",
    "- ***Sparse Tensors*** (`tf.SparseTensor`): efficiently represent tensors containing mostly zeros\n",
    "- the `tf.sparse` package contains operations for sparse tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SparseTensor(indices=tf.Tensor(\n[[0 1]\n [1 0]\n [2 3]], shape=(3, 2), dtype=int64), values=tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
    }
   ],
   "source": [
    "s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]],\n",
    "                    values=[1., 2., 3.],\n",
    "                    dense_shape=[3, 4])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\narray([[0., 1., 0., 0.],\n       [2., 0., 0., 0.],\n       [0., 0., 0., 3.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 190
    }
   ],
   "source": [
    "tf.sparse.to_dense(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x21ed8c5ca90>"
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "s2 = s * 2.0\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "unsupported operand type(s) for +: 'SparseTensor' and 'float'\n"
    }
   ],
   "source": [
    "try:\n",
    "    s3 = s + 1.\n",
    "except TypeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[ 30.,  40.],\n       [ 20.,  40.],\n       [210., 240.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 193
    }
   ],
   "source": [
    "s4 = tf.constant([[10., 20.], [30., 40.], [50., 60.], [70., 80.]])\n",
    "tf.sparse.sparse_dense_matmul(s, s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SparseTensor(indices=tf.Tensor(\n[[0 2]\n [0 1]], shape=(2, 2), dtype=int64), values=tf.Tensor([1. 2.], shape=(2,), dtype=float32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64))\n"
    }
   ],
   "source": [
    "s5 = tf.SparseTensor(indices=[[0, 2], [0, 1]],\n",
    "                     values=[1., 2.],\n",
    "                     dense_shape=[3, 4])\n",
    "print(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "indices[1] = [0,1] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SparseToDense]\n"
    }
   ],
   "source": [
    "try:\n",
    "    tf.sparse.to_dense(s5)\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\narray([[0., 2., 1., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 196
    }
   ],
   "source": [
    "s6 = tf.sparse.reorder(s5)\n",
    "tf.sparse.to_dense(s6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Tensor Arrays* (`tf.TensorArray`): are lists of tensors\n",
    "- they have a fixed size by default, but can optimally be made dynamic\n",
    "- all tensors they contain must have the same shape and data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = tf.TensorArray(dtype=tf.float32, size=3)\n",
    "# all tensors have the same shape and data type\n",
    "array = array.write(0, tf.constant([1., 2.]))\n",
    "array = array.write(1, tf.constant([3., 10.]))\n",
    "array = array.write(2, tf.constant([5., 7.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([ 3., 10.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 198
    }
   ],
   "source": [
    "array.read(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\narray([[1., 2.],\n       [0., 0.],\n       [5., 7.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "array.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 3.], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 200
    }
   ],
   "source": [
    "mean, variance = tf.nn.moments(array.stack(), axes=0)\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([4.6666665, 8.666667 ], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 201
    }
   ],
   "source": [
    "variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Ragged Tensors*** (`tf.RaggedTensor`): represent lists of lists of tensors\n",
    "- every tensor has the same shape and data type\n",
    "- the `tf.ragged` package contains operations for ragged tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tf.constant([\"Café\", \"Coffee\", \"caffè\", \"咖啡\"]) # array of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2])>"
     },
     "metadata": {},
     "execution_count": 203
    }
   ],
   "source": [
    "tf.strings.length(p, unit=\"UTF8_CHAR\") # length of each string in array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]>"
     },
     "metadata": {},
     "execution_count": 204
    }
   ],
   "source": [
    "r = tf.strings.unicode_decode(p, \"UTF8\") # ragged tensor example\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this was just preface: now, onto ragged tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor([ 67 111 102 102 101 101], shape=(6,), dtype=int32)\n"
    }
   ],
   "source": [
    "print(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tf.RaggedTensor [[67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232]]>\n"
    }
   ],
   "source": [
    "print(r[1:3]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857], [65, 66], [], [67]]>\n"
    }
   ],
   "source": [
    "r2 = tf.ragged.constant([[65, 66], [], [67]])\n",
    "print(tf.concat([r, r2], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101, 71], [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]>\n"
    }
   ],
   "source": [
    "r3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]])\n",
    "print(tf.concat([r, r3], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'DEF', b'G', b'', b'HI'], dtype=object)>"
     },
     "metadata": {},
     "execution_count": 209
    }
   ],
   "source": [
    "tf.strings.unicode_encode(r3, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(4, 6), dtype=int32, numpy=\narray([[   67,    97,   102,   233,     0,     0],\n       [   67,   111,   102,   102,   101,   101],\n       [   99,    97,   102,   102,   232,     0],\n       [21654, 21857,     0,     0,     0,     0]])>"
     },
     "metadata": {},
     "execution_count": 210
    }
   ],
   "source": [
    "r.to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***String Tensors*** are regular tensors of type `tf.string`\n",
    "- these represent byte strings, not Unicode strings, so if you create a string tensor using a Unicode string (like `\"toucan\"`), then it will get encoded to UTF-8 automatically (`b\"caf\\xc3\\xa9\"`) <-- (byte string)\n",
    "- alternatively, you can represent Unicode strings using tensors of type `tf.int32`, where each item represents a Unicode code point (`[99, 97, 102, 233]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=string, numpy=b'hello world'>"
     },
     "metadata": {},
     "execution_count": 211
    }
   ],
   "source": [
    "tf.constant(b\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>"
     },
     "metadata": {},
     "execution_count": 212
    }
   ],
   "source": [
    "tf.constant(\"café\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233])>"
     },
     "metadata": {},
     "execution_count": 213
    }
   ],
   "source": [
    "u = tf.constant([ord(c) for c in \"café\"])\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=int32, numpy=4>"
     },
     "metadata": {},
     "execution_count": 214
    }
   ],
   "source": [
    "b = tf.strings.unicode_encode(u, \"UTF-8\")\n",
    "tf.strings.length(b, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([ 99,  97, 102, 233])>"
     },
     "metadata": {},
     "execution_count": 215
    }
   ],
   "source": [
    "tf.strings.unicode_decode(b, \"UTF-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Sets*** are represented as regular tensors (or sparse tensors)\n",
    "- for example, `tf.constant([[1, 2], [3, 4]])` represents the two sets `{1, 2}` and `{3, 4}`\n",
    "- more generally, each set is represented by a vector in the tensor's last axis\n",
    "- you can manipulate sets using operations in the `tf.sets` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 6), dtype=int32, numpy=\narray([[ 2,  3,  4,  5,  6,  7],\n       [ 0,  7,  9, 10,  0,  0]])>"
     },
     "metadata": {},
     "execution_count": 216
    }
   ],
   "source": [
    "set1 = tf.constant([[2, 3, 5, 7], [7, 9, 0, 0]])\n",
    "set2 = tf.constant([[4, 5, 6], [9, 10, 0]])\n",
    "tf.sparse.to_dense(tf.sets.union(set1, set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\narray([[2, 3, 7],\n       [7, 0, 0]])>"
     },
     "metadata": {},
     "execution_count": 217
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.difference(set1, set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[5, 0],\n       [0, 9]])>"
     },
     "metadata": {},
     "execution_count": 218
    }
   ],
   "source": [
    "tf.sparse.to_dense(tf.sets.intersection(set1, set2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***Queues*** store tensors across multiple steps \n",
    "- TensorFlow offers various kinds of queues: simple First In, First Out (FIFO) queues (FIFQueue), queues that can prioritize some items (`PriorityQueue`), shuffle their items (`RandomShuffleQueue`), and batch items of different shapes by padding (`PaddingFIFOQueue`)\n",
    "- these classes are all in the `tf.queue.package`\n",
    "---\n",
    "- with tensors, operations, variables, and various data structures at your disposal, you are now ready to customize your models and training algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Functions\n",
    "- let's start by creating a custom loss function, which is a simple and common use case\n",
    "---\n",
    "- **suppose you want to train a regression model, but your training set is too noisy**\n",
    "- of course, you begin by trying to clean up your dataset by removing or fixing outliers, but that turns out to be insufficient; the dataset is still noisy\n",
    "- **this is a good time to use the Huber loss instead of good old MSE**\n",
    "- the Huber loss is not part of the official Keras API, but it is available in `tf.keras`\n",
    "- for now, let's pretend it's not there: implementing it is easy as pie\n",
    "- just create a function that takes the labels and predictions as arguments, and use TensorFlow operations to compute every instance's loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# training + test set\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "\n",
    "# validation set\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "# scaling datasets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_fn(y_true, y_pred):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = tf.abs(error) < 1\n",
    "    squared_loss = tf.square(error) / 2\n",
    "    linear_loss  = tf.abs(error) - 0.5\n",
    "    return tf.where(is_small_error, squared_loss, linear_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it is also preferable to return a tensor containing one loss per instance, rather than returning the mean loss\n",
    "- this way, Keras can apply class weights or sample weights when requested\n",
    "---\n",
    "- now you can use this loss when you compile the Keras model, then train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=huber_fn, optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 79us/sample - loss: 0.6247 - mae: 0.9969 - val_loss: 0.2884 - val_mae: 0.5913\nEpoch 2/2\n11610/11610 [==============================] - 0s 39us/sample - loss: 0.2193 - mae: 0.5176 - val_loss: 0.2361 - val_mae: 0.5232\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21ed68a0d68>"
     },
     "metadata": {},
     "execution_count": 223
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- and that's it: for each batch during training, Keras will call the `huber_fn()` function to compute the loss and use it to perform a Gradient Descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models That Contain Custom Components (custom loss functions continued)\n",
    "- saving a model containing a custom loss function works fine as Keras saves the name of the function\n",
    "- whenever you load it, you'll need to provide a dictionary that maps the function name to the actual function\n",
    "- more generally, when you load a model containing custom objects, you need to map the names to the objects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_loss.h5\",\n",
    "                                custom_objects={\"huber_fn\": huber_fn}) # mapping the names to the objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n10016/11610 [========================>.....] - ETA: 0s - loss: 0.2042 - mae: 0.49611610/11610 [==============================] - 1s 65us/sample - loss: 0.2056 - mae: 0.4982 - val_loss: 0.2170 - val_mae: 0.5037\nEpoch 2/2\n11610/11610 [==============================] - 0s 36us/sample - loss: 0.2006 - mae: 0.4911 - val_loss: 0.2097 - val_mae: 0.4908\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21ed9095080>"
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with the current implementaiton, any error between -1 and 1 is considered \"small\"\n",
    "- **however, if you want to set a different threshold, one solution is to create a function that creates a configured loss function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# editing huber loss function code to be able to set threshold\n",
    "def create_huber(threshold=1.0):\n",
    "    def huber_fn(y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = threshold * tf.abs(error) - threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    return huber_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=create_huber(2.0), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unfortunately, when you save the model, the `threshold` will not be saved\n",
    "- this means that you will have to specify the `threshold` value when loading the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 74us/sample - loss: 0.2229 - mae: 0.4893 - val_loss: 0.2525 - val_mae: 0.4973\nEpoch 2/2\n11610/11610 [==============================] - 0s 42us/sample - loss: 0.2189 - mae: 0.4856 - val_loss: 0.2338 - val_mae: 0.4765\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21edb9342b0>"
     },
     "metadata": {},
     "execution_count": 229
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss_threshold_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\",\n",
    "                                custom_objects={\"huber_fn\": create_huber(2.0)}) # manually setting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 63us/sample - loss: 0.2148 - mae: 0.4796 - val_loss: 0.2111 - val_mae: 0.4713\nEpoch 2/2\n11610/11610 [==============================] - 0s 36us/sample - loss: 0.2123 - mae: 0.4775 - val_loss: 0.1970 - val_mae: 0.4534\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21edbbb3198>"
     },
     "metadata": {},
     "execution_count": 232
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can solve this by creating a subclass of the `keras.losses.Loss` class, and then implementing its `get_config()` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuberLoss(keras.losses.Loss):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        self.threshold = threshold\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, y_true, y_pred):\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's walk through the code above: \n",
    "---\n",
    "- the constructor (`__init__()`) accepts `**kwargs` and passes them to the parent constructor\n",
    "- the `call()` method takes the labels and predictions, computes all the instance losses, and returns them\n",
    "- the `get_config()` method returns a dictionary mapping each hyperparameter name to its value\n",
    "---\n",
    "- note: putting `*args` and/or `**kwargs` as the last items in your function definition’s argument list allows that function to accept an arbitrary number of arguments and/or keyword arguments\n",
    "---\n",
    "- you can then use any instance of this class when you compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=HuberLoss(2.), optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 66us/sample - loss: 0.6934 - mae: 0.8818 - val_loss: 0.3288 - val_mae: 0.5451\nEpoch 2/2\n11610/11610 [==============================] - 0s 38us/sample - loss: 0.2402 - mae: 0.5077 - val_loss: 0.2315 - val_mae: 0.4861\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21edbcb9c88>"
     },
     "metadata": {},
     "execution_count": 236
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_loss_class.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now, when you save the model, the threshold will be saved along with it\n",
    "- now, when you load the model, you just need to map the class name to the class itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code doesn't work\n",
    "# model = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", \n",
    "#                                 custom_objects={\"HuberLoss\": HuberLoss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- that's it for losses\n",
    "- just as simple are custom activation functions, initializers, regularizers, and constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Activation Functions, Initializers, Regularizers, and Constraints\n",
    "- most Keras functionalities can be customized in a very similar way\n",
    "- **generally, you will just need to write a simple function with the appropriate inputs and outputs (as already seen)**\n",
    "- here are examples of a custom activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom activation function\n",
    "def my_softplus(z): # return value is just tf.nn.softplus(z)\n",
    "    return tf.math.log(tf.exp(z) + 1.0)\n",
    "\n",
    "# custom Glorot initializer\n",
    "def my_glorot_initializer(shape, dtype=tf.float32):\n",
    "    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n",
    "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
    "\n",
    "# custom l1 regularizer\n",
    "def my_l1_regularizer(weights):\n",
    "    return tf.reduce_sum(tf.abs(0.01 * weights))\n",
    "\n",
    "# custom constraint that ensures weights are all positive\n",
    "def my_positive_weights(weights): # return value is just tf.nn.relu(weights)\n",
    "    return tf.where(weights < 0., tf.zeros_like(weights), weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the arguments depend on the type of custom function\n",
    "- these custom functions can be used normally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(1, activation=my_softplus,\n",
    "                           kernel_initializer=my_glorot_initializer,\n",
    "                           kernel_regularizer=my_l1_regularizer,\n",
    "                           kernel_constraint=my_positive_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=my_l1_regularizer,\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 66us/sample - loss: 1.4486 - mae: 0.8727 - val_loss: 2.4208 - val_mae: 0.5681\nEpoch 2/2\n11610/11610 [==============================] - 0s 37us/sample - loss: 0.5848 - mae: 0.5260 - val_loss: 1.6040 - val_mae: 0.5122\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21edbe61588>"
     },
     "metadata": {},
     "execution_count": 245
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_many_custom_parts.h5\") # saving the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model( # loading the custom model\n",
    "    \"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"my_l1_regularizer\": my_l1_regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the `activation` function will be applied to the output of this `Dense` layer (result will be passed to next layer)\n",
    "- the layer's weights will be initialized using the values returned by the `kernel_initializer`\n",
    "- at each training step, the weights will be passed to the `kernel_regularizer` to compute the regularization loss\n",
    "- finally, the `kernel_constraint` function will be called after each training step, and the layer's weights will be replaced by the constrained weights\n",
    "---\n",
    "- if a function has hyperparameters that need to be saved along with the model, then you will want to subclass the appropriate class like we did with our custom loss function\n",
    "- here is a simple class for $l_1$ regularization that saves its `factor` hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyL1Regularizer(keras.regularizers.Regularizer): # subclassed the appropriate class\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    def __call__(self, weights):\n",
    "        return tf.reduce_sum(tf.abs(self.factor * weights))\n",
    "    def get_config(self):\n",
    "        return {\"factor\": self.factor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1, activation=my_softplus,\n",
    "                       kernel_regularizer=MyL1Regularizer(0.01),\n",
    "                       kernel_constraint=my_positive_weights,\n",
    "                       kernel_initializer=my_glorot_initializer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 66us/sample - loss: 1.4486 - mae: 0.8727 - val_loss: 2.4208 - val_mae: 0.5681\nEpoch 2/2\n11610/11610 [==============================] - 0s 38us/sample - loss: 0.5848 - mae: 0.5260 - val_loss: 1.6040 - val_mae: 0.5122\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21edc12f908>"
     },
     "metadata": {},
     "execution_count": 252
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_many_custom_parts.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\n",
    "    \"my_model_with_many_custom_parts.h5\",\n",
    "    custom_objects={\n",
    "       \"MyL1Regularizer\": MyL1Regularizer,\n",
    "       \"my_positive_weights\": my_positive_weights,\n",
    "       \"my_glorot_initializer\": my_glorot_initializer,\n",
    "       \"my_softplus\": my_softplus,\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note\n",
    "- when subclassing the appropriate class, we must:\n",
    "- implement the `call()` method for losses, layers, activation functions, and models\n",
    "- implement the `__call__()` method for regularizers, initializers, and constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Metrics\n",
    "- **losses and metrics are not the same**\n",
    "- losses (cross entropy) are used by Gradient Descent to *train* a model, so they must be differentiable, and their gradients should not be 0 everywhere\n",
    "- metrics (accuracy) are used to *evaluate* a model, so they can be non-differentiable or have 0 gradients everywhere\n",
    "---\n",
    "- in most cases, defining a custom metric function is similar to defining a custom loss function\n",
    "- in fact, we can even use the Huber loss function we created earlier as a metric (not a popular choice: MSE or MAE is preferred):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)]) # using Huber loss function we created earlier as a metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 57us/sample - loss: 2.0326 - huber_fn: 0.9038\nEpoch 2/2\n11610/11610 [==============================] - 0s 32us/sample - loss: 0.5862 - huber_fn: 0.2682\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x21edd3b0208>"
     },
     "metadata": {},
     "execution_count": 258
    }
   ],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for each batch during training, Keras will compute this metric and keep track of its mean since the start of the epoch\n",
    "- most of the time, this is exactly what you want, but not always\n",
    "---\n",
    "### Streaming Metric\n",
    "- suppose a model made 5 positive predictions in the 1st batch, 4 of which were correct: that's 80% precision\n",
    "- the model then made 3 positive predictions in the second batch, but they were all incorrect: that's 0% precision\n",
    "- if you compute the mean of these precisions, you get 40%, but that is not the model's actual precision over the 2 batches: there were a total of 4 true positives out of 8 total predictions, so the overall precision is really 50% \n",
    "- we need an object to keep track of the number of true/false positives and compute their ratio when requested: this is *precisely* what the `keras.metrics.Precision` class does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.8>"
     },
     "metadata": {},
     "execution_count": 259
    }
   ],
   "source": [
    "precision = keras.metrics.Precision()\n",
    "# passing in labels and predictions \n",
    "# 1st batch\n",
    "precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1]) # 4/5 positive predictions correct = 80% precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
     },
     "metadata": {},
     "execution_count": 260
    }
   ],
   "source": [
    "# 2nd batch\n",
    "precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0]) # 0/3 positive predictions correct = 0% precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we created a `Precision` object and used it like a function, passing it the labels and the predictions for the first and second batches\n",
    "- **after the first batch, it returns a precision of 80%, and after the second batch, it returns a precision of 50%**\n",
    "- this is called a *streaming metric* as it is gradually updated after each batch\n",
    "--- \n",
    "- at any point, we can call the `result()` method to get the current value of this metric\n",
    "- we can also look at its variables (tracking the number of true/false positives) by using the `variables` attribute, and we can reset these variables using the `reset_states()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.5>"
     },
     "metadata": {},
     "execution_count": 261
    }
   ],
   "source": [
    "precision.result() # result method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<tf.Variable 'true_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>,\n <tf.Variable 'false_positives:0' shape=(1,) dtype=float32, numpy=array([4.], dtype=float32)>]"
     },
     "metadata": {},
     "execution_count": 262
    }
   ],
   "source": [
    "precision.variables # variables attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision.reset_states() # reset_states() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- if you want to create such a streaming metric, create a subclass of the `keras.metrics.Metric` class\n",
    "- here is a simple example that keeps track of the toal Huber loss and the number of instances seen so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a streaming metric\n",
    "class HuberMetric(keras.metrics.Metric):\n",
    "    def __init__(self, threshold=1.0, **kwargs):\n",
    "        super().__init__(**kwargs) # handles base args (e.g., dtype)\n",
    "        self.threshold = threshold\n",
    "        #self.huber_fn = create_huber(threshold) # TODO: investigate why this fails\n",
    "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
    "    def huber_fn(self, y_true, y_pred): # workaround\n",
    "        error = y_true - y_pred\n",
    "        is_small_error = tf.abs(error) < self.threshold\n",
    "        squared_loss = tf.square(error) / 2\n",
    "        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n",
    "        return tf.where(is_small_error, squared_loss, linear_loss)\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        metric = self.huber_fn(y_true, y_pred)\n",
    "        self.total.assign_add(tf.reduce_sum(metric))\n",
    "        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n",
    "    def result(self):\n",
    "        return self.total / self.count\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"threshold\": self.threshold}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we learned that some metrics, like precision, cannot always be averaged over batches, in which the only option is to implement a streaming metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Layers\n",
    "- if a model is a sequence of layers (A, B, C, A, B, C, A, B, C), then you might want to define a custom layer D containing layers A, B, C, so your model would simply be D, D, D\n",
    "- you may also want to build an architecture with an exotic layer for which TensorFlow does not provide a default implementation\n",
    "---\n",
    "- if you want to build a custom layer without any weights, write a function and wrap it in a `keras.layers.Lambda` layer\n",
    "- the following layer, for example, will apply the exponential function to its inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.36787945, 1.        , 2.7182817 ], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 266
    }
   ],
   "source": [
    "exponential_layer([-1., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this custom layer can be used like any other layer and even as an activation function\n",
    "- the exponential layer is sometimes used in the output layer of a regression model when the values to predict have very different scales (0.001, 10., 1,000.)\n",
    "---\n",
    "- like with everything else, to build a custom stateful layer (a layer with weights), you need to create a subclass of the `keras.layers.Layer` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/5\n11610/11610 [==============================] - 1s 62us/sample - loss: nan - val_loss: nan\nEpoch 2/5\n11610/11610 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\nEpoch 3/5\n11610/11610 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\nEpoch 4/5\n11610/11610 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\nEpoch 5/5\n11610/11610 [==============================] - 0s 36us/sample - loss: nan - val_loss: nan\n5160/5160 [==============================] - 0s 25us/sample - loss: nan\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "nan"
     },
     "metadata": {},
     "execution_count": 267
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    keras.layers.Dense(1),\n",
    "    exponential_layer # using exponential layer as an output layer\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=5,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.activation = keras.activations.get(activation)\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n",
    "            initializer=\"glorot_normal\")\n",
    "        self.bias = self.add_weight(\n",
    "            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.activation(X @ self.kernel + self.bias)\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units])\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"units\": self.units,\n",
    "                \"activation\": keras.activations.serialize(self.activation)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can now use a `MyDense` layer just like any other layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    MyDense(30, activation=\"relu\", input_shape=input_shape),\n",
    "    MyDense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples, validate on 3870 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 61us/sample - loss: 2.1425 - val_loss: 1.2779\nEpoch 2/2\n11610/11610 [==============================] - 0s 35us/sample - loss: 0.6367 - val_loss: 0.5267\n5160/5160 [==============================] - 0s 19us/sample - loss: 0.5359\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.5359284612559533"
     },
     "metadata": {},
     "execution_count": 271
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model.fit(X_train_scaled, y_train, epochs=2,\n",
    "          validation_data=(X_valid_scaled, y_valid))\n",
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_with_a_custom_layer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_with_a_custom_layer.h5\",\n",
    "                                custom_objects={\"MyDense\": MyDense})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to create a layer with multiple inputs, the argument to the `call()` method should be a tuple containing all the inputs\n",
    "- similarly the argument to the `compute_output_shape()` method should be a tuple containing each input's batch shape\n",
    "- the following toy layer takes two inputs and returns three outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMultiLayer(keras.layers.Layer):\n",
    "    def call(self, X):\n",
    "        X1, X2 = X\n",
    "        return X1 + X2, X1 * X2\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        batch_input_shape1, batch_input_shape2 = batch_input_shape\n",
    "        return [batch_input_shape1, batch_input_shape2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this layer can be used like any other layer, but only using the Functional and Subclassing APIs, not the Sequential API (which only accepts layers with one input and one output)\n",
    "---\n",
    "- let's create a layer with different behavior during training and testing (if it uses `Dropout` or `BatchNormalization` layers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(keras.layers.Layer):\n",
    "    def __init__(self, stddev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def call(self, X, training=None):\n",
    "        if training:\n",
    "            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n",
    "            return X + noise\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Models\n",
    "- pretty straightforward: subclass the `keras.Model` class, create layers and variables in the constructor, and implement the `call()` method\n",
    "---\n",
    "- in the model we're going to build, the inputs go through a 1st dense layer, then through a *residual block* composed of 2 dense layers and an addition operation, then through this same residual block 3 more times, then through a 2nd residual block, and then through a dense output layer\n",
    "- **this model is very unrealistic, but it's just to illustrate that you can easily build any kind of model you want, even one that contains loops and skip connections**\n",
    "- let's 1st creation a `ResidualBlock` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new_scaled = X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block composed of 2 dense layers and an addition operation\n",
    "class ResidualBlock(keras.layers.Layer):\n",
    "    def __init__(self, n_layers, n_neurons, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "                       for _ in range(n_layers)]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        return inputs + Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- next, let's use the Subclassing API to define the model itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualRegressor(keras.models.Model):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(30, activation=\"elu\",\n",
    "                                          kernel_initializer=\"he_normal\")\n",
    "        self.block1 = ResidualBlock(2, 30)\n",
    "        self.block2 = ResidualBlock(2, 30)\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = self.hidden1(inputs)\n",
    "        for _ in range(1 + 3):\n",
    "            Z = self.block1(Z)\n",
    "        Z = self.block2(Z)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we create the layers in the constructor and use them in the `call()` method\n",
    "- this model can be used like any other model\n",
    "- if you want to be able to save the model and load it, you must implement `get_config()` method (as we did earlier) in both the `ResidualBlock` class and the `ResidualRegressor` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples\nEpoch 1/5\n11610/11610 [==============================] - 1s 118us/sample - loss: 11.4379\nEpoch 2/5\n11610/11610 [==============================] - 0s 40us/sample - loss: 1.7623\nEpoch 3/5\n11610/11610 [==============================] - 0s 40us/sample - loss: 1.4744\nEpoch 4/5\n11610/11610 [==============================] - 0s 40us/sample - loss: 0.6444\nEpoch 5/5\n11610/11610 [==============================] - 0s 40us/sample - loss: 0.5185\n5160/5160 [==============================] - 0s 35us/sample - loss: 0.6048\n"
    }
   ],
   "source": [
    "model = ResidualRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=5)\n",
    "score = model.evaluate(X_test_scaled, y_test)\n",
    "y_pred = model.predict(X_new_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses and Metrics Based on Model Internals\n",
    "- there will be times when you want to define losses based on other parts of your model, such as the weights or activations of its hidden layers\n",
    "- this may be useful for regularization purposes or to monitor some internal aspect of your model\n",
    "---\n",
    "- to define a custom loss based on model internals, compute it based on any part of the model, then pass the result to the `add_loss()` method\n",
    "- let's build a custom MLP model composed of a stack of five hidden layers plus an output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructingRegressor(keras.models.Model):\n",
    "    # constructor creates a DNN with 5 dense layers and one dense output layer\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = [keras.layers.Dense(30, activation=\"selu\",\n",
    "                                          kernel_initializer=\"lecun_normal\")\n",
    "                       for _ in range(5)]\n",
    "        self.out = keras.layers.Dense(output_dim)\n",
    "        # TODO: check https://github.com/tensorflow/tensorflow/issues/26260\n",
    "        #self.reconstruction_mean = keras.metrics.Mean(name=\"reconstruction_error\")\n",
    "\n",
    "    def build(self, batch_input_shape): \n",
    "        # processes the inputs through all 5 hidden layers, then passes the result through the reconstruction layer, which produces the               reconstruction\n",
    "        n_inputs = batch_input_shape[-1]\n",
    "        self.reconstruct = keras.layers.Dense(n_inputs)\n",
    "        super().build(batch_input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # computes the reconstruction loss and adds it to the model's list of losses\n",
    "        # we scale down the reconstruction loss so it does not dominate the main loss\n",
    "        # finally, the call() method passes the output of the hidden layers to the output layer and returns its output\n",
    "        Z = inputs\n",
    "        for layer in self.hidden:\n",
    "            Z = layer(Z)\n",
    "        reconstruction = self.reconstruct(Z)\n",
    "        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n",
    "        self.add_loss(0.05 * recon_loss)\n",
    "        #if training:\n",
    "        #    result = self.reconstruction_mean(recon_loss)\n",
    "        #    self.add_metric(result)\n",
    "        return self.out(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 11610 samples\nEpoch 1/2\n11610/11610 [==============================] - 1s 92us/sample - loss: 0.7648\nEpoch 2/2\n11610/11610 [==============================] - 0s 35us/sample - loss: 0.4109\n"
    }
   ],
   "source": [
    "model = ReconstructingRegressor(1)\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = model.fit(X_train_scaled, y_train, epochs=2)\n",
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Gradients Using Autodiff\n",
    "- to understand how to use autodiff to compute gradients automatically, let's consider a simple toy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + 2 * w1 * w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's compute an approximation of each partial derivative by measuring how much the function's output changes when you tweak the corresponding hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "36.000003007075065"
     },
     "metadata": {},
     "execution_count": 285
    }
   ],
   "source": [
    "w1, w2 = 5, 3\n",
    "eps = 1e-6\n",
    "(f(w1 + eps, w2) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10.000000003174137"
     },
     "metadata": {},
     "execution_count": 286
    }
   ],
   "source": [
    "(f(w1, w2 + eps) - f(w1, w2)) / eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this works well and is easy to implement, but you need to call `f()` at least once per parameter, which makes this approach intractable for large neural networks\n",
    "- instead, we should use autodiff: TensorFlow makes this simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, w2 = tf.Variable(5.), tf.Variable(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "gradients = tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we first define 2 variables, `w1` and `w2`, then we create a `tf.GradientTape` context that will automatically record every operation that involves a variable, and finally we ask this tape to compute the gradients of the result `z` with regard to both variables `[w1, w2]`\n",
    "- let's take a look at the gradients that TensorFlow computed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
     },
     "metadata": {},
     "execution_count": 288
    }
   ],
   "source": [
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the results are extremely accurate and the `gradient()` method only goes through the recorded computations once (in reverse order), no matter how many variables there are, so it is incredibly efficient\n",
    "---\n",
    "- the tape is automatically erased immediately after you call its `gradient()` method, so you will get an exception if you try to call `gradient()` twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "GradientTape.gradient can only be called once on non-persistent tapes.\n"
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "try:\n",
    "    dz_dw2 = tape.gradient(z, w2)\n",
    "except RuntimeError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to call `gradient()` more than once, make the tape persistent and delete it each time you are done with it to free resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=10.0>)"
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape: # making tape persistent\n",
    "    z = f(w1, w2)\n",
    "\n",
    "dz_dw1 = tape.gradient(z, w1)\n",
    "dz_dw2 = tape.gradient(z, w2) # works now!\n",
    "del tape\n",
    "\n",
    "dz_dw1, dz_dw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the tape only tracks operations involving variables: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[None, None]"
     },
     "metadata": {},
     "execution_count": 291
    }
   ],
   "source": [
    "c1, c2 = tf.constant(5.), tf.constant(3.)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can force the tape to watch any tensors you like, to record every operation that involves them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]"
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(c1)\n",
    "    tape.watch(c2)\n",
    "    z = f(c1, c2)\n",
    "\n",
    "gradients = tape.gradient(z, [c1, c2])\n",
    "gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- in some cases, you may want to stop gradients from backpropagating through some part of your neural network\n",
    "- to do this, use the `tf.stop_gradient()` function, which returns the inputs during the forward pass, but does not let gradients through during backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<tf.Tensor: shape=(), dtype=float32, numpy=30.0>, None]"
     },
     "metadata": {},
     "execution_count": 293
    }
   ],
   "source": [
    "def f(w1, w2):\n",
    "    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = f(w1, w2)\n",
    "\n",
    "tape.gradient(z, [w1, w2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- finally, you may run into some numerical issues when computing gradients\n",
    "- for example, if you compute the gradients of the `my_softplus()` function for large inputs, the result will be NaN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[<tf.Tensor: shape=(), dtype=float32, numpy=nan>]"
     },
     "metadata": {},
     "execution_count": 294
    }
   ],
   "source": [
    "x = tf.Variable(100.) # large input\n",
    "with tf.GradientTape() as tape:\n",
    "    z = my_softplus(x)\n",
    "\n",
    "tape.gradient(z, [x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Training Loops\n",
    "- sometimes the `fit()` method may not be flexible enough for what you need to do\n",
    "- for example, the Wide & Deep paper uses two different optimizers: one for the wide path and the other for the deep path\n",
    "- the `fit()` method, however, only uses one optimizer (the one that we specify when compiling the model), so implementing this paper requires writing your own custom loop \n",
    "---\n",
    "- you may also write custom training loops to simply feel more confident that they do precisely what you intend for them to do\n",
    "- however, writing custom functions will make your code longer, more error-prone, and harder to maintain\n",
    "- so, unless you really need the extra flexibility, just stick with the `fit()` method\n",
    "---\n",
    "- let's build a simple model (no need to compile it as we will handle the training loop manually): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_reg = keras.regularizers.l2(0.05)\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=l2_reg),\n",
    "    keras.layers.Dense(1, kernel_regularizer=l2_reg)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- next, let's create a function that will randomly sample a batch of instances from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's also define a function that will display the training status, including the number of steps, the total number of steps, the mean loss since the start of the epoch, and other metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_status_bar(iteration, total, loss, metrics=None):\n",
    "    metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result())\n",
    "                         for m in [loss] + (metrics or [])])\n",
    "    end = \"\" if iteration < total else \"\\n\"\n",
    "    print(\"\\r{}/{} - \".format(iteration, total) + metrics,\n",
    "          end=end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- okay, let's start by defining some hyperparameters and choosing the optimizer, the loss function, and the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.mean_squared_error\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.MeanAbsoluteError()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now, let's build the custom training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\nWARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n\nIf you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n\nTo change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n\n11610/11610 - mean: 1.3955 - mean_absolute_error: 0.5722\nEpoch 2/5\n11610/11610 - mean: 0.6774 - mean_absolute_error: 0.5280\nEpoch 3/5\n11610/11610 - mean: 0.6351 - mean_absolute_error: 0.5177\nEpoch 4/5\n11610/11610 - mean: 0.6384 - mean_absolute_error: 0.5181\nEpoch 5/5\n11610/11610 - mean: 0.6440 - mean_absolute_error: 0.5222\n"
    }
   ],
   "source": [
    "# beautiful custom training loop\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model(X_batch)\n",
    "            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "            loss = tf.add_n([main_loss] + model.losses)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        for variable in model.variables:\n",
    "            if variable.constraint is not None:\n",
    "                variable.assign(variable.constraint(variable))\n",
    "        mean_loss(loss)\n",
    "        for metric in metrics:\n",
    "            metric(y_batch, y_pred)\n",
    "        print_status_bar(step * batch_size, len(y_train), mean_loss, metrics)\n",
    "    print_status_bar(len(y_train), len(y_train), mean_loss, metrics)\n",
    "    for metric in [mean_loss] + metrics:\n",
    "        metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most importantly, this training loop does not handle layers that behave differently during training (`Dropout` or `BatchNormalization`)\n",
    "- to handle these, you need to call the model with `training=True` and make sure it propagates this to every layer that needs it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Functions and Graphs\n",
    "- graphs are very simple to implement in TensorFlow\n",
    "- let's start with a trivial function that computes the cube of its input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8"
     },
     "metadata": {},
     "execution_count": 303
    }
   ],
   "source": [
    "cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
     },
     "metadata": {},
     "execution_count": 304
    }
   ],
   "source": [
    "cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's use the `tf.function()` to convert this python function to a *TensorFlow Function*\n",
    "- the TF Function can be used exactly like the original Python function and it will return the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.eager.def_function.Function at 0x21edc336860>"
     },
     "metadata": {},
     "execution_count": 305
    }
   ],
   "source": [
    "tf_cube = tf.function(cube) # converting normal python function to a tensorflow function\n",
    "tf_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
     },
     "metadata": {},
     "execution_count": 306
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=8.0>"
     },
     "metadata": {},
     "execution_count": 307
    }
   ],
   "source": [
    "tf_cube(tf.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- alternatively, we could have just used `tf.function` as a decorator (which is actually more common):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function # decorator\n",
    "def tf_cube(x):\n",
    "    return x ** 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=int32, numpy=8>"
     },
     "metadata": {},
     "execution_count": 309
    }
   ],
   "source": [
    "tf_cube(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the original bython function is still available via the TF Function's `python_function` attribute, in case you ever need it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "8"
     },
     "metadata": {},
     "execution_count": 310
    }
   ],
   "source": [
    "tf_cube.python_function(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- under the hood, the `tf.function()` analyzed the computations performed by the `cube()` function and **generated an equivalent computation graph**\n",
    "---\n",
    "- TF Functions are usually much faster than their Python equivalents: so when you want to boost a Python function, just transform it into a TF Function\n",
    "---\n",
    "- moreover, when you write a custom loss function, a custom metric, a custom layer, or any other custom function and use it in a Keras model (as we did throughout this chapter), Keras automatically converts your function into a TF function (so no need to use `tf.Function()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGraph and Tracing\n",
    "- to generate graphs, TensorFlow first analyzes the Python function's source code to capture all the control flow statements (such as `for` loops) --> this first step is called *AutoGraph*\n",
    "- after this initial analyzation, AutoGraph outputs an upgraded version of that function in which all the control flow statements are replaced by the appropriate TensorFlow operations\n",
    "- next, TensorFlow calls this \"upgraded\" function, but instead of passing the argument, it passes a *symbolic tensor* (a tensor without any actual value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "- we started with a brief overview of TensorFlow\n",
    "- then we looked at TensorFlow's low-level API, including tensors, operations, variables, and even special data structures\n",
    "- we then used these tools to customize almost every component in `tf.keras`\n",
    "- finally, we looked at how TensorFlow Functions can boost performance and how graphs are generated using AutoGraph and tracing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "---\n",
    "- 1) *How would you describe TensorFlow in a short sentence? What are its main features? Can you name other popular Deep Learning libraries?*\n",
    "---\n",
    "- TensorFlow is an open-source library for numerical computation that is particularly well suited and fine-tuned for large-scale Machine Learning. Its core is similar to NumPy, but it also features GPU support, support for distributed computing, computation graph analysis, plenty of optimization capabilities, and several powerful APIs such as `tf.keras`, `tf.data`, `tf.image`, `tf.signal`, and more. Other popular Deep Learning libraries include PyTorch, MXNet, Microsoft Cognitive Toolkit, Theano, Caffe2, and Chainer. \n",
    "---\n",
    "- 2) *Is TensorFlow a drop-in replacement for NumPy? What are the main differences between the two?*\n",
    "---\n",
    "- Although TensorFlow offers most of the functionalities provided by NumPy, it is not a drop-in replacement.This is because the names of the functions are not always the same and some functions do not behave in the exact same way. Lastly, NumPy arrays are mutable, whereas TensorFlow tensors are not (but you can use a `tf.Variable` if you need a mutable object).\n",
    "---\n",
    "- 3) *Do you get the same result with `tf.range(10)` and `tf.constant(np.arange(10))`?*\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
     },
     "metadata": {},
     "execution_count": 311
    }
   ],
   "source": [
    "tf.range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>"
     },
     "metadata": {},
     "execution_count": 312
    }
   ],
   "source": [
    "tf.constant(np.arange(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Yes, both `tf.range(10)` and `tf.constant(np.arange(10))` return a one-dimensional tensor containing the integers 0 to 9. Keep in mind, however, that TensorFlow defaults to 32 bits, while NumPy defaults to 64 bits.\n",
    "---\n",
    "- 4) *Can you name six other data structures available in TensorFlow, beyond regular tensors?*\n",
    "---\n",
    "- In addition to regular TensorFlow tensors, TensorFlow offers several other data structures, including sparse tensors, tensor arrays, ragged tensors, queues, string tensors, and sets. The last two are actually represented as regular tensors, but TensorFlow provides special functions to manipulate them (in `tf.strings` and `tf.sets`).\n",
    "---\n",
    "- 5) *A custom loss function can be defined by writing a function or by subclassing the `keras.losses.Loss` class. When would you use each option?*\n",
    "---\n",
    "- When defining a custom loss function, you can generally just implement it as a regular Python function. However, if your custom loss function must support hyperparameters, then you should subclass the `keras.losses.Loss` class and implement the `__init__()` and `call()` methods. If you want the loss function's hyperparameters to be saved along with the model, then you must also implement the `get_config()` method. \n",
    "---\n",
    "- 6) *Similarly, a custom metric can be defined in a function or a subclass of `keras.metrics.Metric`. When should you use each option?*\n",
    "---\n",
    "- Much like loss functions, most metrics can simply be defined as regular Python functions. But again, if you want your custom metric to support hyperparameters, then you should subclass the `keras.metrics.Metric` class. Lastly, if you want the state to be saved along with the model, then you should implement the `get_config()` method. \n",
    "---\n",
    "- 7) *When should you create a custom layer versus a custom model?*\n",
    "---\n",
    "- It is important to distinguish the internal components of your model (layers) from the model itself (the object you will train). The former should subclass the `keras.layers.Layer` class, while the latter should subclass the `keras.models.Model` class. \n",
    "---\n",
    "- 8) *What are some use cases that require writing your own custom training loop?*\n",
    "---\n",
    "- Writing your own custom training loop is very advanced and can introduce many errors, so you should only do it if absolutely necessary. Keras provides several tools to customize training without having to write an entire custom training loop: callbacks, custom regularizers, custom constraints, custom losses, and so on. However, if you want to use different optimizers for different parts of your neural network, like in the Wide & Deep paper, then writing a custom training loop would be appropriate. A custom training loop can also be useful when debugging, or when trying to understand exactly how training works.\n",
    "---\n",
    "- 9) *Can custom Keras components contain arbitrary Python code, or must they be convertible to TF Functions?*\n",
    "---\n",
    "- Custom Keras components should be convertible to TF Functions, so they should stick to TF operations. If you absolutely need to include arbitrary Python code in a custom component, you can either wrap it in a `tf.py_function()` operation or set `dynamic=True` when creating the custom layer or model.\n",
    "---\n",
    "- 10) *When would you need to create a dynamic Keras model? How do you do that? Why not make all your models dynamic?*\n",
    "---\n",
    "- Creating a dynamic Keras model can be useful for debugging or if you want to include arbitrary Python code in your model. However, making a model dynamic prevents Keras from using any of TensorFlow's graph features, so it will slow down training and inference, and you will not have the possibility to export the computation graph, which will severely hinder your model's portability. \n",
    "---\n",
    "- 11) *Implement a custom layer that performs Layer Normalization:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise a): The `build()` method should define two trainable weights *α* and *β*, both of shape `input_shape[-1:]` and data type `tf.float32`. *α* should be initialized with 1s, and *β* with 0s._\n",
    "\n",
    "---\n",
    "\n",
    "_Exercise b): The `call()` method should compute the mean_ μ _and standard deviation_ σ _of each instance's features. For this, you can use `tf.nn.moments(inputs, axes=-1, keepdims=True)`, which returns the mean μ and the variance σ<sup>2</sup> of all instances (compute the square root of the variance to get the standard deviation). Then the function should compute and return *α*⊗(*X* - μ)/(σ + ε) + *β*, where ⊗ represents itemwise multiplication (`*`) and ε is a smoothing term (small constant to avoid division by zero, e.g., 0.001)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(keras.layers.Layer):\n",
    "    def __init__(self, eps=0.001, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps = eps\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        self.alpha = self.add_weight( # trainable weight 1\n",
    "            name=\"alpha\", shape=batch_input_shape[-1:],\n",
    "            initializer=\"ones\")\n",
    "        self.beta = self.add_weight( # trainable weight 2\n",
    "            name=\"beta\", shape=batch_input_shape[-1:],\n",
    "            initializer=\"zeros\")\n",
    "        super().build(batch_input_shape) # must be at the end\n",
    "\n",
    "    def call(self, X):\n",
    "        mean, variance = tf.nn.moments(X, axes=-1, keepdims=True)\n",
    "        return self.alpha * (X - mean) / (tf.sqrt(variance + self.eps)) + self.beta\n",
    "\n",
    "    def compute_output_shape(self, batch_input_shape):\n",
    "        return batch_input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, \"eps\": self.eps}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise c): Ensure that your custom layer produces the same (or very nearly the same) output as the `keras.layers.LayerNormalization` layer._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=4.817434e-08>"
     },
     "metadata": {},
     "execution_count": 314
    }
   ],
   "source": [
    "X = X_train.astype(np.float32)\n",
    "\n",
    "custom_layer_norm = LayerNormalization()\n",
    "keras_layer_norm = keras.layers.LayerNormalization()\n",
    "\n",
    "tf.reduce_mean(keras.losses.mean_absolute_error(\n",
    "    keras_layer_norm(X), custom_layer_norm(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- such a small difference: our custom layer works fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- 12) *Train a model using a custom training loop to tackle the Fashion MNIST dataset:*\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise a): Display the epoch, iteration, mean training loss, and mean accuracy over each epoch (updated at each iteration), as well as the validation loss and accuracy at the end of each epoch._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full.astype(np.float32) / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test.astype(np.float32) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(55000, 28, 28)"
     },
     "metadata": {},
     "execution_count": 317
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our hyperparameters, optimizer, and metrics\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "optimizer = keras.optimizers.Nadam(lr=0.01)\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "mean_loss = keras.metrics.Mean()\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'trange' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-320-d5850c6b1046>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"All epochs\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_steps\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Epoch {}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mX_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trange' is not defined"
     ]
    }
   ],
   "source": [
    "with trange(1, n_epochs + 1, desc=\"All epochs\") as epochs:\n",
    "    for epoch in epochs:\n",
    "        with trange(1, n_steps + 1, desc=\"Epoch {}/{}\".format(epoch, n_epochs)) as steps:\n",
    "            for step in steps:\n",
    "                X_batch, y_batch = random_batch(X_train, y_train)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    y_pred = model(X_batch)\n",
    "                    main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n",
    "                    loss = tf.add_n([main_loss] + model.losses)\n",
    "                gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                for variable in model.variables:\n",
    "                    if variable.constraint is not None:\n",
    "                        variable.assign(variable.constraint(variable))                    \n",
    "                status = OrderedDict()\n",
    "                mean_loss(loss)\n",
    "                status[\"loss\"] = mean_loss.result().numpy()\n",
    "                for metric in metrics:\n",
    "                    metric(y_batch, y_pred)\n",
    "                    status[metric.name] = metric.result().numpy()\n",
    "                steps.set_postfix(status)\n",
    "            y_pred = model(X_valid)\n",
    "            status[\"val_loss\"] = np.mean(loss_fn(y_valid, y_pred))\n",
    "            status[\"val_accuracy\"] = np.mean(keras.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_valid, dtype=np.float32), y_pred))\n",
    "            steps.set_postfix(status)\n",
    "        for metric in [mean_loss] + metrics:\n",
    "            metric.reset_states()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
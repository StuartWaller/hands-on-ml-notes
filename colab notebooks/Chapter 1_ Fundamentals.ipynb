{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Chapter 1: Fundamentals.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP2oXBT0nzL2TcjqmK/LwOw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hGS4NlomuSk9","colab_type":"text"},"source":["# Introductory Notes: \n","- master fundamentals first: most problems can be solved using simple techniques. Deep learning is best suited for complex problems such as image/speech recognition / NLP\n","- definitely do Andrew Ng's ML course on Coursera, deeplearning.ai course, deeplearning.net has good resources, maybe finish Dataquest\n","- check out page XX for good reads - in particular, Francois's /  \n","https://homl.info/patel\n","- once done with book, message author on linkedin (page xxiii) \n","\n"]},{"cell_type":"markdown","metadata":{"id":"-67yY0C5y05A","colab_type":"text"},"source":["# The Machine Learning Landscape"]},{"cell_type":"markdown","metadata":{"id":"8j2DmzJxy_o6","colab_type":"text"},"source":["- make sure everything is crystal clear by the end of this chapter\n","- \"**Machine Learning is the science (and art) of programming computers so they can *learn from data*.**\" \n","- examples that system uses = training set (each instance called a sample) \n","- can automatically notice changes and instantly adapt\n","- traditional programming: write rules | machine learning: train ML algorithm\n","- machine learning helps humans learn, too\n","- data mining: applying ML techniques to dig into large amounts of data, which helps discover hidden patterns \n","- in short, ml is great for deriving solutions from complex problems / fluctuating environments "]},{"cell_type":"markdown","metadata":{"id":"yg6hW5bj3OiF","colab_type":"text"},"source":["## Types of ML Systems: \n","- whether or not trained w/ human supervision (supervised, unsupervised, semisupervised, and Reinforcement Learning)\n","- whether or not they can learn incrementally on the fly (online versus batch learning) \n","- whether they work by simply comparing new data to known data or instead by detecting patterns and building a predictive model (instance-based vs. model-based learning) "]},{"cell_type":"markdown","metadata":{"id":"NC0jXB833uJg","colab_type":"text"},"source":["## Supervised Learning: \n","---\n","- Supervised learning: training set you feed to the algorithm includes the desired solutions (called labels) \n","- a typical example of SL is classification / to predict a target numeric value given a set of features (mileage, age, brand, etc.) called predictors --> this task is called regression. <--> to train the system, you need to give it many examples of cars, including both their predictors and their labels (i.e., their prices)\n","- attribute = data type (e.g., \"mileage\") | feature usually means attribute + its value: (e.g, \"mileage = 15,000\") -> people use both interchangeably\n","- some regression algos can be used for classification as well, and vice versa, i.e. Logistic Regression\n","- **list of important supervised learning algorithms**: KNN, Linear Regression, Logistic Regression, Support Vector Machines (SVMs), Decision Trees/RFs, Neural Networks\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bCe4kLxP7zOt","colab_type":"text"},"source":["## Unsupervised Learning:\n","---\n","- training data is UNLABELED (learns w/o a teacher) \n","- important algorithms: Clustering, Anomaly Detection/Novelty Detection, Visualization & Dimensionality Reduction, Association Rule Learning\n","- i.e. classifying blog visitors: you never tell *it* who belongs to which groups, it forms its own --> clustering\n","- visualization: feed complex, unlabeled data -> outputs 2D/3D representation of data that can easily be plotted\n","- dimensionality reduction: goal --> simplify data w/o losing too much info | involves merging features that are strongly correlated w/ one another (extraction) --> good to do before feeding data into another algo (makes it simpler/faster) \n","- anomaly/novelty detection: captures outliers (i.e., credit card fraud) --> fed normal data during training, then it can tell whether something is normal or an anomaly\n","- Association Rule Learning: dig into large amounts of data to find hidden relations (i.e., people who buy bbq sauce and chips tend to buy steak... hmm, maybe place them all together in the store to generate more revenue\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"Fcb5hrWS-fxq","colab_type":"text"},"source":["## Semisupervised Learning: \n","---\n","- some algorithms can deal with (training) data that is partially labeled \n","- i.e. if person A shows up in pictures 1, 4, and 7, give person A a name (a label) in any one of the pictures and it will automatically associate across all pictures\n","- Semisupervised Learning Algorithms are generally combinations of Sup/Unsup Models (RBMS) \n","---"]},{"cell_type":"markdown","metadata":{"id":"XjhZ4kJzB6VI","colab_type":"text"},"source":["## Reinforcement Learning: \n","---\n","- very different beast; learning system called an \"agent\"\n","- performs actions, receives rewards or penalities, chooses optimal policy based on outcome (reward or penality) \n","--- "]},{"cell_type":"markdown","metadata":{"id":"A7bOkrpMCY1U","colab_type":"text"},"source":["## Batch Learning\n","--- \n","- cannot be trained incrementally: requires use of ALL training data\n","- uses lots of resources, done offline\n","- not the best for constantly changing data / huge huge data\n","--- \n","## Online Learning\n","--- \n","- train system incrementally by feeding data instances sequentially (mini-batches) \n","- each step is fast and cheap | learns new data on the fly \n","- great for flowing, continuous data (i.e., stock prices) \n","- once it has learned new instances, it does not NEED them anymore\n","- good for massive amounts of data (doing a little at a time) --> called out-of-core learning if data cannot fit on computer all at once\n","- LEARNING RATE = how fast system should adapt to changing data | too high --> learns fast, but forgets old data | too slow --> opposite\n","- CHALLENGE: bad data leads to decline of system (important to monitor input data) (AKA: using an Anomaly Detection Algorithm) \n","--- "]},{"cell_type":"markdown","metadata":{"id":"rmPSkZOSFTKM","colab_type":"text"},"source":["## Instance-Based Learning: \n","---\n","- ML systems can be categorized by how they *generalize* \n","- generalize = make good predictions off training examples for new, unseen data\n","- \"the goal is to perform well on NEW instances, not just training data\" \n","- Instance-Based: learns examples by heart then generalizes to new cases by examining similarities between new data and old data \n","- compares new problem instances with instances seen in training data, instead of performing explicit generalization\n","--- \n","## Model-Based Learning: \n","- use model to make predictions based on similarity examples\n","- noisy data = partly random\n","- i.e. selecting a linear model to represent a relationship\n","- where all the assumptions about the problem domain are made explicit in the form of a model\n","\n","### Linear Regression: \n","- people typically use cost function (measures how bad model is) --> measures distance between predictions and training data; the objective is to minimize the distance  \n","- must define parameter values (Î¸s) \n","- training a model: running algorithm to find best parameters\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GQysdlsyR53N","colab_type":"text"},"source":["# Example 1-1. Training and running a linear model using Scikit-Learn\n"]},{"cell_type":"markdown","metadata":{"id":"1y7DVxP0SoI9","colab_type":"text"},"source":["\"This function just merges the OECD's life satisfaction data and the IMF's GDP per capita data. It's a bit too long and boring and it's not specific to Machine Learning, which is why I left it out of the book.\" "]},{"cell_type":"code","metadata":{"id":"Zi36msQ7Sp_8","colab_type":"code","colab":{}},"source":["def prepare_country_stats(oecd_bli, gdp_per_capita):\n","    oecd_bli = oecd_bli[oecd_bli[\"INEQUALITY\"]==\"TOT\"]\n","    oecd_bli = oecd_bli.pivot(index=\"Country\", columns=\"Indicator\", values=\"Value\")\n","    gdp_per_capita.rename(columns={\"2015\": \"GDP per capita\"}, inplace=True)\n","    gdp_per_capita.set_index(\"Country\", inplace=True)\n","    full_country_stats = pd.merge(left=oecd_bli, right=gdp_per_capita,\n","                                  left_index=True, right_index=True)\n","    full_country_stats.sort_values(by=\"GDP per capita\", inplace=True)\n","    remove_indices = [0, 1, 6, 8, 33, 34, 35]\n","    keep_indices = list(set(range(36)) - set(remove_indices))\n","    return full_country_stats[[\"GDP per capita\", 'Life satisfaction']].iloc[keep_indices]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qmuy9PxyStCj","colab_type":"code","colab":{}},"source":["import os\n","datapath = os.path.join(\"datasets\", \"lifesat\", \"\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PemDQ9_SSwEb","colab_type":"code","colab":{}},"source":["# To plot pretty figures directly within Jupyter\n","%matplotlib inline\n","import matplotlib as mpl\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B_lkyE0PSxci","colab_type":"code","outputId":"41185b42-7598-4aed-e253-6c9609722ab4","executionInfo":{"status":"ok","timestamp":1586592721303,"user_tz":420,"elapsed":3266,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Download the data\n","import urllib\n","DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n","os.makedirs(datapath, exist_ok=True)\n","for filename in (\"oecd_bli_2015.csv\", \"gdp_per_capita.csv\"):\n","    print(\"Downloading\", filename)\n","    url = DOWNLOAD_ROOT + \"datasets/lifesat/\" + filename\n","    urllib.request.urlretrieve(url, datapath + filename)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading oecd_bli_2015.csv\n","Downloading gdp_per_capita.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ISVgWyijSycZ","colab_type":"code","outputId":"75668861-f84a-4d20-812b-5ee49e5e5d0a","executionInfo":{"status":"ok","timestamp":1586594809292,"user_tz":420,"elapsed":826,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":304}},"source":["# Code example\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import sklearn.linear_model\n","\n","# Load the data\n","oecd_bli = pd.read_csv(datapath + \"oecd_bli_2015.csv\", thousands=',')\n","gdp_per_capita = pd.read_csv(datapath + \"gdp_per_capita.csv\",thousands=',',delimiter='\\t',\n","                             encoding='latin1', na_values=\"n/a\")\n","\n","# Prepare the data \n","# Training data\n","country_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\n","X = np.c_[country_stats[\"GDP per capita\"]]\n","y = np.c_[country_stats[\"Life satisfaction\"]]\n","\n","# Visualize the data\n","country_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\n","plt.show()\n","\n","# Select a linear model\n","model = sklearn.linear_model.LinearRegression()\n","\n","# Train the model\n","# Trained it on the training data (i.e., the learning algo. searched for the model parameter values that minimize a cost function)\n","model.fit(X, y)\n","\n","# Make a prediction for Cyprus\n","# Applied model to make prediction on new cases (this is called inference) hoping the model would generalize well\n","X_new = [[22587]]  # Cyprus' GDP per capita\n","print(model.predict(X_new)) # outputs [[ 5.96242338]]"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["[[5.96242338]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6g7afQi5WgJW","colab_type":"text"},"source":["---\n","- If I had used an instance-based learning algo, it would have found that Slovenia has the closest GDP to Cyprus's, and since Slovenia's life satisfaction is 5.7, my model would have predicted Cyprus's life satisfaction to be 5.7 as well. \n","---"]},{"cell_type":"code","metadata":{"id":"dudpSgfDXEjj","colab_type":"code","outputId":"922eb86c-9ddc-46e3-c34d-bdf4f2b53f33","executionInfo":{"status":"ok","timestamp":1586593910484,"user_tz":420,"elapsed":691,"user":{"displayName":"Machine Learning","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh4bY_nnd1m8MaHO2C9699E3gHpnCalP6-OaBle=s64","userId":"17087927105868275457"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Replacing Linear Regression model w/ K-Nearest Neighbors Regression\n","\n","import sklearn.neighbors\n","\n","# Select a linear model\n","model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3)\n","\n","# Train the model\n","model.fit(X, y)\n","\n","# Make a prediction for Cyprus\n","X_new = [[22587]]  # Cyprus' GDP per capita\n","print(model.predict(X_new)) # outputs [[5.76666667]]"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[5.76666667]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gIqF0ywibFii","colab_type":"text"},"source":["# Main Challengies of Machine Learning"]},{"cell_type":"markdown","metadata":{"id":"iB0-zaYebH3U","colab_type":"text"},"source":["- In short, 2 things can go wrong: bad algorithm or bad data\n","--- \n","## Insufficient Quantity of Training Data:\n","- oftentimes need thousands/millions of examples\n","- famous paper (https://homl.info/6) --> showed that very different ML algorithms (both simple and complex) performed almost identically well on a complex problem of natural language disambiguation once they were given ENOUGH DATA --> emphasizes importance of a good training set\n","--- \n","## Nonrepresentative Training Data:\n","- training data should be representative of desired new cases | true for both instance/model bl\n","- above, in our happiness prediction model, we used nonrepresentative training data - left out countries, that, when added back in, significantly alter the model (slope of line) (rich countries are not happier than moderately rich countries/even some poor countries) - such could never be predicted with the data we used, therefore making it nonrepresentative\n","- if the sample it too small, we will have *sampling noise* (nonrepresentative data as a result of chance) | if the sample is too big, we have *sampling bias* (favors some outcomes over the others.. aka method is flawed) \n","- example of sampling bias: saying that a candidate is favored to win based on a study, but said study was not representative of the general population \n","---\n","## Poor Quality Data:   \n","- if some instances are outliers, consider discarding them \n","- critical part of success in machine learning project is coming up with a good set of features to train on *feature engineering* --> *feature selection* select most useful features | *feature extraction* combining features to form more useful ones | & creating new features from gathering new data\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"PzX_wSCp3hyO","colab_type":"text"},"source":["Now let's look at some algorithmic errors\n","\n","---\n","## Overfitting the Training Data: \n","- model performs well on training data, but does not generalize well\n","- overfitting happens when the model is too complex relative to the amount and noisiness of the training data (can often happen with too little training data: model will pay attention to ridiculous things it thinks are relevant because too little data to say otherwise) \n","- **How to deal with overfitting:** simplify model (called *regularization*) (select one with fewer parameters), gather more training data, or reduce noise (fix errors/remove outliers) \n","- ***you want to find the balance between fitting the training data and generalizing well***\n","- regularization (simplifying model) can be as simple as changing the slope Î¸ in our linear regression model --> the amount of regularization can be controlled by a *hyperparameter* --> **must be set prior to training** (very lage hyperparameter would not overfit data, but it would be almost flat) \n","- a *hyperparameter* controls the learning process of the algorithm (not model) \n","- *parameters* (node weights) are learned and are used to make predictions \n","--- \n","## Underfitting the Training Data:\n","- model is too simple to learn the underlying structure of the data\n","- to fix: select more powerful model, perform feature engineering, reduce constraints (reduce regularization hyperparameter) \n","---"]},{"cell_type":"markdown","metadata":{"id":"VTCUf9os8kX8","colab_type":"text"},"source":["# Testing and Validating: \n","- split data into two sets: training and test\n","- the error rate on new cases is called *generalization error* (or *out-of-sample error*) \n","- common to use 80% of data for training, 20% for testing, but be reasonable\n","--- \n","## Hyperparameter Tuning / Model Selection: \n","- *validation set/dev set/development set* is for tuning parameters, test set is for generalization evaluation \n","- **more specifically**, you train multiple models with various hyperparameters on a reduced training set and you select the model that performs best --> after this *holdout validation* process, you train the best model on the full training set, which gives you your final model. Then, you evaluate this model on the test set toi get an estimate fo the generalization error\n","- repeated *cross-validation*: using many small validation sets --> this is more accurate, but takes longer (essentilly averages out evaluations of a model) \n","- validation and test sets should be extremely representative of the new data we want to be able to accurately generalize\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"bcj__KclB-DB","colab_type":"text"},"source":["# Exercises: \n","--- \n","**KEY:**\n","\n","a) my initial response\n","\n","b) anything I left out/got wrong\n","\n","--- \n","1) Machine learning is training a model using data to generalize and make predictions on new data without explicitly programming code rules\n","\n","*learn from data | learning = get better at a task*\n","\n","--- \n","\n","2) Machine learning shines on image classification, sentiment analysis, deriving results from fluctuating environments, and discovering hidden patterns that humans would otherwise not be able to detect (i.e., *dating mining*)\n","\n","*complex problems for which we have no algorithmic solution* \n","\n","--- \n","\n","3) A labeled training set is one that includes the feature we want to make the generalization about\n","\n","*the desired solution (label) for each instance* \n","\n","--- \n","\n","4) Two most common types of supervised learning tasks: classification & predicting a numeric value (regression)\n","\n","*classification & regression*\n","\n","--- \n","\n","5) Four common unsupervised learning tasks: customer segmentation (clustering), credit card fraud (anomaly/novelty detection), simplifying data w/o losing too much info (visualization & dimensionality reduction), and finding hidden patterns/correlations within data (association rule learning)\n","\n","*clustering, visualization, dimensionality reduction, and association rule learning*\n","\n","--- \n","\n","6) To allow a robot to walk in various unknown terrains, I would use Reinforcement Learning\n","\n","*reinforcement learning | may be possible to classify as supervsed/semisupervised*\n","\n","--- \n","\n","7) For customer segmentation, I would use clustering (unsupervised learning) \n","\n","*if you don't know the groups you'd like --> go clustering | if you do know the groups you'd like --> feed many examples of each group to a classification algorithm (supervised learning) and it will classify all your customers into the desired groups (hopefully lol)* \n","\n","--- \n","\n","8) Spam detection is supervised learning because you would be training your model to classify emails as \"spam\" or \"ham\" by showing it examples of \"spam\" or \"ham\" (labels in training set, so supervised) \n","\n","*spam detection is a typical supervised learning problem*\n","\n","--- \n","\n","9) an online learning system, as opposed to a batch learning system, learns incrementally by feeding *it* portions of dataset --> better/faster for handling larger datasets\n","\n","*can also adapt rapidly to changing data and autonomous systems* \n","\n","--- \n","\n","10) out-of-core learning, otherwise known as batch learning, is the opposite of online learning, where it cannot learn incrementally and is instead fed the entire training data at once, which is generally hard to process, hence why it is performed offline; it can take a very long time \n","\n","*out-of-core algorithms can handle vast quantities of data that cannot fit in a computer's main memory. OFC algorithms chops the data into mini-batches and uses online learning techniques to learn from them --> this is NOT batch learning, which I confused above.*\n","\n","--- \n","\n","11) Instance-based learning relies on a similarity measure to make predictions.\n","\n","*learns training data by heart, uses most similar learned instances to predict new instances*\n","\n","--- \n","\n","12) a model parameter (node weight) affects how the MODEL makes predictions (how it generalizes), whereas a learning algorithm hyperparameter adjusts the generalization performance of the LEARNING ALGORITHM --> tuning hyperparameters commonly performed using cross-validation (aka taking multiple datasets from the training data to use as validation/dev/devlopment sets) \n","\n","*a learning algorithm tries to find optimal values for parameters such that the model generalizes well to new instances*\n","\n","--- \n","\n","13) Model based learning algorithms search for similarities and make predictions using models. \n","\n","*model based learning algorithms search for optimal model parameters --> we usually train such systems by trying to minimize the cost function*\n","\n","--- \n","\n","14) The four main challenges of machine learning: unrepresentative data, poor quality data, overfitting the data, and underfitting the data\n","\n","*lack of data, excessively simple models that underfit the training data, and excessively complex models that overfit the training data* \n","\n","--- \n","\n","15) When your model performs great on the training data but generalizes poorly to new instances, overfitting is occuring. As a fix, one should try to: gather new data, try constraining (simplifying) the model (regularization, aka tuning hyperparameters), or reduce noise (remove outliers/fix errors) \n","\n","*simplifying the model includes: selecting a simpler algorithm (or) reducing the numbers of parameters/features used (or) regularizing the model* \n","\n","--- \n","\n","16) A test set is usually a smaller portion of the collective data that is set aside to evluate the model for errors once it has been validated/tuned and trained. A model's performance on a test set is a good indicator of how it would generalize new data. \n","\n","*measures generalization error before it is launched into production*\n","\n","--- \n","\n","17) A validation set is a smaller portion of the training data used to tune the learning algorithm's hyperparameters in order for it to learn optimally. \n","\n","*used to compare models | makes it possible to select the best model and tune the hyperparameters*\n","\n","--- \n","\n","18) The train-dev set, otherwise known as the validation set, is used for validation before training the model with the entirety of the training data. You use it to tune the learning algorithm's hyperparameters. \n","\n","\n","*NOT the same as validation set --> in fact, the train-dev set is data set aside from the training set that checks to see if there is **data mismatch** between the validation set/test set & training set. If the model performs well on the training set but not the train-dev set, then the model is likely overfitting the training set. If the model performs well on the training set & dev set, BUT NOT the validation set, then there is data mismatch between the training set & the validation/test set.* --> keep in mind: validation set is supposed to be as similar to test set as possible (or well, the data we want to generalize) \n","\n","--- \n","\n","19) \n","\n","==================== **REGARDING VALIDATION / HYPERPARAMETERS:** ====================\n","\n","\"***The idea behind holdout and cross validation is to estimate the generalization performance of a learning algorithm***--that is, the expected performance on unknown/unseen data drawn from the same distribution as the training data. This can be used to tune hyperparameters or report the final performance. ***The validity of this estimate depends on the independence of the data used for training and estimating performance***. If this independence is violated, the performance estimate will be overoptimistically biased. The most egregious way this can happen is by estimating performance on data that has already been used for training or hyperpameter tuning, but there are many more subtle and insidious ways too.\n","\n","DO NOT USE VALIDATION SET FOR TRAINING. The goal of hyperparameter tuning is to select hyperparameters that will give good generalization performance. Typically, this works by estimating the generalization performance for different choices of hyperparameters (e.g. using a validation set), and then choosing the best. ***But, as above, this estimate will be overoptimistic if the same data has been used for training***. The consequence is that sub-optimal hyperparameters will be chosen. In particular, there will be a bias toward high capacity models that will overfit.\n","\n","Second, data that has already been used to tune hyperparameters and is re-used to estimate performance (training) is BAD. This will give a deceptive estimate, as above. This isn't overfitting itself but it means that, if overfitting is happening (and it probably is, as above), then you won't know it.\n","\n","The remedy is to use three separate datasets: a training set for training, a validation set for hyperparameter tuning, and a test set for estimating the final performance. Or, use nested cross validation, which will give better estimates, and is necessary if there isn't enough data.\"\n","\n","My guess is overfitting..\n","\n","*If you tune hyperparameters using the test set, you risk overfitting the test set, and therefore guiding the model to perform inaccurate generalizations on truly new data*\n","\n","--- "]},{"cell_type":"markdown","metadata":{"id":"R3X-ddM7hU7n","colab_type":"text"},"source":["End of Chapter 1. I feel great right now... I've never felt more on track or motivated. I feel like I am finally learning and getting somewhere. Sure, I don't have much to show for it, but what's the rush? The knowledge I am gaining and cementing in my mind is invaluable. Not to mention, reading is great."]}]}
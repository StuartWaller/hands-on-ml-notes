{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 13: Loading and Preprocessing Data with TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    !pip install -q -U tfx==0.21.2\n",
    "    print(\"You can safely ignore the package incompatibility errors.\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"data\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data API Overview\n",
    "- TensorFlow's *Data API* makes it easy to ingest large datasets and preprocess them efficiently\n",
    "- the whole Data API revolves around the concept of a *dataset*: a sequence of data items\n",
    "- let's create a dataset entirely in RAM using `tf.data.Dataset.from_tensor_slices()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<TensorSliceDataset shapes: (), types: tf.int32>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X = tf.range(10)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the `from_tensor_slices()` function takes a tensor and creates a `tf.data.Dataset` whose elements are all the slices of `X`\n",
    "- so this dataset contains 10 items: tensor 0, 1, 2, ..., 9\n",
    "- we could have also written: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<RangeDataset shapes: (), types: tf.int64>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.range(10)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can simply iterate over a dataset's items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(1, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(3, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\ntf.Tensor(5, shape=(), dtype=int64)\ntf.Tensor(6, shape=(), dtype=int64)\ntf.Tensor(7, shape=(), dtype=int64)\ntf.Tensor(8, shape=(), dtype=int64)\ntf.Tensor(9, shape=(), dtype=int64)\n"
    }
   ],
   "source": [
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaining Transformations\n",
    "- once you have a dataset, you can apply all sorts of transformations to it by calling its transformation methods\n",
    "- each method returns a new dataset, so you can chain transformations like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int64)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int64)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int64)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int64)\ntf.Tensor([8 9], shape=(2,), dtype=int64)\n"
    }
   ],
   "source": [
    "dataset = dataset.repeat(3).batch(7) # set drop_remainder=True to drop final batch so all batches are same size\n",
    "for item in dataset:\n",
    "    print(item) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we first call the `repeat()` method on the original dataset, which returns a new dataset that will repeat the items of the original dataset 3 times\n",
    "- then we call the `batch()` method on this new dataset, which again, creates a new dataset that group the items of the previous dataset in batches of 7 items\n",
    "- the dataset methods do *not* modify datasets, they create new ones, so make sure to keep a reference to these new datasets (`dataset = ...`) or nothing will happen\n",
    "---\n",
    "- you can transform the items by calling the `map()` method\n",
    "- for example, this creates a new dataset with all items doubled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor([ 0  2  4  6  8 10 12], shape=(7,), dtype=int64)\ntf.Tensor([14 16 18  0  2  4  6], shape=(7,), dtype=int64)\ntf.Tensor([ 8 10 12 14 16 18  0], shape=(7,), dtype=int64)\ntf.Tensor([ 2  4  6  8 10 12 14], shape=(7,), dtype=int64)\ntf.Tensor([16 18], shape=(2,), dtype=int64)\n"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda x: x * 2) # (new dataset)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the `map()` method can apply any preprocessing you desire to your data\n",
    "---\n",
    "- while the `map()` method applies a transformation to each item, the `apply()` method applies a transformation to the whole dataset\n",
    "- for example, the following code applies the `unbatch()` function to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.apply(tf.data.experimental.unbatch()) # now deprecated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it is also possible to filter the dataset using the `filter()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "`predicate` return type must be convertible to a scalar boolean tensor. Was TensorSpec(shape=(None,), dtype=tf.bool, name=None).",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-eb5669cef959>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# keep only items < 10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# to only look at just a few items from a dataset (the take() method)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, predicate)\u001b[0m\n\u001b[0;32m   1717\u001b[0m           \u001b[0;31m`\u001b[0m\u001b[0mpredicate\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1718\u001b[0m     \"\"\"\n\u001b[1;32m-> 1719\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFilterDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredicate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1721\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransformation_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, predicate, use_legacy_function)\u001b[0m\n\u001b[0;32m   4076\u001b[0m                    \u001b[1;34m\"boolean tensor. Was {}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4077\u001b[0m                        wrapped_func.output_structure)\n\u001b[1;32m-> 4078\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4079\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predicate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4080\u001b[0m     variant_tensor = gen_dataset_ops.filter_dataset(\n",
      "\u001b[1;31mValueError\u001b[0m: `predicate` return type must be convertible to a scalar boolean tensor. Was TensorSpec(shape=(None,), dtype=tf.bool, name=None)."
     ]
    }
   ],
   "source": [
    "dataset = dataset.filter(lambda x: x < 10)  # keep only items < 10\n",
    "for item in dataset.take(3): # to only look at just a few items from a dataset (the take() method)\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the Data (w/ CSV Files)\n",
    "### Small Datasets\n",
    "- Gradient Descent works best when the instances in the training set are independent and identically distributed\n",
    "- a good way to ensure this is to use the `shuffle()` method, which creates a new dataset that starts by filling up a buffer with the first items from the source dataset\n",
    "- then, whenever it is asked for an item, it will pull one out randomly from the buffer and replace it with a fresh one from the source dataset until it has iterated through it\n",
    "- you must specify the buffer size, and it's crucial you make the buffer large enough\n",
    "- **for example, the following code creates and displays a dataset containing integers 0 to 9, repeated 3 times, shuffled using a buffer size of 3 and random seed of 42, and batched with a batch size of 7**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor([1 3 0 4 2 5 6], shape=(7,), dtype=int64)\ntf.Tensor([8 7 1 0 3 2 5], shape=(7,), dtype=int64)\ntf.Tensor([4 6 9 8 9 7 0], shape=(7,), dtype=int64)\ntf.Tensor([3 1 4 5 2 8 7], shape=(7,), dtype=int64)\n"
    }
   ],
   "source": [
    "tf.random.set_seed(42) # if you want the same random order every time you run your program\n",
    "\n",
    "dataset = tf.data.Dataset.range(10).repeat(3)\n",
    "dataset = dataset.shuffle(buffer_size=3, seed=42).batch(7, drop_remainder=True)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large Datasets\n",
    "- for a large dataset that does not fit in memory, this simple shuffling-buffer approach is not sufficient, as the buffer will be so small compared to the actual size of the dataset\n",
    "- **one solution is to shuffle the source data itself**\n",
    "- a common technique is to split the source data into multiple files, then read them in a random order during training\n",
    "- on top of that, you can indeed at a shuffling buffer using the `shuffle()` method\n",
    "---\n",
    "- let's start by loading and preparing the California housing dataset\n",
    "- we first load it, then split it into a training set, a validation set, and a test set, and finally we scale it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it, split it into a training set, a validation set, and a test set\n",
    "housing = fetch_california_housing()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target.reshape(-1, 1), random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the training set\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for a very large dataset that does not fit in memory, you will typically want to split it into many files first, then have TensorFlow read these files in parallel\n",
    "- to demonstrate this, let's start by splitting the housing dataset and save it to 20 CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_multiple_csv_files(data, name_prefix, header=None, n_parts=10):\n",
    "    housing_dir = os.path.join(\"datasets\", \"housing\")\n",
    "    os.makedirs(housing_dir, exist_ok=True)\n",
    "    path_format = os.path.join(housing_dir, \"my_{}_{:02d}.csv\")\n",
    "\n",
    "    filepaths = []\n",
    "    m = len(data)\n",
    "    for file_idx, row_indices in enumerate(np.array_split(np.arange(m), n_parts)):\n",
    "        part_csv = path_format.format(name_prefix, file_idx)\n",
    "        filepaths.append(part_csv)\n",
    "        with open(part_csv, \"wt\", encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header)\n",
    "                f.write(\"\\n\")\n",
    "            for row_idx in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_idx]]))\n",
    "                f.write(\"\\n\")\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.c_[X_train, y_train]\n",
    "valid_data = np.c_[X_valid, y_valid]\n",
    "test_data = np.c_[X_test, y_test]\n",
    "\n",
    "header_cols = housing.feature_names + [\"MedianHouseValue\"]\n",
    "header = \",\".join(header_cols)\n",
    "\n",
    "train_filepaths = save_to_multiple_csv_files(train_data, \"train\", header, n_parts=20)\n",
    "valid_filepaths = save_to_multiple_csv_files(valid_data, \"valid\", header, n_parts=10)\n",
    "test_filepaths = save_to_multiple_csv_files(test_data, \"test\", header, n_parts=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- okay, now let's take a peek at the first few lines of one of these CSV files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n0  3.5214      15.0  3.049945   1.106548      1447.0  1.605993     37.63   \n1  5.3275       5.0  6.490060   0.991054      3464.0  3.443340     33.69   \n2  3.1000      29.0  7.542373   1.591525      1328.0  2.250847     38.44   \n3  7.1736      12.0  6.289003   0.997442      1054.0  2.695652     33.55   \n4  2.0549      13.0  5.312457   1.085092      3297.0  2.244384     33.93   \n\n   Longitude  MedianHouseValue  \n0    -122.43             1.442  \n1    -117.39             1.687  \n2    -122.98             1.621  \n3    -117.70             2.621  \n4    -116.93             0.956  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>MedianHouseValue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.5214</td>\n      <td>15.0</td>\n      <td>3.049945</td>\n      <td>1.106548</td>\n      <td>1447.0</td>\n      <td>1.605993</td>\n      <td>37.63</td>\n      <td>-122.43</td>\n      <td>1.442</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.3275</td>\n      <td>5.0</td>\n      <td>6.490060</td>\n      <td>0.991054</td>\n      <td>3464.0</td>\n      <td>3.443340</td>\n      <td>33.69</td>\n      <td>-117.39</td>\n      <td>1.687</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.1000</td>\n      <td>29.0</td>\n      <td>7.542373</td>\n      <td>1.591525</td>\n      <td>1328.0</td>\n      <td>2.250847</td>\n      <td>38.44</td>\n      <td>-122.98</td>\n      <td>1.621</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.1736</td>\n      <td>12.0</td>\n      <td>6.289003</td>\n      <td>0.997442</td>\n      <td>1054.0</td>\n      <td>2.695652</td>\n      <td>33.55</td>\n      <td>-117.70</td>\n      <td>2.621</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0549</td>\n      <td>13.0</td>\n      <td>5.312457</td>\n      <td>1.085092</td>\n      <td>3297.0</td>\n      <td>2.244384</td>\n      <td>33.93</td>\n      <td>-116.93</td>\n      <td>0.956</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_csv(train_filepaths[0]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- or in text mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MedianHouseValue\n3.5214,15.0,3.0499445061043287,1.106548279689234,1447.0,1.6059933407325193,37.63,-122.43,1.442\n5.3275,5.0,6.490059642147117,0.9910536779324056,3464.0,3.4433399602385686,33.69,-117.39,1.687\n3.1,29.0,7.5423728813559325,1.5915254237288134,1328.0,2.2508474576271187,38.44,-122.98,1.621\n7.1736,12.0,6.289002557544757,0.9974424552429667,1054.0,2.6956521739130435,33.55,-117.7,2.621\n"
    }
   ],
   "source": [
    "with open(train_filepaths[0]) as f:\n",
    "    for i in range(5):\n",
    "        print(f.readline(), end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- let's suppose that `train_filepaths` contains the list of training file paths (and you have a `valid_filepaths` and `test_filepaths`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['datasets\\\\housing\\\\my_train_00.csv',\n 'datasets\\\\housing\\\\my_train_01.csv',\n 'datasets\\\\housing\\\\my_train_02.csv',\n 'datasets\\\\housing\\\\my_train_03.csv',\n 'datasets\\\\housing\\\\my_train_04.csv',\n 'datasets\\\\housing\\\\my_train_05.csv',\n 'datasets\\\\housing\\\\my_train_06.csv',\n 'datasets\\\\housing\\\\my_train_07.csv',\n 'datasets\\\\housing\\\\my_train_08.csv',\n 'datasets\\\\housing\\\\my_train_09.csv',\n 'datasets\\\\housing\\\\my_train_10.csv',\n 'datasets\\\\housing\\\\my_train_11.csv',\n 'datasets\\\\housing\\\\my_train_12.csv',\n 'datasets\\\\housing\\\\my_train_13.csv',\n 'datasets\\\\housing\\\\my_train_14.csv',\n 'datasets\\\\housing\\\\my_train_15.csv',\n 'datasets\\\\housing\\\\my_train_16.csv',\n 'datasets\\\\housing\\\\my_train_17.csv',\n 'datasets\\\\housing\\\\my_train_18.csv',\n 'datasets\\\\housing\\\\my_train_19.csv']"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "train_filepaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- alternatively, you could use file patterns\n",
    "- `list_files()` returns a dataset that shuffles the file paths, which is generally a good thing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(b'datasets\\\\housing\\\\my_train_15.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_08.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_03.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_01.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_10.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_05.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_19.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_16.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_02.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_09.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_00.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_07.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_12.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_04.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_17.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_11.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_14.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_18.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_06.csv', shape=(), dtype=string)\ntf.Tensor(b'datasets\\\\housing\\\\my_train_13.csv', shape=(), dtype=string)\n"
    }
   ],
   "source": [
    "for filepath in filepath_dataset:\n",
    "    print(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- next, you can call the `interleave()` method to read from 5 files at a time and interleave their lines (skipping the first line of each file, which is the header row, using the `skip()` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_readers = 5 # to read 5 files at a time\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1), # skipping the header row\n",
    "    cycle_length=n_readers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "b'4.6477,38.0,5.03728813559322,0.911864406779661,745.0,2.5254237288135593,32.64,-117.07,1.504'\nb'8.72,44.0,6.163179916317992,1.0460251046025104,668.0,2.794979079497908,34.2,-118.18,4.159'\nb'3.8456,35.0,5.461346633416459,0.9576059850374065,1154.0,2.8778054862842892,37.96,-122.05,1.598'\nb'3.3456,37.0,4.514084507042254,0.9084507042253521,458.0,3.2253521126760565,36.67,-121.7,2.526'\nb'3.6875,44.0,4.524475524475524,0.993006993006993,457.0,3.195804195804196,34.04,-118.15,1.625'\n"
    }
   ],
   "source": [
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- these are the first rows (ignoring the header rows) if 5 CSV files, chosen randomly\n",
    "- this looks good, but these are just byte strings: we need to parse them and scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data (w/ CSV Files)\n",
    "- let's implement a small function that will perform this preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 8 # X_train.shape[-1]\n",
    "\n",
    "@tf.function\n",
    "def preprocess(line): # to parse the byte strings and scale the data\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean) / X_std, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the code assumes that we have precomputed the mean and standard deviation of each feature in the training set\n",
    "- `X_mean` and `X_std` are just 1D tensors (or NumPy arrays) containing 8 floats (1 per input feature)\n",
    "- let's test this preprocessing function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n array([ 0.16579157,  1.216324  , -0.05204565, -0.39215982, -0.5277444 ,\n        -0.2633488 ,  0.8543046 , -1.3072058 ], dtype=float32)>,\n <tf.Tensor: shape=(1,), dtype=float32, numpy=array([2.782], dtype=float32)>)"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- looks good: now let's apply the function to the dataset and put everything together\n",
    "---\n",
    "### Putting Everything together\n",
    "- **to make the code reusable, let's put together everything we have discussed so far into a small helper function: it will create and return a dataset that will efficiently load California housing data from multiple CSV files, preprocess it, shuffle it, optionally repeat it, and batch it**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5, # reads 5 files at a time\n",
    "                       n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                       n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths).repeat(repeat)\n",
    "    dataset = dataset.interleave(\n",
    "        lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "        cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads) # map() transforms the data\n",
    "    dataset = dataset.batch(batch_size) # batches of 32\n",
    "    return dataset.prefetch(1) # prefetch(1) is important for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefetching\n",
    "- by calling `prefetch(1)` at the end, we are creating a dataset that will do its best to always be one batch ahead\n",
    "- in other words, while our training algorithm is working on one batch, the dataset will already be working in parallel on getting the next batch ready, which can drastically improve performance\n",
    "- in general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more\n",
    "- with prefetching, the CPU and the GPU work in parallel: as the GPU works on one batch, the CPU works on the next\n",
    "---\n",
    "- **a large amount of RAM is crucial for computer vision**\n",
    "---\n",
    "- if the dataset is small enough to fit in memory, you can significantly speed up training by using the dataset's `cache()` method to cache its content to RAM\n",
    "- you should do this after loading and preprocessing the data, but before shuffling, repeating, batching, and prefetching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Dataset with tf.keras (w/ CSV Files)\n",
    "- now we can use the `csv_reader_dataset()` function to create a dataset for the training set\n",
    "- note that we do not need to repeat it, as this will be taken care of by `tf.keras`\n",
    "- more specifically, the `fit()` method will take care of repeating the training set\n",
    "- we also create datasets for the validation set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = csv_reader_dataset(train_filepaths, repeat=None) \n",
    "valid_set = csv_reader_dataset(valid_filepaths) \n",
    "test_set = csv_reader_dataset(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we can simply build and train a Keras model using these datasets\n",
    "- all we need to do it pass the training and validation datasets to the `fit()` method, instead of `X_train`, `y_train`, `X_valid`, and `y_valid`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 362 steps\nEpoch 1/10\n362/362 [==============================] - 1s 3ms/step - loss: 1.4500 - val_loss: 21.6255\nEpoch 2/10\n362/362 [==============================] - 0s 1ms/step - loss: 0.9054 - val_loss: 0.7099\nEpoch 3/10\n362/362 [==============================] - 0s 1ms/step - loss: 0.6092 - val_loss: 0.6093\nEpoch 4/10\n362/362 [==============================] - 0s 1ms/step - loss: 0.6023 - val_loss: 0.5710\nEpoch 5/10\n362/362 [==============================] - 0s 1ms/step - loss: 0.5790 - val_loss: 0.6348\nEpoch 6/10\n362/362 [==============================] - 0s 1ms/step - loss: 0.5521 - val_loss: 0.6665\nEpoch 7/10\n362/362 [==============================] - 0s 1ms/step - loss: 0.5219 - val_loss: 0.5159\nEpoch 8/10\n362/362 [==============================] - 0s 1ms/step - loss: 0.5318 - val_loss: 0.4932\nEpoch 9/10\n362/362 [==============================] - 0s 1ms/step - loss: 0.4981 - val_loss: 0.4707\nEpoch 10/10\n362/362 [==============================] - 0s 1ms/step - loss: 0.4712 - val_loss: 0.4675\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2ce5d4d2be0>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(train_set, steps_per_epoch=len(X_train) // batch_size, epochs=10,\n",
    "          validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can pass a dataset top the `evaluate()` and `predict()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "161/161 [==============================] - 0s 1ms/step - loss: 0.4788\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.47875127770145487"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=len(X_test) // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[1.772925 ],\n       [1.4905939],\n       [2.510351 ],\n       ...,\n       [1.8501377],\n       [1.2474439],\n       [2.8345075]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "new_set = test_set.map(lambda X, y: X) # we could instead just pass test_set, Keras would ignore the labels\n",
    "X_new = X_test\n",
    "model.predict(new_set, steps=len(X_new) // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- unlike the other sets, the `new_set` will usually not contain labels (if it does, Keras will ignore them)\n",
    "---\n",
    "- congratulations, you now know how to build powerful input pipelines using the Data API\n",
    "- however, so far we have used CSV files, which are common, simple, and convenient, but not really efficient, and do not support large or complex data structures (such as images or audio) very well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TFRecord Format\n",
    "- the TFRecord format is TensorFlow's preferred format for storing large amounts of data and reading it efficiently\n",
    "- it is a very simple binary format that just contains a sequence of binary records of varying sizes\n",
    "- you can easily *create* a TFRecord file using the `tf.io.TFRecordWriter` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_data.tfrecord\") as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- and you can then use a `tf.data.TFRecordDataset` to read one or more TFRecord files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(b'This is the first record', shape=(), dtype=string)\ntf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
    }
   ],
   "source": [
    "filepaths = [\"my_data.tfrecord\"]\n",
    "dataset = tf.data.TFRecordDataset(filepaths)\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- by default, a `TFRecordDataset` will read files one by one, but you can make it read multiple files in parallel and interleave their records by setting `num_parallel_reads`\n",
    "---\n",
    "### Compressed TFRecord files\n",
    "- it can sometimes be useful to compress your TFRecord files, especially if they need to be loaded via a network connection\n",
    "- you can create a compressed TFRecord file by setting the `options` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compressing TFRecord files\n",
    "options = tf.io.TFRecordOptions(compression_type=\"GZIP\") \n",
    "with tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f:\n",
    "    f.write(b\"This is the first record\")\n",
    "    f.write(b\"And this is the second record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- and when reading a compressed TFRecord file, you need to specify the compression type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(b'This is the first record', shape=(), dtype=string)\ntf.Tensor(b'And this is the second record', shape=(), dtype=string)\n"
    }
   ],
   "source": [
    "# specifying compression type when reading compressed TFRecord file\n",
    "dataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"],\n",
    "                                  compression_type=\"GZIP\") \n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Brief Introduction to Protocol Buffers\n",
    "- even though each record can use any binary format you want, TFRecord files usually contain serialized protocal buffers (*protobufs*)\n",
    "- this is a portable, extensible, and efficient binary format\n",
    "- protobufs are defined using a simple language that looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overwriting person.proto\n"
    }
   ],
   "source": [
    "%%writefile person.proto\n",
    "syntax = \"proto3\";\n",
    "message Person {\n",
    "  string name = 1;\n",
    "  int32 id = 2;\n",
    "  repeated string email = 3;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this definition says we are using version 3 of the protobuf format, and it specifies that each `Person` object may (optionally) have a `name` of type `string`, an `id` of type `int32`, and zero or more `email` fields, each of type `string`\n",
    "- the numbers `1`, `2`, and `3` are the field identifiers: they will be used in each record's binary representation\n",
    "- once you have a definition in a *.proto* file, you can compile it\n",
    "- to illustrate the basics, let's look at a simple example that uses the access classes generated for the `Person` protobuf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "name: \"Al\"\nid: 123\nemail: \"a@b.com\"\n\n"
    }
   ],
   "source": [
    "from person_pb2 import Person\n",
    "\n",
    "person = Person(name=\"Al\", id=123, email=[\"a@b.com\"])  # create a Person\n",
    "print(person)  # display the Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Al'"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "person.name  # read a field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.name = \"Alice\"  # modify a field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'a@b.com'"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "person.email[0]  # repeated fields can be accessed like arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "person.email.append(\"c@d.com\")  # add an email address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "b'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'"
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "s = person.SerializeToString()  # serialize to a byte string\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "27"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "person2 = Person()  # create a new Person\n",
    "person2.ParseFromString(s)  # parse the byte string (27 bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "person == person2  # now they are equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we could save the serialized `Person` object to a TFRecord file, then we could load and parse it\n",
    "- however, `SerializeToString()` and `ParseFromString()` are not TensorFlow operations (and neither are the other operations in the above code), so they cannot be included in a TensorFlow function\n",
    "- fortunately, TensorFlow does include special protobuf definitions for which it provides parsing operations\n",
    "---\n",
    "### TensorFlow Protobufs\n",
    "- the main protobuf used in a TFRecord file is the `Example` protobuf, which represents one instance in a dataset\n",
    "- it contains a list of named features, where each feature can either be a list of byte strings, a list of floats, or a list of integers\n",
    "- here is the protobuf definition:\n",
    "```proto\n",
    "syntax = \"proto3\";\n",
    "\n",
    "message BytesList { repeated bytes value = 1; }\n",
    "message FloatList { repeated float value = 1 [packed = true]; }\n",
    "message Int64List { repeated int64 value = 1 [packed = true]; }\n",
    "message Feature {\n",
    "    oneof kind {\n",
    "        BytesList bytes_list = 1;\n",
    "        FloatList float_list = 2;\n",
    "        Int64List int64_list = 3;\n",
    "    }\n",
    "};\n",
    "message Features { map<string, Feature> feature = 1; };\n",
    "message Example { Features features = 1; };\n",
    "```\n",
    "- here is how you could create a `tf.train.Example` representing the same person as earlier and write it to a TFRecord file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BytesList = tf.train.BytesList\n",
    "FloatList = tf.train.FloatList\n",
    "Int64List = tf.train.Int64List\n",
    "Feature = tf.train.Feature\n",
    "Features = tf.train.Features\n",
    "Example = tf.train.Example\n",
    "\n",
    "person_example = Example(\n",
    "    features=Features(\n",
    "        feature={\n",
    "            \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\n",
    "            \"id\": Feature(int64_list=Int64List(value=[123])),\n",
    "            \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"]))\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this code is a bit verbose and repetitive, but it's rather straightforward\n",
    "- now that we have an `Example` protobuf, we can serialize it by calling its `SerializeToString()` method, then write the resulting data to a TFRecord file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f:\n",
    "    f.write(person_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now that we have a nice TFRecord file containing a serialized `Example`, let's try to load it:\n",
    "---\n",
    "### Loading and Parsing Examples\n",
    "- to load the serialized `Example` protobufs, we will use a `tf.data.TFRecordDataset` once again, and we will parse each `Example` using `tf.io.parse_single_example()`\n",
    "- the following code defines a description dictionary, then it iterates over the `TFRecordDataset` and parses the serialized `Example` protobuf this dataset contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "    \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"emails\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "for serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]):\n",
    "    parsed_example = tf.io.parse_single_example(serialized_example,\n",
    "                                                feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'emails': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x2ce5f85bcf8>,\n 'id': <tf.Tensor: shape=(), dtype=int64, numpy=123>,\n 'name': <tf.Tensor: shape=(), dtype=string, numpy=b'Alice'>}"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "parsed_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can convert a sparse tensor to a dense tensor using `tf.sparse.to_dense()`, but in this case, it is simpler just to access its values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'a@b.com', b'c@d.com'], dtype=object)>"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "parsed_example[\"emails\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Lists of Lists Using the SequenceExample Protobuf\n",
    "- here is the definition of the `SequenceExample` protobuf:\n",
    "```proto\n",
    "syntax = \"proto3\";\n",
    "\n",
    "message FeatureList { repeated Feature feature = 1; };\n",
    "message FeatureLists { map<string, FeatureList> feature_list = 1; };\n",
    "message SequenceExample {\n",
    "  Features context = 1;\n",
    "  FeatureLists feature_lists = 2;\n",
    "};\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureList = tf.train.FeatureList\n",
    "FeatureLists = tf.train.FeatureLists\n",
    "SequenceExample = tf.train.SequenceExample\n",
    "\n",
    "context = Features(feature={\n",
    "    \"author_id\": Feature(int64_list=Int64List(value=[123])),\n",
    "    \"title\": Feature(bytes_list=BytesList(value=[b\"A\", b\"desert\", b\"place\", b\".\"])),\n",
    "    \"pub_date\": Feature(int64_list=Int64List(value=[1623, 12, 25]))\n",
    "})\n",
    "\n",
    "content = [[\"When\", \"shall\", \"we\", \"three\", \"meet\", \"again\", \"?\"],\n",
    "           [\"In\", \"thunder\", \",\", \"lightning\", \",\", \"or\", \"in\", \"rain\", \"?\"]]\n",
    "comments = [[\"When\", \"the\", \"hurlyburly\", \"'s\", \"done\", \".\"],\n",
    "            [\"When\", \"the\", \"battle\", \"'s\", \"lost\", \"and\", \"won\", \".\"]]\n",
    "\n",
    "def words_to_feature(words):\n",
    "    return Feature(bytes_list=BytesList(value=[word.encode(\"utf-8\")\n",
    "                                               for word in words]))\n",
    "\n",
    "content_features = [words_to_feature(sentence) for sentence in content]\n",
    "comments_features = [words_to_feature(comment) for comment in comments]\n",
    "            \n",
    "sequence_example = SequenceExample(\n",
    "    context=context,\n",
    "    feature_lists=FeatureLists(feature_list={\n",
    "        \"content\": FeatureList(feature=content_features),\n",
    "        \"comments\": FeatureList(feature=comments_features)\n",
    "    }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "context {\n  feature {\n    key: \"author_id\"\n    value {\n      int64_list {\n        value: 123\n      }\n    }\n  }\n  feature {\n    key: \"pub_date\"\n    value {\n      int64_list {\n        value: 1623\n        value: 12\n        value: 25\n      }\n    }\n  }\n  feature {\n    key: \"title\"\n    value {\n      bytes_list {\n        value: \"A\"\n        value: \"desert\"\n        value: \"place\"\n        value: \".\"\n      }\n    }\n  }\n}\nfeature_lists {\n  feature_list {\n    key: \"comments\"\n    value {\n      feature {\n        bytes_list {\n          value: \"When\"\n          value: \"the\"\n          value: \"hurlyburly\"\n          value: \"\\'s\"\n          value: \"done\"\n          value: \".\"\n        }\n      }\n      feature {\n        bytes_list {\n          value: \"When\"\n          value: \"the\"\n          value: \"battle\"\n          value: \"\\'s\"\n          value: \"lost\"\n          value: \"and\"\n          value: \"won\"\n          value: \".\"\n        }\n      }\n    }\n  }\n  feature_list {\n    key: \"content\"\n    value {\n      feature {\n        bytes_list {\n          value: \"When\"\n          value: \"shall\"\n          value: \"we\"\n          value: \"three\"\n          value: \"meet\"\n          value: \"again\"\n          value: \"?\"\n        }\n      }\n      feature {\n        bytes_list {\n          value: \"In\"\n          value: \"thunder\"\n          value: \",\"\n          value: \"lightning\"\n          value: \",\"\n          value: \"or\"\n          value: \"in\"\n          value: \"rain\"\n          value: \"?\"\n        }\n      }\n    }\n  }\n}"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "sequence_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_sequence_example = sequence_example.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_feature_descriptions = {\n",
    "    \"author_id\": tf.io.FixedLenFeature([], tf.int64, default_value=0),\n",
    "    \"title\": tf.io.VarLenFeature(tf.string),\n",
    "    \"pub_date\": tf.io.FixedLenFeature([3], tf.int64, default_value=[0, 0, 0]),\n",
    "}\n",
    "sequence_feature_descriptions = {\n",
    "    \"content\": tf.io.VarLenFeature(tf.string),\n",
    "    \"comments\": tf.io.VarLenFeature(tf.string),\n",
    "}\n",
    "parsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example(\n",
    "    serialized_sequence_example, context_feature_descriptions,\n",
    "    sequence_feature_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'title': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x2ce522c8470>,\n 'author_id': <tf.Tensor: shape=(), dtype=int64, numpy=123>,\n 'pub_date': <tf.Tensor: shape=(3,), dtype=int64, numpy=array([1623,   12,   25], dtype=int64)>}"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "parsed_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(4,), dtype=string, numpy=array([b'A', b'desert', b'place', b'.'], dtype=object)>"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "parsed_context[\"title\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'comments': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x2ce5d2eff28>,\n 'content': <tensorflow.python.framework.sparse_tensor.SparseTensor at 0x2ce5d2efcc0>}"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "parsed_feature_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<tf.RaggedTensor [[b'When', b'shall', b'we', b'three', b'meet', b'again', b'?'], [b'In', b'thunder', b',', b'lightning', b',', b'or', b'in', b'rain', b'?']]>\n"
    }
   ],
   "source": [
    "print(tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Features API\n",
    "### Preprocessing the Input Features\n",
    "- preparing your data for a neural network requires converting all features into numerical features, generally normalizing them, and more\n",
    "- in particular, if your data contains categorical features or text features, they need to be converted to numbers\n",
    "- **as an example, let's use the variant of the California housing dataset that we used in Chapter 2, since it contains categorical features and missing values**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    os.makedirs(housing_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n0    -122.23     37.88                41.0        880.0           129.0   \n1    -122.22     37.86                21.0       7099.0          1106.0   \n2    -122.24     37.85                52.0       1467.0           190.0   \n3    -122.25     37.85                52.0       1274.0           235.0   \n4    -122.25     37.85                52.0       1627.0           280.0   \n\n   population  households  median_income  median_house_value ocean_proximity  \n0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n4       565.0       259.0         3.8462            342200.0        NEAR BAY  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>housing_median_age</th>\n      <th>total_rooms</th>\n      <th>total_bedrooms</th>\n      <th>population</th>\n      <th>households</th>\n      <th>median_income</th>\n      <th>median_house_value</th>\n      <th>ocean_proximity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-122.23</td>\n      <td>37.88</td>\n      <td>41.0</td>\n      <td>880.0</td>\n      <td>129.0</td>\n      <td>322.0</td>\n      <td>126.0</td>\n      <td>8.3252</td>\n      <td>452600.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-122.22</td>\n      <td>37.86</td>\n      <td>21.0</td>\n      <td>7099.0</td>\n      <td>1106.0</td>\n      <td>2401.0</td>\n      <td>1138.0</td>\n      <td>8.3014</td>\n      <td>358500.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-122.24</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1467.0</td>\n      <td>190.0</td>\n      <td>496.0</td>\n      <td>177.0</td>\n      <td>7.2574</td>\n      <td>352100.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1274.0</td>\n      <td>235.0</td>\n      <td>558.0</td>\n      <td>219.0</td>\n      <td>5.6431</td>\n      <td>341300.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-122.25</td>\n      <td>37.85</td>\n      <td>52.0</td>\n      <td>1627.0</td>\n      <td>280.0</td>\n      <td>565.0</td>\n      <td>259.0</td>\n      <td>3.8462</td>\n      <td>342200.0</td>\n      <td>NEAR BAY</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### housing_median_age\n",
    "- `tf.feature_column.numeric_column()` represents real valued or numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1.0 52.0\n"
    }
   ],
   "source": [
    "print(housing.housing_median_age.min(), housing.housing_median_age.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NumericColumn(key='housing_median_age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "housing_median_age = tf.feature_column.numeric_column(\"housing_median_age\")\n",
    "housing_median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NumericColumn(key='housing_median_age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function <lambda> at 0x000002CE62FACD08>)"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_mean = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "\n",
    "age_mean, age_std = X_mean[1], X_std[1]\n",
    "housing_median_age = tf.feature_column.numeric_column(\n",
    "    \"housing_median_age\", normalizer_fn=lambda x: (x - age_mean) / age_std) # the most common use case of this function is normalization\n",
    "housing_median_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `tf.feature_column.bucketized_column()` returns a bucketized column\n",
    "- buckets include the left boundary, and exclude the right boundary: namely, `boundaries=[0., 1., 2.]` generates buckets `(-inf, 0.)`, `[0., 1.)`, `[1., 2.)`, and `[2., +inf)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BucketizedColumn(source_column=NumericColumn(key='housing_median_age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function <lambda> at 0x000002CE348E1048>), boundaries=(-1.0, -0.5, 0.0, 0.5, 1.0))"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "bucketized_age = tf.feature_column.bucketized_column(\n",
    "    housing_median_age, boundaries=[-1., -0.5, 0., 0.5, 1.]) # age was just scaled\n",
    "bucketized_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### median_income\n",
    "- again using `tf.feature_column.bucketized_column()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.4999 15.0001\n"
    }
   ],
   "source": [
    "print(housing.median_income.min(), housing.median_income.max()) # hence our boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BucketizedColumn(source_column=NumericColumn(key='median_income', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(1.5, 3.0, 4.5, 6.0))"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "median_income = tf.feature_column.numeric_column(\"median_income\")\n",
    "bucketized_income = tf.feature_column.bucketized_column(\n",
    "    median_income, boundaries=[1.5, 3., 4.5, 6.])\n",
    "bucketized_income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ocean_proximity\n",
    "- we start by making a vocabulary list of all the categorical features in the `ocean_proximity` column\n",
    "- `tf.feature_column.categorical_column_with_vocabulary_list()` is to be used when your inputs are in string or integer format, and you have an in-memory vocabulary mapping each value to an integer ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "VocabularyListCategoricalColumn(key='ocean_proximity', vocabulary_list=('<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'), dtype=tf.string, default_value=-1, num_oov_buckets=0)"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "ocean_prox_vocab = ['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN']\n",
    "ocean_proximity = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    \"ocean_proximity\", ocean_prox_vocab) # column name, vocabulary list\n",
    "ocean_proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example using tf.feature_column.categorical_column_with_hash_bucket\n",
    "- `tf.feature_column.categorical_column_with_hash_bucket()` is to be used when your sparse features are in string or integer format, and you want to distribute your inputs into a finite number of buckets by hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "HashedCategoricalColumn(key='city', hash_bucket_size=1000, dtype=tf.string)"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "# just an example, it's not used later on\n",
    "city_hash = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    \"city\", hash_bucket_size=1000)\n",
    "city_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating age_and_ocean_proximity (a crossed column)\n",
    "- `tf.feature_column.crossed_column()` returns a column for performing crosses of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "CrossedColumn(keys=(BucketizedColumn(source_column=NumericColumn(key='housing_median_age', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=<function <lambda> at 0x000002CE348E1048>), boundaries=(-1.0, -0.5, 0.0, 0.5, 1.0)), VocabularyListCategoricalColumn(key='ocean_proximity', vocabulary_list=('<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'), dtype=tf.string, default_value=-1, num_oov_buckets=0)), hash_bucket_size=100, hash_key=None)"
     },
     "metadata": {},
     "execution_count": 89
    }
   ],
   "source": [
    "age_and_ocean_proximity = tf.feature_column.crossed_column(\n",
    "    [bucketized_age, ocean_proximity], hash_bucket_size=100) # number of buckets\n",
    "age_and_ocean_proximity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### latitude\n",
    "- the code below provides insight to the creation of our boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "32.54 41.95\n"
    }
   ],
   "source": [
    "print(housing.latitude.min(), housing.latitude.max()) # hence boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "NumericColumn(key='latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)"
     },
     "metadata": {},
     "execution_count": 91
    }
   ],
   "source": [
    "latitude = tf.feature_column.numeric_column(\"latitude\")\n",
    "latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[32.0, 32.55555555555556, 33.111111111111114, 33.666666666666664, 34.22222222222222, 34.77777777777778, 35.333333333333336, 35.888888888888886, 36.44444444444444, 37.0, 37.55555555555556, 38.111111111111114, 38.666666666666664, 39.22222222222222, 39.77777777777778, 40.333333333333336, 40.888888888888886, 41.44444444444444, 42.0]\n"
    }
   ],
   "source": [
    "print(list(np.linspace(32., 42., 20 - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BucketizedColumn(source_column=NumericColumn(key='latitude', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), boundaries=(32.0, 32.55555555555556, 33.111111111111114, 33.666666666666664, 34.22222222222222, 34.77777777777778, 35.333333333333336, 35.888888888888886, 36.44444444444444, 37.0, 37.55555555555556, 38.111111111111114, 38.666666666666664, 39.22222222222222, 39.77777777777778, 40.333333333333336, 40.888888888888886, 41.44444444444444, 42.0))"
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "bucketized_latitude = tf.feature_column.bucketized_column(\n",
    "    latitude, boundaries=list(np.linspace(32., 42., 20 - 1)))\n",
    "bucketized_latitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-124.35 -114.31\n"
    }
   ],
   "source": [
    "print(housing.longitude.min(), housing.longitude.max()) # hence our boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "longitude = tf.feature_column.numeric_column(\"longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketized_longitude = tf.feature_column.bucketized_column(\n",
    "    longitude, boundaries=list(np.linspace(-125., -114., 20 - 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating location\n",
    "- again using `tf.feature_column.crossed_column()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = tf.feature_column.crossed_column(\n",
    "    [bucketized_latitude, bucketized_longitude], hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating ocean_proximity_one_hot\n",
    "- `tf.feature_column.indicator_column()` represents the multi-hot representation of a given categorical column\n",
    "- for a DNN model, an `indicator_column` can be used to wrap any `categorical_column` (e.g., to feed to DNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "IndicatorColumn(categorical_column=VocabularyListCategoricalColumn(key='ocean_proximity', vocabulary_list=('<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'), dtype=tf.string, default_value=-1, num_oov_buckets=0))"
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "source": [
    "ocean_proximity_one_hot = tf.feature_column.indicator_column(ocean_proximity)\n",
    "ocean_proximity_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating ocean_proximity_embed\n",
    "- `tf.feature_column.embedding_column()` is to be used when your inputs are sparse, but you want to convert them to a dense representation (e.g., to feed to a DNN)\n",
    "- input must be a `categorical_column`\n",
    "- we use an `embedding_column` if the number of buckets/unique(values) are large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "EmbeddingColumn(categorical_column=VocabularyListCategoricalColumn(key='ocean_proximity', vocabulary_list=('<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'), dtype=tf.string, default_value=-1, num_oov_buckets=0), dimension=2, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x000002CE643DB128>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, max_norm=None, trainable=True)"
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "ocean_proximity_embed = tf.feature_column.embedding_column(ocean_proximity,\n",
    "                                                           dimension=2)\n",
    "ocean_proximity_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF Transform\n",
    "- if preprocessing is computationally expensive, then handling it before training rather than on the fly may give you a significant speed up: the data will be preprocessed just once per instance *before* training, rather than once per instance and per epoch *during* training\n",
    "- if the dataset is small enough to fit in RAM, you can use its `cache()` method\n",
    "- if the dataset is too large, then tools like Apache Beam or Spark will help as they let you run efficient data processing pipelines over large amounts of data\n",
    "---\n",
    "- this works great and indeed can speed up training, but there is one problem: once your model is trained, suppose you want to deploy it to a mobile app\n",
    "- in that case, you will need to write some code in your app to preprocess the data before it is fed to the model\n",
    "- this can become a maintenance nightmare, as whenever you want to change the preprocessing logic, you will need to update your Apache code and your mobile app code, which is very time consuming and can introduce a lot of bugs\n",
    "---\n",
    "- one improvement would be to take the trained model (trained on data that was preprocessed by your Apache Beam or Spark code) and, before deploying it to your app, add extra preprocessing layers to take care of preprocessing on the fly\n",
    "---\n",
    "- **however, if this all sounds like a hassle and you just want to define your preprocessing operations just once, then TF Transform is right for you**\n",
    "- TF Transform is part of TensorFlow Extended (TFX), an end-to-end platform for productionizing TensorFlow models\n",
    "- first, to use a TFX component such as TF Transform, you must install it; it does not come bundled with TensorFlow\n",
    "- second, you define your preprocessing function just once (in Python) by using TF Transform functions for scaling, bucketizing, and more (you can also use any TensorFlow operations you need)\n",
    "- here is what this preprocessing function might look like if we just had 2 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tensorflow_transform as tft\n",
    "\n",
    "    def preprocess(inputs):  # inputs is a batch of input features\n",
    "        median_age = inputs[\"housing_median_age\"]\n",
    "        ocean_proximity = inputs[\"ocean_proximity\"]\n",
    "        standardized_age = tft.scale_to_z_score(median_age - tft.mean(median_age))\n",
    "        ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity)\n",
    "        return {\n",
    "            \"standardized_median_age\": standardized_age,\n",
    "            \"ocean_proximity_id\": ocean_proximity_id\n",
    "        }\n",
    "except ImportError:\n",
    "    print(\"TF Transform is not installed. Try running: pip3 install -U tensorflow-transform\") # sweet, it worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- next, TF Transform lets you apply this `preprocess()` function to the whole training set using Apache Beam (it provides an `AnalyzeAndTransformDataset` class that you can use for this purpose in your Apache Beam pipeline) \n",
    "- in this process, it will also compute all the necessary statistics over the whole training set: in the example above, the mean and standard deviation of the `housing_median_age` feature, and the vocabulary for the `ocean_proximity` feature\n",
    "- the components that compute these statistics are called *analyzers*\n",
    "---\n",
    "- importantly, TF Transform will also generate an equivalent TensorFlow Function that you can plug into the model you deploy\n",
    "- this TF Function includes some constants that correspond to all the necessary statistics computed by Apache Beam (the mean, standard deviation, and vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TensorFlow Datasets (TFDS) Project\n",
    "- the TFDS project makes it very easy to download common datasets\n",
    "- the list includes image datasets, text datasets, audio datasets, and video datasets\n",
    "- visit http://homl.info/tfds to view the full list along with descriptions\n",
    "- for example, let's download MNIST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: 11.06 MiB, generated: 21.00 MiB, total: 32.06 MiB) to C:\\Users\\stuar\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\nWARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\nlocal data directory. If you'd instead prefer to read directly from our public\nGCS bucket (recommended if you're running on GCP), you can instead pass\n`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.\n\nDl Completed...: 100%|██████████| 4/4 [00:02<00:00,  1.36 file/s]\n\n\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\stuar\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- you can apply any transformation you want (typically shuffling, batching, and prefetching), and you're ready to train your model\n",
    "- the `load()` function shuffles each data shard it downloads (only for the training set), which may not be sufficient, so it's best to shuffle the training data some more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x216 with 5 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"87.242263pt\" version=\"1.1\" viewBox=\"0 0 349.2 87.242263\" width=\"349.2pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 87.242263 \r\nL 349.2 87.242263 \r\nL 349.2 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g clip-path=\"url(#p7ec8c225b3)\">\r\n    <image height=\"58\" id=\"image216a8adfdb\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"7.2\" xlink:href=\"data:image/png;base64,\r\niVBORw0KGgoAAAANSUhEUgAAADoAAAA6CAYAAADhu0ooAAAABHNCSVQICAgIfAhkiAAABwVJREFUaIHtmk1vEu0ax3/3vMEMrwOlpZVgW5CFsRIbo1EXdWOiCz+De7+F38AvYeLauLfRxJULE99SadM2balQoEMpDG8DZ+FhTprj8xwTGOzx8b8jgflfv9zXfXFd9z1iOBwO+QdI+tUBTEt/QH83/WNAFa8NHMfBtm2GwyFCCCRJQtd1hBBeW5+R56Bfv37l6dOnNJtNdF1nYWGBx48fMzc357X1GXkOenx8zPr6OpZlYRgGuVyOR48eeW37X/J8jwohUFUVSZJoNpvU63Wq1Sq1Wo1+v++1vaupgCrK98TpdDrYtk2j0eD09BTHcby2d+U5qKZpzM7OEo/HcRyHcrnM8+fPefbsGXt7ezSbzakAew6qKAqRSIRgMIjjOFiWxZs3b3j9+jVHR0e0220Gg4HXYXhfjIAzII7jUK1WUVWVL1++0O/3WVlZIRqNehrDVEABRrNDr9fDsixkWWZzcxNZlslms///oKZpsra2xtbWFjs7OyiKQiKRYGFhgdXVVbLZLKFQyOswpgN69+5dEokEL1++RAjBxYsXWVxc5Nq1a2QyGa9DAKYAqmkaiUSCWCyGqqo4jkOxWEQIQbFYJBQKYZomqqp6GsdUQOfn5ymVSqiqSq/XY3t7m1arxc7ODoFAAF3XPQed2vQy2puJRAJZlun3+1QqFUqlEt1u13t/zx3+LV3XyWQyCCFcuN3dXVRVZWVlxXP/qa2opmmkUilSqRSKojAYDKjX61iWNZWed2qgwWCQ1dVV8vk8fr+fXq/HwcEBu7u7tNttz/3HTt1qtUqpVKLZbGJZFrquE4vFkGXZnVpkWabZbAIgyzKSJOE4DkdHR/j9fmzbxnEcJEnybCAfG/Tjx4+8ePGCra0t3r17Rzqd5s6dOxiGQTQaRVVVgsEgiqKg6zqS9D2JOp0Onz9/plKpUKlUsG0bXdeRZXlsqB9pbNDRGNbtdqnVavh8Pra2tvD7/YTDYRdQVVUMw6BSqbhVttfr0e12qdfrHB8foyjK+QWNRqNks1kODw/p9/vs7e1RKpXc86GRhBAIIRgOh24aA3S7XQqFAoZhcP36dfx+/7gh/VBjFyPDMJidnSUcDiPLMsPhkFarRbvdptfrMRwO0TQNVVVdWFVV8fl8Lni5XGZ/fx/btifB9EONvaKpVIp4PM7BwQGhUAjbts8chEUiEZaWlhBC0Gw2abfb7O3tYds2pVKJdrvN+vo6hUKBfD5POp32pCCNDappGrIsEw6HicViWJZFs9lEVVVM0ySRSLC8vOxW3na7jaZp1Ot16vU6tm1jWRaaptFqtej1eiiKcibtJ6GxQUd/CYuLi9y7d4+NjQ1evXpFPB7n9u3bZDIZHjx4gK7rDAYDer0eR0dH7O/v8+TJEzY3N/n27RvNZpODgwPK5TLxeBxd1yfB52oiLaAQAsMwmJ+fp1wuI4RAlmUikQimaZJMJt3AHcchEAggyzKmaRIMBqnVarTbbRqNBpZlEQqFzicowNzcHDdu3OD09PRv006WZaLRKP1+n7W1NZLJJK9evcKyLN6/f0+/3+f+/ftEIpFJhQZMsAX0+XzEYjGCwaBbXf9KiqLg9/uJx+PMzs66I1qtVuPw8NCT6juxFR11QD+bcpIkEYlEiEajKIqC4zjs7u7iOA4nJyeTCus/fpN60KhD+tlqKYRA13UMw3D/fxuNBrVajU6nw6Qv4ie2oqNWrtVq/dT3fT4fly9fJhwOEw6HEUJQrVbpdrtUKhUsyyIQCKBp2kTimxio4zh0Oh36/b7b/v3dNCLLMolEgl6v566qbdtuY9FqtfD5fBMDnVjqDgYDut0umqZx4cIF0uk0uVyOdDr9w0ZdkiTC4TAzMzMsLy+TzWZRFIVWq8WnT594+/YtpVJpUuFNDnQ4HDIYDFBV1a2mc3NzmKb5w30rhMDn8xEIBIjH4yQSCffw7PDwkO3tbRqNxqTCm1zqRqNRcrkcsViMpaUl9/P/mjElSSKRSJBMJikUCgwGA2q1Gvv7+2emnHE1MVDDMDAMg4WFBa5evfrTv5MkiVAoRDwed68XT05OqFQqEz1i+eUva6iqSj6f5+bNm8zMzCBJEq1Wi1qthmVZ1Ot1Op3O2D5TO+78K6mqSi6XwzRNotEoQghs26Zer3NyckKj0UCWZXw+31g+v3xFhRAEAgFM0ySdTpPJZOh2uxSLRTY2Nvjw4QOVSmVsn3MBGgwGMU2TxcVFMpkM/X6fYrFIoVD4fUBHkiSJVCpFLpcjGAwC39Pa7/e7RWqs54/9hAlpdCGcz+fdS2FN0zAMYyKgv7wYjSRJEslkEk3TePjwIVeuXOHWrVtcunQJ0zTHfr44T+/rjkIZDAZnXqmbxGHZuQL1Uudmj3qtP6C/m/6A/m76A/q76R8D+i/x7rmTXaVnNgAAAABJRU5ErkJggg==\" y=\"-22.042263\"/>\r\n   </g>\r\n   <g id=\"text_1\">\r\n    <!-- 4 -->\r\n    <defs>\r\n     <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n    </defs>\r\n    <g transform=\"translate(32.244569 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-52\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_2\">\r\n   <g clip-path=\"url(#p98dbc9bdb3)\">\r\n    <image height=\"58\" id=\"image53315c98dd\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"76.468966\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-22.042263\"/>\r\n   </g>\r\n   <g id=\"text_2\">\r\n    <!-- 1 -->\r\n    <defs>\r\n     <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n    </defs>\r\n    <g transform=\"translate(101.513534 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-49\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_3\">\r\n   <g clip-path=\"url(#p8902ed0a7e)\">\r\n    <image height=\"58\" id=\"image380ab16813\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"145.737931\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-22.042263\"/>\r\n   </g>\r\n   <g id=\"text_3\">\r\n    <!-- 0 -->\r\n    <defs>\r\n     <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n    </defs>\r\n    <g transform=\"translate(170.7825 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-48\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_4\">\r\n   <g clip-path=\"url(#p1d7d12bdc3)\">\r\n    <image height=\"58\" id=\"imagead2edb90e4\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"215.006897\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-22.042263\"/>\r\n   </g>\r\n   <g id=\"text_4\">\r\n    <!-- 7 -->\r\n    <defs>\r\n     <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n    </defs>\r\n    <g transform=\"translate(240.051466 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-55\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_5\">\r\n   <g clip-path=\"url(#p5869e6e1c8)\">\r\n    <image height=\"58\" id=\"imagedbda158d1a\" transform=\"scale(1 -1)translate(0 -58)\" width=\"58\" x=\"284.275862\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-22.042263\"/>\r\n   </g>\r\n   <g id=\"text_5\">\r\n    <!-- 8 -->\r\n    <defs>\r\n     <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n    </defs>\r\n    <g transform=\"translate(309.320431 16.318125)scale(0.12 -0.12)\">\r\n     <use xlink:href=\"#DejaVuSans-56\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p7ec8c225b3\">\r\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"7.2\" y=\"22.318125\"/>\r\n  </clipPath>\r\n  <clipPath id=\"p98dbc9bdb3\">\r\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"76.468966\" y=\"22.318125\"/>\r\n  </clipPath>\r\n  <clipPath id=\"p8902ed0a7e\">\r\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"145.737931\" y=\"22.318125\"/>\r\n  </clipPath>\r\n  <clipPath id=\"p1d7d12bdc3\">\r\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"215.006897\" y=\"22.318125\"/>\r\n  </clipPath>\r\n  <clipPath id=\"p5869e6e1c8\">\r\n   <rect height=\"57.724138\" width=\"57.724138\" x=\"284.275862\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAABYCAYAAABWMiSwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO2dWXBc132nv4vuRm/oHY1GY98JgKu4SJRp0ZSoiu2RK47HdkWesZNK5SkpP2Rq5mEeMg/JzMs8pCZTE8+4UpnVLrvKVtmKY7soKopMyqYkCgJBmcSOxo5e0QuARu995wE5RwBJUaQEoBvg/apYkgAIPPf0vb97/ruiqioaGhoaGvtDTaUXoKGhofEkoYmuhoaGxj6iia6GhobGPqKJroaGhsY+oomuhoaGxj6iia6GhobGPqKJroaGhsY+UnWiqyhKr6IoWUVRvl/ptVQaRVG+rSjKkKIoOUVR/k+l11MtKIriVhTlp4qipBVFmVcU5V9Vek2VRFGUjXv+lBRF+W+VXlelURSlQ1GUXyqKklAUJaQoyt8oiqKv9LqqTnSB7wDvVXoRVcIK8J+A/1XphVQZ3wHygA/418D/UBTlaGWXVDlUVa0Tf9jakwzw4wovqxr470AE8AOngM8Bf1rRFVFloqsoystAEnij0mupBlRV/Ymqqq8Cq5VeS7WgKIoV+CrwH1RV3VBV9dfAz4BvVXZlVcPX2BKatyq9kCqgE/iRqqpZVVVDwBWg4i/nqhFdRVHswF8C/7bSa9GoavqAkqqqk9u+dpsqeJiqhD8E/p+q1fcD/FfgZUVRLIqiNANfZEt4K0rViC7wH4H/qarqYqUXolHV1AGpe76WAmwVWEtVoShKG1sm9P+t9FqqhGtsvYzXgCVgCHi1oiuiSkRXUZRTwIvAf6n0WjSqng3Afs/X7MB6BdZSbfwB8GtVVWcrvZBKoyhKDfAa8BPACtQDLuA/V3JdUCWiC1wCOoAFRVFCwL8DvqooynAlF6VRlUwCekVRerd97SRwt0LrqSb+AO2UK3ADrcDfqKqaU1V1FfjfwL+o7LKqR3T/FuhmK8J4Cvgu8Avg85VcVKVRFEWvKIoJ0AE6RVFM1ZDyUklUVU2zdXr5S0VRrIqiXAC+DHyvsiurLIqifAZoRstaAEBV1RgwC/zJPz9HTrb83bcru7IqEV1VVTdVVQ2JP2yZkFlVVaOVXluF+XO20n/+PfDNf/73P6/oiqqDPwXMbEXpfwj8iaqqT/pJ9w+Bn6iqqrlZPuRfAl8AosA0UAT+TUVXBChakFNDQ0Nj/6iKk66GhobGk4ImuhoaGhr7iCa6GhoaGvuIJroaGhoa+4gmuhoaGhr7yMflfD4pqQ3KY/ysticPRtuX+9H25H6e+D3RTroaGhoa+4gmuhoaGhr7iCa6GhoaGvuIJroaGhoa+8gT3TxF43Cgqirlcpl8Pk+5XKZUKqGqKnq9npqaGgwGA3q9dqtrVAfanahxoCmVSsTjcVKpFFeuXGFpaYm5uTnS6TRHjx7F7/dz6dIlBgcH0el01NRoxp1GZdFEV+NAUy6XSafTxONxPvjgA8bHx7l79y5ra2usr6/T2dnJwMAAXV1dGI1GamtrK71kjSpDWEalUolyuQyAoijo9fo9sZA00dU40BSLRRYXF1lcXGR8fJyJiQk2NzcplUqMj4+zvLyM2WwmEAhw/vx5jh8/Tk1NDYryuGnIGoeNfD5PoVDgt7/9LcFgkPfee49AIIDZbMZoNPKFL3yB3/md38FgMGAwGHbt7z0woitaUJbLZVRVRVGUJ/7hEXsheNL2Q1VVisUiq6urhMNhwuEw0eiHLZgjkQjRaJQ7d+6Qy+Vob2+nv79/z04wGtWDqqrc27Z2+3+rqko+nyebzTI/P8/k5CSvvfYaw8PD2O12LBYL7e3tPP/88zIusFsciDuvVCoRCASIx+O88cYbzM7O8uyzz9Lb20tXVxfNzc2VXuK+UiwWyeVyvPXWW6ysrFAsFgG4ePEiR44ceSKEN5fLsbCwQDAY5Gc/+xmzs7M7BFegqipTU1NEo1EaGhpQFIXe3l66u7srsGqNvSKfz7O+vk6xWKRQKJBOp5mZmSGXy1EqlSiVSqRSKfL5PGtra5RKJRwOBzU1Nbz11lsEAgEWF7dm4mazWQqFApubmxQKhV0VXDggolsulwmFQszPz/MP//APDA0NUSgUKJfLuFyuJ050S6US2WyWDz74gNHRUfL5PAA9PT309fUBHHrhLRaLBINB5ubmGB4eJhAIkM1mH/iz4XCYSCTC2NgY9fX1OJ3OAye6Dxo2cNg/40dFVVUKhQJra2sUCgUymQzxeJyRkRE2NzelGyEcDrO5uUk4HCafz9Pe3o7VauXmzZvMz8/L3yeep3w+v8PPu1scCNEtlUpMT08zOjpKMpkEtjZkc3NTnvKeFDKZDNeuXWNhYYHr168TCAQol8vU1NQwMzNDX18fTqeTurq6Si91TxABj2QyKa8/FovJB+Rh/9/ExATZbBafz8f58+f3cdWfjjt37vDWW2+RSqWIRCK43W6OHTuGz+fjzJkzT1RwUKQHZrNZ0uk0CwsLDA0NkUwmWV5eJp/Pk8lkyGQyhEIheTgrFotkMhmKxSIbGxuoqsr6+jo6nY5EIrGv13AgRLdcLrO0tMTk5CQbGxsAFAoFstnsEye6+XyeoaEh7ty5w/DwMMFgEACDwcDy8jKxWAyj0XioRbdQKLC+vs7w8DAzMzMkk0l5OnkY8/PzhEIhLl++vA8r3T1mZ2d55ZVXWFlZYWJigs7OTn73d3+XgYEBTpw48USJ7nYBTSQSjI2N8ZOf/IRoNMrc3Jx0CwjL4EHWgPheOByuiLVQ1aKrqirpdJpUKsXc3BwzMzPo9Xqampro7e3l+PHj1NfXV3qZ+0K5XCaZTBKNRhkfH+fOnTvyBWQwGDAajZjNZiwWy6EOEiUSCd544w15P4TDYQqFgvy+oig4nU5qa2tJpVI7XA4iuHJQ5gJmMhnS6TSRSIRgMCitvGQyyXvvvUc8Hsfv9+NyuWhra5O+R51Oh81mk/dBuVwmEomwublJOp0mn89jsVgwGo3U19fjcrlQFKWq3RXZbJZMJsP4+Di3bt0iHo8TDodZXl5mZmaGzc1NstkstbW1tLS0oNfrsdvtmEwmmpqaMBqNGAwGcrkc165dIxwOP/DvURRFphaaTCZZYLObVPXTKUQ3kUiwsLDAzMwMbW1tuN1ujhw5wvHjx3E6nZVe5r4gTOpwOMzY2Bh37344/Fav11NbWytFd7cd/9VEPB7n5z//ObOzs8zMzJBOp4EPTzQ1NTW43W7q6urIZrP3+XnvzfioVlRVJZPJsLq6SjQaJRQKyWtJJpMMDQ0RDofxeDw0NTUBYDabURRFVuAZjUbgw7S6WCwmxdftdmO326mpqcFut6PT6apadMXJ9ubNm3z/+98nGo2ysLBw388JkbVarfj9fpxOJ2fPnsVms2G1WllbW2NsbOwjRVf8DqvVitlsfvJEt1AoMDk5yfz8PMlkElVVMZvNOBwO7HY7NpvtUAvMdgqFAvPz88zNzZHJZOTXFUXBbDbL/TiseyL8d+J+CIVCO1xLiqJgtVqxWq1cvnyZtrY23njjDQKBAKurq9KPVy6XyWQypFIpjEYjJpOpglf1YMSJdGhoiOvXr/Pb3/6WbDYrfdbiOpLJJMPDw0xPTzM/P7/jpOtwOHacdEOhEBsbG/Kka7Vaqa2tZWxsjLa2NgYGBhgYGECn01WVpSSqDd9//32Gh4cZHR0lHA6TyWTkQcPtduN2u+nu7sblcnHkyBHMZjM2mw2z2UxLSws1NTXyhSPuG4PBsENQC4UCqqridDrx+/243W6MRuOu70f17O4DKBQK3L59m4mJCWKxGOVyGYvFgtvtxul04nA4Kr3EfSOfzzM5Ocn09DSbm5s7vldXV4fT6TzUe7K+vs7777/P1NQUk5OTJBKJHX5cnU6H3W7H6/Xyla98hbNnz5LP56mpqZHBE9GXIZ1OE4vFcDqdVSm66+vrRCIR/umf/om//uu/fmCAUOQnX79+HXi0TIZ7T/iKotDd3U1zczMvv/wy7e3t0qSuFsLhMLOzs7z66qv84Ac/kNdQW1sr3SP9/f309vbypS99Ca/XK3Oxt+/JxsYGwWBQ3gfAffna5XKZQqGA1+ulr68Pn8+3J/fHvuzu5uYmyWSSWCzGzMwMTqeT/v5+eWr9qBumXC6zvr7O6uqqfDvZ7Xbq6+ur8mHZSwqFAtPT04yNje0wqQ0GAx0dHXR1deHxeCq8yt0nnU4TCoWYm5vjxo0bLC8vk8lkZOmmoijU1dVhtVp57rnnaG9vx+/3y4fSZDKh0+mAD0+IqVSKYDBITU1NVe1ZPp8nn88zNjbG0NAQExMTOwRXfN7i34VICD+1eI4eJK4CYS5vz11VFIWJiQnee+89Ojo6ZNphJRE+2tu3b/POO+8wMzODqqrSmmlvb+fYsWN4PB7a29vx+Xw0Nzdjs9ke6CrR6XQ4nU58Ph+9vb0YDAbq6urQ6/UsLS2RTCZlcVFXVxef/exnaWtr25Nr2xfRTSaTTE5OMjw8zI9+9CP6+/v5oz/6IxoaGuSFP4hyuSz9WblcTvrrWlpasFqt+7H0qiGbzfLee+9x+/btHaKr0+l46qmnePrppw9lvrLwX969e5dXXnmFtbU18vm8FBa9Xo/L5aKxsZFvfOMbHD9+HJ/Ph16vx2KxYLVapegKoYlGo0xOTmI0Guns7Kzk5e0gnU6zvr7O9evX+d73vicDZwK9Xo/JZJKCUiqVdgQRH4Xa2loMBoN8cUWjUaLRKDdu3CCfz3P58uWqEN1kMkk8Hufq1av88Ic/lC8fu91OW1sbL774In/8x3+MzWbbEQj8qAOcCMCbTCaeeeYZ2tvb8Xg86PV6XnvtNTY3N8nlcuh0Os6cOcM3vvGNPcsK2RfRFSW7hUKB1dVVIpEI4XAYg8HwwMRjUaKXTqdlIEFUhvj9fjo7O7HZbPux9KqgWCySz+fJ5XI7fHvCp9XY2Hjo9qRUKpHP54nFYoyOjjI9PS2ri2ArYFZbW0tdXR3Hjx+ntbWVxsZG7HY7BoPhocGyjY0NQqEQ7e3t+3U5j0QsFpNVdmtra+RyOQCcTidNTU24XC5aW1vR6/XodDqy2SzhcPih+cnbEZkdRqORoaEhAoGA/F6hUCCXy1VFCqaqqtI6Frm15XIZRVFoa2vj0qVLHDt2TGYniJfqw6ipqaGurg5FUTh27BjNzc07Aq25XA5VVaVPu7a29pF+7ydhX0RXPCD5fJ7l5WWMRiOTk5OUy2Weeuqp+36+XC6ztrZGLBYjEAgwPT0tTy5Hjx7lM5/5DC6Xaz+WXnFE4Gdzc5NMJiNvEhFAczgcDA4O8swzz1R19PlxyefzJJNJAoEAV65cIRKJyNMZfJgW1djYyO/93u/R29tLb28vTqcTRVEemrcbjUYZHR2lo6Njn67m41FVlenpad5++23GxsaIxWLyxdHe3s4LL7zAwMAAly9flulPqVSKu3fvPlKOMmw9h83NzVgsFv7iL/5ih+iKKsdqEF3YCqAtLi6yvr6+46Vy+vRp/uzP/gyLxYLNZnvke16n0+H1evF6vbS2tpLP53nnnXdYWFigVCqxtrYmrYDdbnBzL/siujqdTkYBhV/tYak7wuwRZXulUgmbzSZ9d4c9F3U7+Xye2dlZZmdnd6Q/iWi93W7HaDQemj6xwmQOhUJ88MEH3L17l3g8viPhHbZO+e3t7bS0tNDc3ExDQwNGo/GRHsJsNntfDm8lSafTZLNZlpaWmJqaYnV1dce1OhwOent7aW1txeFwyJQwRVFoaWl5ZKEU/u+amhrp9xR/TzqdJhqNSpGrZPOke0+62xGFEQaD4bHXJ35er9dTKpVkkE6467xeLx6PZ8/TUPdFuWpra3E4HFgslkf6+Vwux+joKIFAgLW1NVRVxePx4PP5ZO38YTrVPYy1tTWuXLkiK68EOp2O+vp6mpubMZvNFVzh7pLL5Ugmk9y8eZPvfOc7Mh+zWCzuEKK6ujqef/55uru7OXXqFA0NDY9sDqZSKZaWlkilUjsCUJVAVVVCoRDBYJAbN25w5cqV+14Gra2tfP7zn8dms8l7X1EUTCbTYwmEqqpEo1FSqdR93wuHw2xsbPD0009TKBQq3oltdXWV+fl51tfXd3x9bW2N2dlZmpubcbvdn/izKxQKvPvuuzI4q9PpOHnyJCdOnNhzC2hfdlW04HvUxhEiMVycchVFwWazyby5J0VwYevmWF5eZnFxUfr4YMtU9Pl8tLa2PvLL7CCQSCSYmJggEAgQiURIpVI7BFen02EymXA4HPKUazabH8v/VldXR0NDQ9UEY0VAtFgsSp+9uOcdDgderxeLxYLJZNph0YhG24+KyAYSrprtGI1G7Ha7LLCo9DNWV1eHx+PBbDbveNmGw2Fu3brF8vIywWAQk8kkS95FZoZer8dgMMicdbPZLPdN5DcnEglisRiJRIJyuYzBYKCpqYkjR47gdrv39Nr2RXQLhQIbGxv3fdAfhUjrSSaTFItFdDod7e3t9Pb2Yrfb93i11UU6nebtt99mcnJSmkGwdYOdO3eOs2fP4vf7K7jC3WV0dJS/+7u/Y2Fhgfn5+ftOuAaDgZaWFrq7u7lw4QLt7e2P3Weis7OTS5cu0dPTU3FxURQFi8WC0+lEr9eTy+WkaPT29nLu3DlOnz69K0Uv5XKZ8fFxWWCwncbGRgYHB2lpaaG2trai+6IoCn19fTgcDoaHh3d875133mFkZESKa2NjI0ePHsVkMmGxWKitrZU566dPn8bpdNLb2yur8wqFAiMjI8zPzzM6Osr8/DxWqxW3281nPvMZvv71r+95L4t9Ed1cLkc8HpdVQQ+LLIu3vchyECkxbrcbv99/qEzpR0FVVXK53H2Re4PBgNvtpqGh4VDkLIuG0qurqywvL7O6uioHTQLy9GW1Wunp6aG7uxun04nFYnkkf/b2e06v18uKrGpAZKG0tLRw7NgxGUEfHBzkyJEjNDY27tp8t1wuJ/vEbken01FbW3tfUUEl2P4iamlpobe3l/X1dekOymQyMkcZkBkrJpMJo9EoLQSj0YjD4WBzc1M+I7lcjvHxcZaWltjY2KBYLMreJft17fsiuuFwmJs3bzIzM/NQF4PoLxAMBrl27RpjY2PE43GMRiOnTp3iueeeo6GhYT+WXLWIrAWbzUZvby8nTpw4FKIbiUQIBALcunWLkZER2ZJPoCiKbGby7W9/m7a2Npqbmz/W3fSgl7wwOaulXNrhcGCz2fjWt77F5cuXqa2tpba2FpfLhcfjkQ1YPq0gbE/FvHdvqw2Xy4XNZuOrX/0qx48f5ze/+Q2/+c1vZGrb+vo6iUSCeDzO+++/vyPwV1NTQ01NDWazWZ58hftJuBey2SzJZFK6cbxeLxsbG8zNzeHz+fbUxbDnoisikcFgkEQiIfuhplIpEokEoVAIs9ks31zRaJSlpSUSiQQbGxvU1NRgMplkEKFaTid7jWhhmMvldmR6iHJX0dTlXj/fQUOIYjKZZHZ2VtbV3ysKZrOZhoYGWlpa8Pv9eL3eh0awxX12r3sCtvbQYDDsWR7m4yJOsR6PR6Yr1dbWYrFYdqVFp+gdu7GxIf2YIj6wXaCqqSxa+Gfr6+tlNks0GpX56iKlVExREVlRgOwvXS6XyeVy0pUisqZE6qFItSuVSuRyOYLBoGwfm0qlZPWb+Dx27dp27Tc9AFHrPjc3x+uvv04ymaRUKrG6usqNGzeYmppiaWkJnU4n02YWFhZkK8dCoYDf76ehoUGmBT0pqWK5XI6lpSWWlpZkcEVVVQwGA+fPn6enpwefz1c1wvFJES+Wd999l+9+97tEo9H7XAEGg4Guri6++c1v0tHRQVtbmyz3fBAi7WxjY0NOE4APBcZqteLxeKoqACkKF0TuqSgo2g0KhQJvvvkm09PT/PznP2dsbIy1tTXgwwq1I0eO8OKLL1bNRA3xWbW1teH3++nv7+f3f//3peiK9gChUIixsTHZX1uIrzj5bmxs8O6777K+vi7T4cSzJAiFQkQiEf72b/8Wk8kkX+qf/exnuXDhAk1NTbua0bCnCia6uK+trRGPx2UgrVAoyKihxWJBURQpuouLizuKAJxOp4zefpLcvIOKOPXHYrEdprYIHjQ3N1fNqeTTIF7Ma2trLC8vy2Y+23Mq6+rqcLlcdHZ20tLS8tAqJGFZiZOd8NsJEROFOjabTQZXqgWdTrerL1HRGnVjY0O2Rg2FQvLwAx+2MXS73fh8vqoLVAtXi8g0KRQK5PN5NjY2pAtCfC2TyUgxFVZOIpFAp9NJsS0Wi7KiT7iYxNQRsS/ZbJZEIkFTU5N0YYlOZbvxItxT0V1aWuLu3buMjo7KoXGw1RtzYWEBvV4vq2LE20eYCvl8HpvNxqVLlxgYGKCxsfGJEVzYylP88Y9/zOzsLMlkUr69LRYLzz77LGfPnsXr9VZ6mZ8acSoVfZPvLWl1OBycOnWK48ePc/78eVwu10PFslgscvPmTWZnZ7l58yajo6PyBS4CVG1tbZw+fbrqRHe3yWazvP7668zOzvLTn/6U6elpUqmUbJID0NfXx+DgIM8++yzHjh2revedEEyR+9/U1ER/f/8O9wJsZf1MTEywsLDA1atXpZtJr9fT3NyM0+nk4sWL+P1+JicniUajcrhpJpMhmUzyi1/8gnfeeYevfOUr+Hw+2c3vU1/Dp/4ND2Fzc5NIJCKnb4qo5HbTKZ/PS3NK+DHFg6coivTjPUlZC8LvtLS0xPLysvS/CVPb5XLh9Xqr/gF5GGLg3/r6OvF4/L5GNgKj0YjX65UTDj7OxymaJC0vLxOPx+9Lrheduh416+EgIp6jdDrN4uIi09PTLC0t7UgTE+Ll9Xrp6OiQzaeqne2uF5GxsP10LuanCcQJt1wuy4IPcT/19PTIij6r1Uo2m6VcLhMOh8lms0QiEZLJJJFIhGw2u2sv6T0V3WQyyfT0NLFYDL1ej9/v5+TJk3Kj9Hq9POJbLBZisRg/+MEPiMViwJZpIcbyVJvZs1cUCgWSySShUIipqSmWl5elSeTxeGhsbMTj8exoUn0QCYfDRKNRrly5wmuvvcbS0tIDUwkdDgcnT56ku7v7ka5XDDG9devWfQMHxXwt0WLU7XYfCmvhXrLZLCMjIywvL/PLX/5SnuQEiqLQ3NyM1+vl85//PF/84hf3vCBgvxB9iBcXF7ly5QqxWIyVlRVKpRJtbW14PB6++c1v0t3dTU9PD3a7naeeeopcLsfU1BTBYJAf/ehH/OM//qN0O4g2k7tVTLOnT62oRKutrZU+o+7ubpmDt72npdlsZmVlRZ7eRMTQ4XDgcrmqJr1nrykWi6ytrcniEOH8NxgM2O122VnpoO/H2toaoVCIiYkJ3n///fuatogqLZG14HK5PvZkKlwV8XicSCSyo4IPPsyUED7BaqlI2w2EeZ3P51lfX2d5eZn5+XmWlpYIBoNyf8W+is5lra2tdHZ2HvhTv8hAiMfjTE9PMzc3x+joqGwjYDQa8Xg8tLS00NfXR19fn8xx93q9O3ozb2+mtd1lsVtjnvZUdI8dO4bX6yWdTpNMJuVoDZGyIxpvpNNppqam5AUajUb6+/tpbW2lvr5+RxnfYWd1dZWrV68yNTW1w+S2WCxcvHiRnp6eqmq8/UlQVZWbN2/yy1/+krGxsR0BEIHRaMTpdNLZ2Sm7yj3spFssFpmbmyMSiTA+Ps709PQDXQsi39dqtR4an26xWJSlrb/61a9YWVnh2rVrRCIRFhcX5T2kKIpMNXzppZe4cOEC/f39FW1u82kRL5vJyUl++tOfsri4yK9//WvW1tZk57DBwUH8fj9f+9rXaG1tpa+vD7vdfp97LhKJMDs7K3tTOBwOnE4nra2tuxq43lPR9Xg8jyQQiUSCqakp6XsRviafz/fYdfUHnUwmw+zsLPPz8zsqsgwGA21tbXR1dR2KrIVIJMLExASRSOSBXbJqa2ux2+24XC78fv9DT6Uix3t1dZVgMEg8Hn9gUE4EYMRUicNwXwlrcn19nWg0yp07d5ifn2dkZES6V8TpVvSUdbvd9PT0cPLkyYdObqlmRM6tON1HIhGGhoZYWVlhamqKfD4vK81EytepU6dkr5J7P3uR9RKPx6VPWDQUEsUru3Xwqwqn4MbGBsPDwwQCAbLZLAaDgebmZjmz6Ulic3NT5i9vFyNRlScelINOW1sbTz/9tJxqey9ut5uzZ8/S19f3UHEUHekikQhXrlxhfn5e9ki9lxMnTjAwMMCZM2doaGg48C6aXC7H6uoq4XCYq1evsrKywq9+9SsSicSOOXp6vZ7u7m48Hg+XL1+mt7eXM2fOHEi3nWjkHw6HpWUzNTVFKBSSneMURaG+vp5nnnmGpqYmXnrpJZlm+SCrWQwJuHv3Lm+++aacMtzf389zzz3H4ODgrqWLQZWIbj6fl4UAohZaHO0PcrDokyCmJayuru7I4hCNXlpbWyu8wt1BTEGYmpp64PetVqt0L8FO35pAVVXZh3ZxcZGRkRHm5uZk4r9ARLubmprklInD4M8V/utgMCgDZ4FA4L5xTjqdDp/PR0tLC2fPnuXkyZO43e4DlREk3AiiMCIcDjMxMcHs7CxDQ0OyHkAc2mw2G/39/bS3t3P27Fk8Hs9HllKL6rRQKCTT6mCrv+7Ro0dpbGzcVfdmVShaJpNhZmaGxcVFCoWCHDzX3d19oG6MT8P2Zi/hcFiaxyaTic7OzkO1F6IXrMPh+EhLRpjNsViMt99+m1KpRCQSkcGOYrHI8vKyHEMuAijCDy7Q6XS0tbXR0NDAxYsXuXz5Mo2Njft1qXtCsVgknU4zNzfHK6+8wvLyMrdu3SKVSu24dpPJxMWLF2lqauL555+ntbWVrq4u3G73gbEghcvx9u3bfPDBBywsLDA5OSkr0tLpNJFIRAZc7XY73d3dNDY2cvnyZTk5/KOa2aiqyurqqnx5CatL+HKPHj266xkuVSG6xWJRDsgrlUro9Xrq6+vx+XwHOhf1ccjn88TjceLxOKlUSo6KNhgMtAz/FhoAAAjRSURBVLa2ypZ7hwXRdOZhQ0lFpdrU1BTpdJpAICB7FGSzWUZHR0kkEty5c4d0Or0j6R8+7Gkgsmb6+/s5fvz4gfRhbkeIbigU4saNG9LUvjcDxGg0cvToUQYHB7l06dKBtJJEFVkgEOD69evcvXuXoaEh4MPAKGydSm02G83NzbJ098yZMx97UBF9KeLxOMlkklQqhcPhoK6uTo722e2Aa0VFN5/PS5+UGDzZ2dlJc3MzHR0dsuTzSSAUCvH3f//3TE1N7ZgFZrVauXDhgswpfFIIh8Ncu3YNo9GI1WqVEXpANkiKRqOyVeH2enpRMi4qGS9evEh/fz/9/f0HXnBha37YjRs3GB8fZ25uTpavbi/8OHfuHH6/n8997nN0dHTs+Qia3WZ1dZWNjQ1u3brF5OQkIyMjjIyMEI/HAfD7/Rw5cgS/38/g4KCsTrPZbLS1tclGNR+FKJHe3NzkypUr3Lp1i4mJCQCOHDki22ruRSC/4qIbjUaJx+MUCgV0Oh1NTU20t7fT1NT0RLVxjMViXL9+nWAwKBt3wJbAnDx5kt7e3qpq0LLXJBKJ+4obtnNvipkQU5ESVldXx7Fjx+jv7+fFF1/k2LFje7re/SSVSnH79m2mp6cJBoOyp4mo0rLb7Zw7d47u7m6eeuop/H7/gcrUUFWVVCpFJBLhxo0bvPnmm3JShMDr9fLUU09x/PhxvvSlL2E2mx+7ok6U+964cYOrV6/KfWxvb+f8+fO0t7fviXVZUdEVeYUzMzNsbGywubnJ/Pw8hUKBW7dusb6+Tk9Pz4EoT/ykCDNa3ACi0buiKPKUJ1LvDlNQ0ev10tvbS0tLC16vl0wmw8bGxif6XSaTCZPJRFdXF06nk8HBQerr6zlx4oTsUncYCIfDjI2NMT4+zo0bN4jFYnKIpGjic+bMGZqamjh//rwcZHkQ83BXVlaYnJxkdnaW5eXl++4Ns9mM1+vFarXKhlDbJ6t8FKVSiVgsJjOmxN+Ty+VkC9ne3l7Onj1LU1PTnlxbxUX32rVrLC0tychjLpcjk8kwPDxMOp2WjSYOKyLPUIju2tqazFUWHaA8Hg9ut/vQia7ZbKatrQ2v1yvNycdF9IK12+2cOnVKjitva2ujvr7+UFkHkUiEa9euMTExwc2bN8nlcrJE3Gg04nK5uHDhAl1dXTz77LMHtohGVVWCwSBjY2PMzc0RCoUAdkwvNplM8vMV5brpdPpjq8YKhQLj4+NEo1FeffVVAoEAiUSCbDYrKz57eno4ffr0nl1fVTzF4i1sMBhkK8eenh66uroOTcT+o8jn8yQSCXnKFV3W9Ho9Pp9PjhYXI7MPC8IFcPr0aYrFIktLSwQCATkWW/TZvReTyUR7eztms5nGxkasVit+v5+6ujpOnDiBx+OhublZjnA5DGwv/piYmGBxcZFisSj92Eajka6uLtra2jhx4gStra0H+rkRvSGOHj3KzMwMgUBgR7tXgMXFRV5//XVcLhcjIyMyuPhxw29FQ5t0Os3Kygqbm5t4PB5pHfT29jIwMLCn11dx0d2e/6bT6eRDMzAwQF9f36HIp3wYuVyOWCwmO2IJE0lU0jQ1NWE0Gg9dGbRwCTz33HOcPXuWiYkJRkZGGBoakqlADxJdIa4+n49z587hcrno7+/Hbrcf2skiYm6gmISbTCZ3VCtaLBaOHTtGd3c3zzzzDD6fr8Ir/nQoikJHRwd2u122hl1dXd0huoFAgNnZWYxGIxaLRZ50txfFPGyqCHzYaa27u5umpia+/vWv87nPfW7Pg/cVFd1isSjTo3Q6HTabjeeee47Ozk68Xu+BH0XzKKTTaRYWFohEIjtuGLPZTG9vL11dXYdSSAR6vR6TyYTP52NwcFC6U8SgynvNRavVysDAAHa7XU4CFvm+BylY9Disra2xsLDA0tIS6+vrMuAjWht6PB6OHz9OR0fHocn2EYetp59+GoPBIKc7pNNp1tbW5JgekbEhDizb7xdhMdbV1e0YyS4GcDY2NlJXV8fAwAA+n4/29vZ9KQ+vePZCJBJhdXUVnU5HQ0MDL7/8shxFc1gakjyMZDLJ3bt3mZub22Ea1dXVce7cOTo6Og60qfhxiJlgXV1ddHZ27hgk+SD/3PbczHv/eViJRqMMDQ0xOjpKLBaT+bh6vR63201HRwcvvPACTU1Nh8YydDgcOBwOvvzlL/PSSy8xPz/P/Pw8KysrzM3NydzaQqFAsVgkGAyysrJy3+8R/XPFFGmr1SqnSJ87d47GxkbZRGr7vbWXVFR0RdURbJnTYsptXV3doT213IvNZqO7u5tUKoXZbJYfutVqlT7dwxRAexj7ddMfNPR6PRaLRR5CxMvIarVy5MgRenp6cDgch7IxuxhQ6XQ6KRaLWCwWHA6HzHYRPZKTySSdnZ33NU8SbWVNJhONjY0yI8hoNNLW1obT6dz3LoYVF12R7mG1WnE4HDJS/6Tg8/m4dOkSsFV6mE6nZQu+I0eO0NzcfKjdCxofj3Ah2Gy2HV93u91cvnyZzs5OaSofNkSjmcbGRnw+30daQveO69mOeJnfaxmJVLr9ftFXVHRdLheXLl0inU5jNpsPlXn0qIgG7i0tLbzwwgvST9Xe3i59UNrp78lGNNr2eDw4nU4ymQy5XE72G6ivrz/0luFhsoIqKrp9fX381V/9lSwGEDmXTxLC0f/0009z4sQJ+fYWebqHzVzUeHxcLhdWq5VgMEhPTw/xeJxQKITb7aa/vx+/339o0uOeBCoqujqd7lCaRI+LoihSfDU07kWU9zY0NHDmzBnW19eJxWL09PTgcrnksFeNg4HyMRUcuzMUqPp5nDtW25MHo+3L/ezqnuTzeTY3N6X/Uq/XY7Vad7XB9idEe37u5yP3RBPdLbSb5n400X0w2r1yP9qe3M9H7onmMNTQ0NDYRzTR1dDQ0NhHPs69oKGhoaGxi2gnXQ0NDY19RBNdDQ0NjX1EE10NDQ2NfUQTXQ0NDY19RBNdDQ0NjX1EE10NDQ2NfeT/A8sHUnizppHfAAAAAElFTkSuQmCC\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "mnist_train = mnist_train.repeat(5).batch(32).prefetch(1)\n",
    "for item in mnist_train:\n",
    "    images = item[\"image\"]\n",
    "    labels = item[\"label\"]\n",
    "    for index in range(5):\n",
    "        plt.subplot(1, 5, index + 1)\n",
    "        image = images[index, ..., 0]\n",
    "        label = labels[index].numpy()\n",
    "        plt.imshow(image, cmap=\"binary\")\n",
    "        plt.title(label)\n",
    "        plt.axis(\"off\")\n",
    "    break # just showing part of the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- note that each item in the dataset is a dictionary containing both the features and the labels\n",
    "- Keras, however, expects each item to be a tuple containing 2 elements (the features and labels)\n",
    "- to handle this, you can transform the dataset using the `map()` method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(32, 28, 28, 1)\n[4 1 0 7 8 1 2 7 1 6 6 4 7 7 3 3 7 9 9 1 0 6 6 9 9 4 8 9 4 7 3 3]\n"
    }
   ],
   "source": [
    "datasets = tfds.load(name=\"mnist\")\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
    "mnist_train = mnist_train.repeat(5).batch(32)\n",
    "mnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"])) # transforming the data\n",
    "mnist_train = mnist_train.prefetch(1)\n",
    "for images, labels in mnist_train.take(1):\n",
    "    print(images.shape)\n",
    "    print(labels.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- however, it is simpler to ask the `load()` function to do this for you by setting `as_supervised=True`\n",
    "- this way, you can then pass the dataset directly to your `tf.keras` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 1875 steps\nEpoch 1/5\n1875/1875 [==============================] - 3s 2ms/step - loss: 31.9638 - accuracy: 0.8409\nEpoch 2/5\n1875/1875 [==============================] - 2s 859us/step - loss: 25.4987 - accuracy: 0.8701\nEpoch 3/5\n1875/1875 [==============================] - 2s 925us/step - loss: 25.0071 - accuracy: 0.8723\nEpoch 4/5\n1875/1875 [==============================] - 2s 888us/step - loss: 24.3472 - accuracy: 0.8758\nEpoch 5/5\n1875/1875 [==============================] - 2s 905us/step - loss: 23.7253 - accuracy: 0.8783\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2ce6507b588>"
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "datasets = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True) # transforming data\n",
    "mnist_train, mnist_test = datasets[\"train\"], datasets[\"test\"]\n",
    "mnist_train = mnist_train.repeat().prefetch(1) # shuffling data\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Lambda(lambda images: tf.cast(images, tf.float32)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(mnist_train, steps_per_epoch=60000 // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this was quite a technical chapter, and you may feel that it is a bit far from the abstract beauty of neural networks, but Deep Learning often involves large amounts of data, and knowing how to load, parse, and preprocess it efficiently is a crucial skill to have\n",
    "- in the next chapter, we will look at convolutional neural networks, which are among the most successful neural network architectures for image processing and many other applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "---\n",
    "- 1) *Why would you want to use the Data API?*\n",
    "---\n",
    "- The Data API makes it easy to efficiently preprocess a large dataset.\n",
    "---\n",
    "- 2) *What are the benefits of splitting a large dataset into multiple files?*\n",
    "---\n",
    "- Splitting a large dataset into multiple files makes it possible to shuffle it at a coarse level before shuffling it at a finer level using a shuffling buffer. It also makes it possible to handle huge datasets that do not fit on a single machine.\n",
    "---\n",
    "- 3) *During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?*\n",
    "---\n",
    "- You can use TensorBoard to visualize profiling data: if the GPU is not fully utilized then your input pipeline is likely to be the bottleneck. Just make sure your preprocessing code is optimized.\n",
    "---\n",
    "- 4) *Can you save any binary data to a TFRecord file, or only serialized protocol buffers?*\n",
    "---\n",
    "- You can save any binary data to a TFRecord file, however, in practice, most TFRecord files contain sequences of serialized protocol buffers. \n",
    "---\n",
    "- 5) *Why would you go through the hassle of converting all your data to the `Example` protobuf format? Why not use your own protobuf definition?*\n",
    "---\n",
    "- The `Example` protobuf format has the advantage that TensorFlow provides some operations to parse it. However, if it does not cover your use case, you can define your own protocol buffer (which is more complicated).\n",
    "---\n",
    "- 6) *When using TFRecords, when would you want to activate compression? Why not do it systematically?*\n",
    "---\n",
    "- You generally want to activate compression if the TFRecord files will need to be downloaded by the training script, as compression will make files smaller and thus reduce download time. \n",
    "---\n",
    "- 7) *Data can be preprocessed directly when writing the data files, or within the `tf.data` pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?*\n",
    "---\n",
    "- If you preprocess the data when creating the data files, the training script will run faster. The trained model will expect preprocessed data, so you will have to add preprocessing code in your application before it calls the model.\n",
    "\n",
    "- `tf.data` makes it easy to build highly efficient and customizable preprocessing pipelines. On the downside, preprocessing the data will slow down training, and each training instance will be preprocessed once per epoch rather than just once if the data was preprocessed when creating the data files.\n",
    "\n",
    "- If you add preprocessing layers to your mode, you will only have to write the preprocessing code once for both training and inference. On the downside (like with `tf.data` ), preprocessing the data will slow down training, and each training instance will be preprocessed once per epoch.\n",
    "\n",
    "- **Lastly, using TF Transform for preprocessing gives you many benefits from the previous options: the preprocessed data is materialized, each instance is preprocessed just once (speeding up training), and preprocessing layers get generated automatically so you only need to write the preprocessing code once. This is, overall, the best option, however, TF Transform is a difficult tool to master.**\n",
    "---\n",
    "- 8) *Name a few common techniques you can use to encode categorical features. What about text?*\n",
    "---\n",
    "- To encode categorical features with no natural order, use one-hot-encoding or, if there are many categories, use embeddings.\n",
    "\n",
    "- For text, use bag-of-words representation or encode each word using embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9)\n",
    "_Exercise a): Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a training set, a validation set, and a test set; shuffle the training set; and save each dataset to multiple TFRecord files. Each record should be a serialized `Example` protobuf with two features: the serialized image (use `tf.io.serialize_tensor()` to serialize each image), and the label. Note: for large images, you could use `tf.io.encode_jpeg()` instead. This would save a lot of space, but it would lose a bit of image quality._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train)) # shuffling the training set\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    #image_data = tf.io.encode_jpeg(image[..., np.newaxis])\n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\": Feature(int64_list=Int64List(value=[label])),\n",
    "            }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the following function saves a given dataset to a set of TFRecord files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import ExitStack\n",
    "\n",
    "def write_tfrecords(name, dataset, n_shards=10):\n",
    "    paths = [\"{}.tfrecord-{:05d}-of-{:05d}\".format(name, index, n_shards)\n",
    "             for index in range(n_shards)]\n",
    "    with ExitStack() as stack:\n",
    "        writers = [stack.enter_context(tf.io.TFRecordWriter(path))\n",
    "                   for path in paths]\n",
    "        for index, (image, label) in dataset.enumerate():\n",
    "            shard = index % n_shards\n",
    "            example = create_example(image, label)\n",
    "            writers[shard].write(example.SerializeToString())\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepaths = write_tfrecords(\"my_fashion_mnist.train\", train_set)\n",
    "valid_filepaths = write_tfrecords(\"my_fashion_mnist.valid\", valid_set)\n",
    "test_filepaths = write_tfrecords(\"my_fashion_mnist.test\", test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise b): Then use tf.data to create an efficient dataset for each set. Finally, use a Keras model to train these datasets, including a preprocessing layer to standardize each input feature. Try to make the input pipeline as efficient as possible, using TensorBoard to visualize profiling data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tfrecord):\n",
    "    feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }\n",
    "    example = tf.io.parse_single_example(tfrecord, feature_descriptions)\n",
    "    image = tf.io.parse_tensor(example[\"image\"], out_type=tf.uint8)\n",
    "    #image = tf.io.decode_jpeg(example[\"image\"])\n",
    "    image = tf.reshape(image, shape=[28, 28])\n",
    "    return image, example[\"label\"]\n",
    "\n",
    "def mnist_dataset(filepaths, n_read_threads=5, shuffle_buffer_size=None,\n",
    "                  n_parse_threads=5, batch_size=32, cache=True):\n",
    "    dataset = tf.data.TFRecordDataset(filepaths,\n",
    "                                      num_parallel_reads=n_read_threads)\n",
    "    if cache:\n",
    "        dataset = dataset.cache()\n",
    "    if shuffle_buffer_size:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = mnist_dataset(train_filepaths, shuffle_buffer_size=60000)\n",
    "valid_set = mnist_dataset(train_filepaths)\n",
    "test_set = mnist_dataset(train_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "class Standardization(keras.layers.Layer):\n",
    "    def adapt(self, data_sample):\n",
    "        self.means_ = np.mean(data_sample, axis=0, keepdims=True)\n",
    "        self.stds_ = np.std(data_sample, axis=0, keepdims=True)\n",
    "    def call(self, inputs):\n",
    "        return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\n",
    "\n",
    "standardization = Standardization(input_shape=[28, 28])\n",
    "# or perhaps soon:\n",
    "#standardization = keras.layers.Normalization()\n",
    "\n",
    "sample_image_batches = train_set.take(100).map(lambda image, label: image)\n",
    "sample_images = np.concatenate(list(sample_image_batches.as_numpy_iterator()),\n",
    "                               axis=0).astype(np.float32)\n",
    "standardization.adapt(sample_images)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    standardization,\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\n1719/1719 [==============================] - 8s 5ms/step - loss: 0.2330 - accuracy: 0.9138 - val_loss: 0.2090 - val_accuracy: 0.9249\nEpoch 2/5\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2218 - accuracy: 0.9199 - val_loss: 0.1914 - val_accuracy: 0.9279\nEpoch 3/5\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.2056 - accuracy: 0.9251 - val_loss: 0.1844 - val_accuracy: 0.9322\nEpoch 4/5\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1947 - accuracy: 0.9287 - val_loss: 0.1680 - val_accuracy: 0.9406\nEpoch 5/5\n1719/1719 [==============================] - 5s 3ms/step - loss: 0.1851 - accuracy: 0.9329 - val_loss: 0.1653 - val_accuracy: 0.9404\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2ce6a2eea90>"
     },
     "metadata": {},
     "execution_count": 140
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "logs = os.path.join(os.curdir, \"my_logs\",\n",
    "                    \"run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    "\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10)\n",
    "_Exercise a): Download the [Large Movie Review Dataset](https://homl.info/imdb), which contains 50,000 movies reviews from the [Internet Movie Database](https://imdb.com/). The data is organized in two directories, `train` and `test`, each containing a `pos` subdirectory with 12,500 positive reviews and a `neg` subdirectory with 12,500 negative reviews. Each review is stored in a separate text file. There are other files and folders (including preprocessed bag-of-words), but we will ignore them in this exercise._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n84131840/84125825 [==============================] - 14s 0us/step\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "WindowsPath('C:/Users/stuar/.keras/datasets/aclImdb')"
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://ai.stanford.edu/~amaas/data/sentiment/\"\n",
    "FILENAME = \"aclImdb_v1.tar.gz\"\n",
    "filepath = keras.utils.get_file(FILENAME, DOWNLOAD_ROOT + FILENAME, extract=True)\n",
    "path = Path(filepath).parent / \"aclImdb\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(12500, 12500, 12500, 12500)"
     },
     "metadata": {},
     "execution_count": 142
    }
   ],
   "source": [
    "def review_paths(dirpath):\n",
    "    return [str(path) for path in dirpath.glob(\"*.txt\")]\n",
    "\n",
    "train_pos = review_paths(path / \"train\" / \"pos\")\n",
    "train_neg = review_paths(path / \"train\" / \"neg\")\n",
    "test_valid_pos = review_paths(path / \"test\" / \"pos\")\n",
    "test_valid_neg = review_paths(path / \"test\" / \"neg\")\n",
    "\n",
    "len(train_pos), len(train_neg), len(test_valid_pos), len(test_valid_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise b): Split the test set into a validation set (15,000) and a test set (10,000)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(test_valid_pos)\n",
    "\n",
    "test_pos = test_valid_pos[:5000]\n",
    "test_neg = test_valid_neg[:5000]\n",
    "valid_pos = test_valid_pos[5000:]\n",
    "valid_neg = test_valid_neg[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise c): Use tf.data to create an efficient dataset for each set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_dataset(filepaths_positive, filepaths_negative, n_read_threads=5):\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_negative,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_positive,\n",
    "                                          num_parallel_reads=n_read_threads)\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
    "    return tf.data.Dataset.concatenate(dataset_pos, dataset_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "32.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
    }
   ],
   "source": [
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).repeat(10): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "16.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
    }
   ],
   "source": [
    "%timeit -r1 for X, y in imdb_dataset(train_pos, train_neg).cache().repeat(10): pass # added cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = imdb_dataset(train_pos, train_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
    "valid_set = imdb_dataset(valid_pos, valid_neg).batch(batch_size).prefetch(1)\n",
    "test_set = imdb_dataset(test_pos, test_neg).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise d): Create a binary classification model, using a `TextVectorization` layer to preprocess each review. If the `TextVectorization` layer is not yet available (or if you like a challenge), try to create your own custom preprocessing layer: you can use the functions in the `tf.strings` package, for example `lower()` to make everything lowercase, `regex_replace()` to replace punctuation with spaces, and `split()` to split words on spaces. You should use a lookup table to output word indices, which must be prepared in the `adapt()` method._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 50), dtype=string, numpy=\narray([[b'it', b's', b'a', b'great', b'great', b'movie', b'i', b'loved',\n        b'it', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>'],\n       [b'it', b'was', b'terrible', b'run', b'away', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>', b'<pad>',\n        b'<pad>']], dtype=object)>"
     },
     "metadata": {},
     "execution_count": 150
    }
   ],
   "source": [
    "def preprocess(X_batch, n_words=50): # function to preprocess reviews\n",
    "    shape = tf.shape(X_batch) * tf.constant([1, 0]) + tf.constant([0, n_words])\n",
    "    Z = tf.strings.substr(X_batch, 0, 300)\n",
    "    Z = tf.strings.lower(Z)\n",
    "    Z = tf.strings.regex_replace(Z, b\"<br\\\\s*/?>\", b\" \")\n",
    "    Z = tf.strings.regex_replace(Z, b\"[^a-z]\", b\" \")\n",
    "    Z = tf.strings.split(Z)\n",
    "    return Z.to_tensor(shape=shape, default_value=b\"<pad>\")\n",
    "\n",
    "X_example = tf.constant([\"It's a great, great movie! I loved it.\", \"It was terrible, run away!!!\"])\n",
    "preprocess(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[b'<pad>',\n b'it',\n b'great',\n b's',\n b'a',\n b'movie',\n b'i',\n b'loved',\n b'was',\n b'terrible',\n b'run',\n b'away']"
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_vocabulary(data_sample, max_size=1000):\n",
    "    preprocessed_reviews = preprocess(data_sample).numpy()\n",
    "    counter = Counter()\n",
    "    for words in preprocessed_reviews:\n",
    "        for word in words:\n",
    "            if word != b\"<pad>\":\n",
    "                counter[word] += 1\n",
    "    return [b\"<pad>\"] + [word for word, count in counter.most_common(max_size)]\n",
    "\n",
    "get_vocabulary(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextVectorization(keras.layers.Layer):\n",
    "    def __init__(self, max_vocabulary_size=1000, n_oov_buckets=100, dtype=tf.string, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        self.max_vocabulary_size = max_vocabulary_size\n",
    "        self.n_oov_buckets = n_oov_buckets\n",
    "\n",
    "    def adapt(self, data_sample):\n",
    "        self.vocab = get_vocabulary(data_sample, self.max_vocabulary_size)\n",
    "        words = tf.constant(self.vocab)\n",
    "        word_ids = tf.range(len(self.vocab), dtype=tf.int64)\n",
    "        vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "        self.table = tf.lookup.StaticVocabularyTable(vocab_init, self.n_oov_buckets)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        preprocessed_inputs = preprocess(inputs)\n",
    "        return self.table.lookup(preprocessed_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocabulary_size = 1000\n",
    "n_oov_buckets = 100\n",
    "\n",
    "sample_review_batches = train_set.map(lambda review, label: review)\n",
    "sample_reviews = np.concatenate(list(sample_review_batches.as_numpy_iterator()),\n",
    "                                axis=0)\n",
    "\n",
    "text_vectorization = TextVectorization(max_vocabulary_size, n_oov_buckets,\n",
    "                                       input_shape=[])\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[b'<pad>', b'the', b'a', b'of', b'and', b'i', b'to', b'is', b'this', b'it']"
     },
     "metadata": {},
     "execution_count": 154
    }
   ],
   "source": [
    "text_vectorization.vocab[:10] # first 10 words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\narray([[2., 2., 0., 1.],\n       [3., 0., 2., 0.]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 155
    }
   ],
   "source": [
    "simple_example = tf.constant([[1, 3, 1, 0, 0], [2, 2, 0, 0, 0]])\n",
    "tf.reduce_sum(tf.one_hot(simple_example, 4), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BagOfWords(keras.layers.Layer):\n",
    "    def __init__(self, n_tokens, dtype=tf.int32, **kwargs):\n",
    "        super().__init__(dtype=tf.int32, **kwargs)\n",
    "        self.n_tokens = n_tokens\n",
    "    def call(self, inputs):\n",
    "        one_hot = tf.one_hot(inputs, self.n_tokens)\n",
    "        return tf.reduce_sum(one_hot, axis=1)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = max_vocabulary_size + n_oov_buckets + 1 # add 1 for <pad>\n",
    "bag_of_words = BagOfWords(n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\n782/782 [==============================] - 22s 28ms/step - loss: 0.5435 - accuracy: 0.7146 - val_loss: 0.5181 - val_accuracy: 0.7347\nEpoch 2/5\n782/782 [==============================] - 10s 12ms/step - loss: 0.4723 - accuracy: 0.7681 - val_loss: 0.5087 - val_accuracy: 0.7419\nEpoch 3/5\n782/782 [==============================] - 10s 12ms/step - loss: 0.4240 - accuracy: 0.8027 - val_loss: 0.5183 - val_accuracy: 0.7395\nEpoch 4/5\n782/782 [==============================] - 9s 12ms/step - loss: 0.3571 - accuracy: 0.8472 - val_loss: 0.5386 - val_accuracy: 0.7352\nEpoch 5/5\n782/782 [==============================] - 8s 11ms/step - loss: 0.2769 - accuracy: 0.8971 - val_loss: 0.5708 - val_accuracy: 0.7279\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2ce6875c2b0>"
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "model = keras.models.Sequential([ # now we're ready to train our model\n",
    "    text_vectorization,\n",
    "    bag_of_words,\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise e): Add an `Embedding` layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[2.3570225, 3.2998314, 1.4142135],\n       [2.       , 0.       , 0.       ]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 159
    }
   ],
   "source": [
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_mean(inputs, axis=1) * sqrt_n_words\n",
    "\n",
    "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
    "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
    "compute_mean_embedding(another_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[2.3570225, 3.2998314, 1.4142135],\n       [2.       , 0.       , 0.       ]], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 160
    }
   ],
   "source": [
    "tf.reduce_mean(another_example, axis=1) * tf.sqrt([[2.], [1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 20\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    text_vectorization,\n",
    "    keras.layers.Embedding(input_dim=n_tokens,\n",
    "                           output_dim=embedding_size,\n",
    "                           mask_zero=True), # <pad> tokens => zero vectors\n",
    "    keras.layers.Lambda(compute_mean_embedding),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise f): Train the model and see what accuracy you get. Try to optimize your pipelines to make training as fast as possible._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/5\n782/782 [==============================] - 8s 10ms/step - loss: 0.5571 - accuracy: 0.7069 - val_loss: 0.5174 - val_accuracy: 0.7361\nEpoch 2/5\n782/782 [==============================] - 6s 8ms/step - loss: 0.4961 - accuracy: 0.7566 - val_loss: 0.5219 - val_accuracy: 0.7304\nEpoch 3/5\n782/782 [==============================] - 6s 8ms/step - loss: 0.4862 - accuracy: 0.7608 - val_loss: 0.5124 - val_accuracy: 0.7419\nEpoch 4/5\n782/782 [==============================] - 6s 8ms/step - loss: 0.4776 - accuracy: 0.7623 - val_loss: 0.5111 - val_accuracy: 0.7391\nEpoch 5/5\n782/782 [==============================] - 6s 8ms/step - loss: 0.4711 - accuracy: 0.7633 - val_loss: 0.5099 - val_accuracy: 0.7415\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2ce69c66630>"
     },
     "metadata": {},
     "execution_count": 162
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise g): Use TFDS to load the same dataset more easily: `tfds.load(\"imdb_reviews\")`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Dl Completed...: 0 url [00:00, ? url/s]\u001b[1mDownloading and preparing dataset imdb_reviews/plain_text/1.0.0 (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to C:\\Users\\stuar\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0...\u001b[0m\n\nDl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:00<?, ? url/s]\nDl Size...:   0%|          | 0/80 [00:00<?, ? MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\nDl Size...:   1%|▏         | 1/80 [00:01<01:47,  1.37s/ MiB]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:01<?, ? url/s]\nDl Size...:   2%|▎         | 2/80 [00:01<01:25,  1.10s/ MiB]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\nDl Size...:   4%|▍         | 3/80 [00:02<01:09,  1.11 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\nDl Size...:   5%|▌         | 4/80 [00:02<00:54,  1.40 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:02<?, ? url/s]\nDl Size...:   6%|▋         | 5/80 [00:02<00:45,  1.64 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:   8%|▊         | 6/80 [00:03<00:36,  2.04 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:   9%|▉         | 7/80 [00:03<00:29,  2.50 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  10%|█         | 8/80 [00:03<00:24,  2.91 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  11%|█▏        | 9/80 [00:03<00:21,  3.32 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:03<?, ? url/s]\nDl Size...:  12%|█▎        | 10/80 [00:03<00:18,  3.77 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  14%|█▍        | 11/80 [00:04<00:15,  4.44 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  15%|█▌        | 12/80 [00:04<00:13,  4.87 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  16%|█▋        | 13/80 [00:04<00:12,  5.23 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  18%|█▊        | 14/80 [00:04<00:12,  5.45 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  19%|█▉        | 15/80 [00:04<00:11,  5.72 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:04<?, ? url/s]\nDl Size...:  20%|██        | 16/80 [00:04<00:11,  5.76 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\nDl Size...:  21%|██▏       | 17/80 [00:05<00:11,  5.47 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\nDl Size...:  22%|██▎       | 18/80 [00:05<00:10,  5.99 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\nDl Size...:  24%|██▍       | 19/80 [00:05<00:08,  6.80 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\nDl Size...:  26%|██▋       | 21/80 [00:05<00:07,  7.47 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\nDl Size...:  29%|██▉       | 23/80 [00:05<00:06,  8.16 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:05<?, ? url/s]\nDl Size...:  31%|███▏      | 25/80 [00:05<00:06,  8.42 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\nDl Size...:  34%|███▍      | 27/80 [00:06<00:05,  9.00 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\nDl Size...:  36%|███▋      | 29/80 [00:06<00:05,  9.19 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\nDl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\nDl Size...:  39%|███▉      | 31/80 [00:06<00:05,  9.52 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\nDl Size...:  40%|████      | 32/80 [00:06<00:05,  9.15 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\nDl Size...:  41%|████▏     | 33/80 [00:06<00:05,  8.24 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:06<?, ? url/s]\nDl Size...:  42%|████▎     | 34/80 [00:06<00:05,  8.11 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\nDl Size...:  44%|████▍     | 35/80 [00:07<00:05,  8.16 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\nDl Size...:  45%|████▌     | 36/80 [00:07<00:05,  8.16 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\nDl Size...:  46%|████▋     | 37/80 [00:07<00:05,  7.78 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\nDl Size...:  48%|████▊     | 38/80 [00:07<00:05,  7.24 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\nDl Size...:  49%|████▉     | 39/80 [00:07<00:05,  7.37 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\nDl Size...:  50%|█████     | 40/80 [00:07<00:05,  7.54 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\nDl Size...:  51%|█████▏    | 41/80 [00:07<00:05,  7.62 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:07<?, ? url/s]\nDl Size...:  52%|█████▎    | 42/80 [00:07<00:05,  7.51 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\nDl Size...:  54%|█████▍    | 43/80 [00:08<00:05,  6.34 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\nDl Size...:  55%|█████▌    | 44/80 [00:08<00:05,  6.89 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\nDl Size...:  56%|█████▋    | 45/80 [00:08<00:04,  7.33 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\nDl Size...:  57%|█████▊    | 46/80 [00:08<00:04,  7.11 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\nDl Size...:  59%|█████▉    | 47/80 [00:08<00:04,  7.67 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\nDl Size...:  60%|██████    | 48/80 [00:08<00:04,  7.75 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:08<?, ? url/s]\nDl Size...:  61%|██████▏   | 49/80 [00:08<00:03,  7.93 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\nDl Size...:  62%|██████▎   | 50/80 [00:09<00:03,  7.70 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\nDl Size...:  64%|██████▍   | 51/80 [00:09<00:03,  7.67 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\nDl Size...:  65%|██████▌   | 52/80 [00:09<00:03,  7.17 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\nDl Size...:  66%|██████▋   | 53/80 [00:09<00:03,  7.43 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\nDl Size...:  68%|██████▊   | 54/80 [00:09<00:03,  6.79 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\nDl Size...:  69%|██████▉   | 55/80 [00:09<00:04,  6.23 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:09<?, ? url/s]\nDl Size...:  70%|███████   | 56/80 [00:09<00:03,  6.63 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\nDl Size...:  71%|███████▏  | 57/80 [00:10<00:03,  6.60 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\nDl Size...:  72%|███████▎  | 58/80 [00:10<00:03,  6.66 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\nDl Size...:  74%|███████▍  | 59/80 [00:10<00:03,  6.92 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\nDl Size...:  75%|███████▌  | 60/80 [00:10<00:02,  7.03 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\nDl Size...:  76%|███████▋  | 61/80 [00:10<00:03,  5.96 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:10<?, ? url/s]\nDl Size...:  78%|███████▊  | 62/80 [00:10<00:03,  5.46 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\nDl Size...:  79%|███████▉  | 63/80 [00:11<00:02,  5.67 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\nDl Size...:  80%|████████  | 64/80 [00:11<00:02,  5.94 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\nDl Size...:  81%|████████▏ | 65/80 [00:11<00:02,  6.11 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\nDl Size...:  82%|████████▎ | 66/80 [00:11<00:02,  5.69 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\nDl Size...:  84%|████████▍ | 67/80 [00:11<00:02,  6.13 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:11<?, ? url/s]\nDl Size...:  85%|████████▌ | 68/80 [00:11<00:01,  6.77 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\nDl Size...:  86%|████████▋ | 69/80 [00:12<00:01,  7.21 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\nDl Size...:  88%|████████▊ | 70/80 [00:12<00:01,  6.28 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\nDl Size...:  89%|████████▉ | 71/80 [00:12<00:01,  6.44 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\nDl Size...:  90%|█████████ | 72/80 [00:12<00:01,  6.43 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\nDl Size...:  91%|█████████▏| 73/80 [00:12<00:01,  6.62 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\nDl Size...:  92%|█████████▎| 74/80 [00:12<00:00,  6.98 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:12<?, ? url/s]\nDl Size...:  94%|█████████▍| 75/80 [00:12<00:00,  6.87 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\nDl Size...:  95%|█████████▌| 76/80 [00:13<00:00,  6.87 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\nDl Size...:  96%|█████████▋| 77/80 [00:13<00:00,  6.47 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\nDl Size...:  98%|█████████▊| 78/80 [00:13<00:00,  6.87 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\nDl Size...:  99%|█████████▉| 79/80 [00:13<00:00,  7.40 MiB/s]\u001b[A\nDl Completed...:   0%|          | 0/1 [00:13<?, ? url/s]\nDl Completed...: 100%|██████████| 1/1 [00:13<00:00, 13.81s/ url]\nDl Size...: 100%|██████████| 80/80 [00:13<00:00,  5.79 MiB/s]\nDl Completed...: 100%|██████████| 1/1 [00:13<00:00, 13.81s/ url]\n0 examples [00:00, ? examples/s]\n\n 19%|█▉        | 4761/25000 [00:00<00:00, 47264.43 examples/s]Shuffling and writing examples to C:\\Users\\stuar\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incomplete7VWMFP\\imdb_reviews-train.tfrecord\n 23%|██▎       | 5805/25000 [00:00<00:00, 57583.82 examples/s]Shuffling and writing examples to C:\\Users\\stuar\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incomplete7VWMFP\\imdb_reviews-test.tfrecord\n  7%|▋         | 3395/50000 [00:00<00:01, 33703.66 examples/s]Shuffling and writing examples to C:\\Users\\stuar\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0.incomplete7VWMFP\\imdb_reviews-unsupervised.tfrecord\n 93%|█████████▎| 46622/50000 [00:00<00:00, 63298.37 examples/s]\u001b[1mDataset imdb_reviews downloaded and prepared to C:\\Users\\stuar\\tensorflow_datasets\\imdb_reviews\\plain_text\\1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"imdb_reviews\")\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(b'This is a big step down after the surprisingly enjoyable original. This sequel isn\\'t nearly as fun as part one, and it instead spends too much time on plot development. Tim Thomerson is still the best thing about this series, but his wisecracking is toned down in this entry. The performances are all adequate, but this time the script lets us down. The action is merely routine and the plot is only mildly interesting, so I need lots of silly laughs in order to stay entertained during a \"Trancers\" movie. Unfortunately, the laughs are few and far between, and so, this film is watchable at best.', shape=(), dtype=string)\ntf.Tensor(0, shape=(), dtype=int64)\n"
    }
   ],
   "source": [
    "for example in train_set.take(1):\n",
    "    print(example[\"text\"])\n",
    "    print(example[\"label\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}